#LyX 1.4.4 created this file. For more info see http://www.lyx.org/
\lyxformat 245
\begin_document
\begin_header
\textclass article
\language spanish
\inputencoding auto
\fontscheme default
\graphics default
\paperfontsize default
\spacing single
\papersize default
\use_geometry false
\use_amsmath 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language swedish
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\end_header

\begin_body

\begin_layout Title
Estado del Arte
\end_layout

\begin_layout Title
Cluster de Computadores de Alto Desempeño con Acceso Remoto
\end_layout

\begin_layout Title
Carrera de Ingeniería en Computación
\end_layout

\begin_layout Title
Facultad de Ingeniería
\end_layout

\begin_layout Author
Santiago Iturriaga, Paulo Maya, Damian Pintos
\end_layout

\begin_layout Section*
Resumen
\end_layout

\begin_layout Section*
Introducción
\end_layout

\begin_layout Paragraph

\lang english
En el principio de la historia de las computadoras, se tenía la idea de
 que un servidor tenia que ser necesariamente una máquina, conocida como
 super computador o mainframe, que contaba con un súper procesador que daba
 servicio a varias terminales tontas.
 Asi, para asegurar mayor poder de cálculo es necesario tener un procesador
 más poderoso.
 De este modo, cuando los requerimientos de procesamiento aumentan, es necesario
, en la mayoría de los casos, cambiar la super computadora actual por una
 nueva mejor, pasando a subutilizar la super computadora anterior, con un
 alto costo asociado.
\end_layout

\begin_layout Paragraph

\lang english
Esta idea encontró oposición en 1994, cuando Donald Becker y Thomas Sterling
 crearon el proyecto Beowulf, en el que lograron un elevado nivel de procesamien
to poniendo a trabajar varias computadoras en paralelo con procesadores
 16 DX4, interconectadas en una red de 10 Mbit Ethernet.
\end_layout

\begin_layout Paragraph

\lang english
A partir de ese momento, y debido al éxito alcanzado en el proyecto, muchos
 otros proyectos siguieron la investigaci´n del tema, bajo la premisa de
 convertir hardware de relativo bajo costo en clusters que logren equiparar
 o superar la performance alcanzada por las supercomputadoras.
\end_layout

\begin_layout Paragraph

\lang english
Actualmente, el cluster de computadoras se ha utilizado para varias y diferentes
 tareas como Data Mining, Servidores de Archivos, Servidores de Base de
 Datos, Servidores Web, Simuladores de Vuelo, Graphics Rendering, Modeladores
 Climáticos o incluso para CD Ripping alcanzando velocidades realmente altas.
\end_layout

\begin_layout Paragraph

\lang english
La idea es simplemente reunir el poder de cómputo de una serie de nodos
 para proveer alta escabilidad, poder de cómputo, o construir redundancia
 y de esta manera proveer alta disponibilidad.
 Entonces en vez de un simple cliente haciendo peticiones de uno o más servidore
s, los cluster utilizan múltiples máquinas para proveer un ambiente más
 poderoso de cómputo, a través de una sola imagen de sistema, esto es que
 el procesamiento sea visible al usuario como una sola unidad, aunque se
 componga de muchas computadoras trabajando en paralelo para el mismo fin.
\end_layout

\begin_layout Paragraph

\lang english
El alto poder de procesamiento de varias computadoras trabajando en paralelo,
 (o cluster de computadoras), se perfila como una solución viable a las
 empresas, universidades y escuelas a un bajo costo de implementación.
 
\end_layout

\begin_layout Paragraph

\lang english
Con el tiempo, la investigación se ha ido especializando en áreas específicas.
 El Cluster de Alto Rendimiento se aplica principalmente a aplicaciones
 científicas mientras que el Cluster de Alta Disponibilidad y Cluster de
 Balanceo de Carga son más utilizado en el ambiente de negocios.
 En este trabajo nos enfocaremos en el primero.
\end_layout

\begin_layout Subsection*

\lang english
Trabajos previos
\end_layout

\begin_layout Standard

\lang english
No son pocos los trabajos realizados sobre investigación, desarrollo, construcci
ón y aplicaciones de clusters, pero es relativamente poca la bibliografía
 asociada, dado que el tema ha ganado auge principalmente en la última década,
 más alla de que la idea no es nueva.
 Se remontan estudios desde 1970, pero se puede decir que IBM fue la primera
 en establecer una teoría formal con respecto a los clusters.
 Este trabajo fue realizado por Gene Myron Amdahl (arquitecto de computadoras
 estadounidense noruego), que en el momento de realizar el estudio donde
 demostró la ley de Amdal (llamada axial en honor a su nombre).
 Esta ley se usa para encontrar la máxima mejora esperada a un sistema en
 su totalidad, cuando solo parte del sistema se mejora.
 Esto es usado muy a menudo en computación paralela para predecir la máxima
 mejora de velocidad usando múltiples procesadores.
 
\end_layout

\begin_layout Itemize

\lang english
Ley de amdahl: 
\begin_inset Quotes eld
\end_inset

La mejora obtenida en el rendimiento de un sistema debido a la alteración
 de uno de sus componentes está limitada por la fracción de tiempo que se
 utiliza dicho componente
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\lang english
\begin_inset Formula $A=F_{a}*((1-F_{m})+F_{m}/A_{m}$
\end_inset

) donde 
\begin_inset Formula $F_{a}$
\end_inset

 es el tiempo de ejecución antiguo, 
\begin_inset Formula $A$
\end_inset

 es la aceleración obtenida, 
\begin_inset Formula $A_{m}$
\end_inset

 es el factor de mejora introducido y 
\begin_inset Formula $F_{m}$
\end_inset

 es la fracción de tiempo que el sistema utiliza el subsistema mejorado
\end_layout

\end_deeper
\begin_layout Standard

\lang english
Hoy en día, hay muchísimos trabajos de diferente índole con respecto a los
 clusters de computadoras, debido a que en elámbito de computación paralela,
 el cluster es la nueva modalidad de construcción de supercomputadoras,
 por su bajo costo y accesibilidad.
\end_layout

\begin_layout Subsection*

\lang english
Proyectos / Productos de Clusters
\end_layout

\begin_layout Subsubsection*

\lang english
Clusters de Alta Disponibilidad
\end_layout

\begin_layout Itemize

\lang english
Oracle Real Application Clusters
\end_layout

\begin_layout Itemize

\lang english
IBM DB2 Integrated Cluster Environment
\end_layout

\begin_layout Itemize

\lang english
Linux HA
\end_layout

\begin_layout Itemize

\lang english
FreeNAS
\end_layout

\begin_layout Itemize

\lang english
HA-OSCAR: The High Availability Linux Project
\end_layout

\begin_layout Itemize

\lang english
Kimberlite 
\end_layout

\begin_layout Itemize

\lang english
Lifekeeper
\end_layout

\begin_layout Itemize

\lang english
Linux FailSafe
\end_layout

\begin_layout Itemize

\lang english
Linux Replicated High Availability Manager
\end_layout

\begin_layout Itemize

\lang english
MC/Serviceguard
\end_layout

\begin_layout Subsubsection*

\lang english
Clusters de Alto Rendimiento
\end_layout

\begin_layout Itemize

\lang english
PVM
\end_layout

\begin_layout Itemize

\lang english
pvmsync
\end_layout

\begin_layout Itemize

\lang english
MPI
\end_layout

\begin_layout Itemize

\lang english
LAM/MPI
\end_layout

\begin_layout Itemize

\lang english
PLUS: MPI&PVM integration
\end_layout

\begin_layout Itemize

\lang english
Beowulf (for Linux)
\end_layout

\begin_layout Itemize

\lang english
Berkeley NOW (for Solaris)
\end_layout

\begin_layout Itemize

\lang english
Parallel Knoppix
\end_layout

\begin_layout Subsubsection*

\lang english
Mallas (Grids) de computadoras
\end_layout

\begin_layout Itemize

\lang english
SGE (Sun Grid Engine)
\end_layout

\begin_layout Itemize

\lang english
GridSim toolkit for Simulation of Application Scheduling
\end_layout

\begin_layout Itemize

\lang english
Gridbus: Toolkit for service-oriented cluster and grid computing 
\end_layout

\begin_layout Subsubsection*

\lang english
Balanceo de Carga
\end_layout

\begin_layout Itemize

\lang english
Alinka Oranges 
\end_layout

\begin_layout Itemize

\lang english
Keepalived
\end_layout

\begin_layout Itemize

\lang english
Linux Virtual Server
\end_layout

\begin_layout Itemize

\lang english
LVSM
\end_layout

\begin_layout Itemize

\lang english
Red Hat Cluster Suite
\end_layout

\begin_layout Itemize

\lang english
Turbolinux Cluster Server
\end_layout

\begin_layout Itemize

\lang english
Ultra Monkey
\end_layout

\begin_layout Itemize

\lang english
Condor
\end_layout

\begin_layout Itemize

\lang english
openMosix
\end_layout

\begin_layout Itemize

\lang english
Cluster Knoppix
\end_layout

\begin_layout Section*
Computación de Alto Rendimiento
\end_layout

\begin_layout Paragraph
El campo de la computación de alto rendimiento es muy importante en la resolució
n a problemas complejos.
 
\end_layout

\begin_layout Paragraph
La computación de alto rendimiento se apoya en tecnologías como los clusters,
 supercomputadores o mediante el uso de la computación paralela.
 
\end_layout

\begin_layout Subsection*
Computación Distribuida
\end_layout

\begin_layout Paragraph
La computación distribuida es un modelo para resolver problemas de computación
 masiva utilizando un gran número de computadoras en una infraestructura
 de telecomunicaciones distribuida.
\end_layout

\begin_layout Paragraph
La informática distribuida consiste en compartir recursos heterogéneos (basadas
 en distintas plataformas, arquitecturas de equipos y programas, lenguajes
 de programación), situados en distintos lugares y pertenecientes a diferentes
 dominios de administración sobre una red que utiliza estándares abiertos.
 
\end_layout

\begin_layout Paragraph
La computación distribuida ha sido diseñada para resolver problemas demasiado
 grandes para cualquier supercomputadora y main-frame, manteniendose la
 flexibilidad de trabajar en múltiples problemas más pequeños.
 Por lo tanto, la computación en grid es naturalmente un entorno multi-usuario;
 por ello, las técnicas de autorización segura son esenciales antes de permitir
 que los recursos informáticos sean controlados por usuarios remotos.
\end_layout

\begin_layout Section*
Clusters
\end_layout

\begin_layout Paragraph
El término cluster se aplica a los conjuntos de computadoras que se comportan
 como si fuesen una única computadora.
 Hoy en día tiene un papel importante en aplicaciones científicas y de ingenierí
a, comerciales, simulaciones, etc.
\end_layout

\begin_layout Paragraph
La tecnología de clusters ha evolucionado apoyándose en actividades que
 van desde aplicaciones de supercómputo y software de misiones críticas,
 servidores Web y comercio electrónico, hasta bases de datos de alto rendimiento.
\end_layout

\begin_layout Paragraph
El uso de clusters surge gracias a la convergencia de varias tendencias
 actuales como la disponibilidad de microprocesadores económicos de alto
 rendimiento y redes de alta velocidad, la existentica de herramientas para
 cómputo distribuido de alto rendimiento, así como la creciente necesidad
 de potencia computacional para aplicaciones que la requieran.
\end_layout

\begin_layout Paragraph
Un cluster es un grupo de múltiples ordenadores unidos mediante una red
 de alta velocidad, de tal forma que el conjunto es visto como un único
 ordenador, más potente que los comunes de escritorio.
 
\end_layout

\begin_layout Paragraph
Se espera de un cluster que presente combinaciones de los siguientes característ
icas:
\end_layout

\begin_layout Enumerate
Alto rendimiento
\end_layout

\begin_layout Enumerate
Alta disponibilidad
\end_layout

\begin_layout Enumerate
Equilibrio de carga 
\end_layout

\begin_layout Enumerate
Escalabilidad 
\end_layout

\begin_layout Paragraph
La construcción de los cluster es muy fácil y económica, debido a su flexibilida
d: pueden tener todos la misma configuración de hardware y sistema operativo
 (cluster homogéneo), diferente rendimiento pero con arquitecturas y sistemas
 operativos similares (cluster semi-homogéneo), o tener diferente hardware
 y sistema operativo (cluster heterogéneo).
\end_layout

\begin_layout Paragraph
Se pueden construir cluster con ordenadores personales desechados por "anticuado
s" que consiguen competir en capacidad de cálculo con superordenadores carísimos.
\end_layout

\begin_layout Paragraph
Es necesario proveer un sistema para el manejo del cluster, el cual se encargue
 de interactuar con el usuario y los procesos que corren en él para optimizar
 el funcionamiento.
\end_layout

\begin_layout Subsubsection*
Cluster de alto rendimiento
\end_layout

\begin_layout Paragraph
Un cluster de alto rendimiento está diseñado para dar altas prestaciones
 en cuanto a capacidad de cálculo.
\end_layout

\begin_layout Paragraph
Los motivos para utilizar un cluster de alto rendimiento son:
\end_layout

\begin_layout Itemize
el tamaño del problema por resolver y
\end_layout

\begin_layout Itemize
el precio de la máquina necesaria para resolverlo.
\end_layout

\begin_layout Paragraph
Por medio de un cluster se pueden conseguir capacidades de cálculo superiores
 a las de un ordenador más caro.
\end_layout

\begin_layout Paragraph
Para garantizar esta capacidad de cálculo, los problemas necesitan ser paraleliz
ables, ya que el método con el que los clusters agilizan el procesamiento
 es dividir el problema en problemas más pequeños y calcularlos en los nodos.
\end_layout

\begin_layout Subsubsection*
Cluster de alta disponibilidad
\end_layout

\begin_layout Paragraph
Un cluster de alta disponibilidad se caracterizan por compartir los discos
 de almacenamiento de datos y por estar constantemente monitorizándose entre
 sí.
 
\end_layout

\begin_layout Paragraph
Podemos dividirlo en dos clases:
\end_layout

\begin_layout Itemize
Alta disponibilidad de infrestructura
\end_layout

\begin_layout Itemize
Alta disponibilidad de aplicación
\end_layout

\begin_layout Subsubsection*
Cluster de balanceo de carga
\end_layout

\begin_layout Paragraph
Un cluster de equilibrio de carga o de cómputo adaptativo está compuesto
 por uno o más ordenadores que actúan como frontend del cluster, y que se
 ocupan de repartir las peticiones de servicio que reciba el cluster, a
 otros ordenadores del cluster que forman el back-end de éste.
 
\end_layout

\begin_layout Subsubsection*
Escalabilidad
\end_layout

\begin_layout Paragraph
La escalabilidad es la propiedad deseable de un sistema, una red o un proceso,
 que indica su habilidad para, o bien manejar el crecimiento continuo de
 trabajo de manera fluida, o bien para estar preparado para hacerse más
 grande sin perder calidad en los servicios ofrecidos.
\end_layout

\begin_layout Paragraph
Las características más destacadas de este tipo de cluster son:
\end_layout

\begin_layout Itemize
Se puede ampliar su capacidad fácilmente añadiendo más ordenadores al cluster.
\end_layout

\begin_layout Itemize
Robustez.
 Ante la caída de alguno de los ordenadores del cluster el servicio se puede
 ver mermado, pero mientras haya ordenadores en funcionamiento, éstos seguirán
 dando servicio.
\end_layout

\begin_layout Section*
Programación Paralela
\end_layout

\begin_layout Paragraph
La programación paralela es una técnica de programación basada en la ejecución
 simultánea, bien sea en un mismo ordenador (con uno o varios procesadores)
 o en un cluster de ordenadores, en cuyo caso se denomina computación distribuid
a.
 Al contrario que en la programación concurrente, esta técnica enfatiza
 la verdadera simultaneidad en el tiempo de la ejecución de las tareas.
\end_layout

\begin_layout Paragraph
Los sistemas con multiprocesador y multicomputadores consiguen un aumento
 del rendimiento si se utilizan estas técnicas.
 En los sistemas monoprocesador el beneficio en rendimiento no es tan evidente,
 ya que la CPU es compartida por múltiples procesos en el tiempo, lo que
 se denomina multiplexación o multiprogramación.
\end_layout

\begin_layout Paragraph
El mayor problema de la computación paralela radica en la complejidad de
 sincronizar unas tareas con otras, ya sea mediante secciones críticas,
 semáforos o paso de mensajes, para garantizar la exclusión mutua en las
 zonas del código en las que sea necesario.
\end_layout

\begin_layout Standard
Aplicaciones demandantes de recursos
\end_layout

\begin_layout Standard
Podemos hacer una division de las aplicaciones que requieren una demanda
 de recusrsos computaionales importantes de la siguiente manera.
\end_layout

\begin_layout Standard
Aplicacions de calculo intencivas
\end_layout

\begin_layout Standard
Son aplicaciones que requieren un alto numero de ciclos de máquinas, estas
 son las que han impulsado
\end_layout

\begin_layout Standard
el desarrollo de supercomputadores.
 Son típicas en ciencias e ingeniería, aunque recientemente
\end_layout

\begin_layout Standard
han aparecido en otras áreas como simulación financiera y económica.
 Dependen grandemente de
\end_layout

\begin_layout Standard
la velocidad y el procesamiento de punto flotantes de los supercomputadores.
 
\end_layout

\begin_layout Standard
Algunos ejemplos son:
\end_layout

\begin_layout Standard
1.
 Dinámica de fluidos computacional.
\end_layout

\begin_layout Standard
2.
 Simulaciones electromagnéticas.
\end_layout

\begin_layout Standard
3.
 Modelado ambiental.
\end_layout

\begin_layout Standard
4.
 Dinámica estructural.
\end_layout

\begin_layout Standard
5.
 Modelado biológico.
\end_layout

\begin_layout Standard
6.
 Dinámica molecular.
\end_layout

\begin_layout Standard
7.
 Simulación de redes.
\end_layout

\begin_layout Standard
8.
 Modelado financiero y económico.
\end_layout

\begin_layout Standard
Aplicaciones de almacenemiento masivo
\end_layout

\begin_layout Standard
Dependen de la capacidad para almacenar y procesar grandes cantidades de
 información.
 Requieren de un acceso rápido y seguro a una masa considerable de datos
\end_layout

\begin_layout Standard
almacenados.
 
\end_layout

\begin_layout Standard
Algunas de ellas son:
\end_layout

\begin_layout Standard
1.
 Análisis de data sísmica.
\end_layout

\begin_layout Standard
2.
 Procesamiento de imágenes.
\end_layout

\begin_layout Standard
3.
 Minería de datos.
\end_layout

\begin_layout Standard
4.
 Análisis estadístico de datos.
\end_layout

\begin_layout Standard
5.
 Análisis de mercados.
\end_layout

\begin_layout Standard
Aplicaciones exigentes comunicacionalmente
\end_layout

\begin_layout Standard
Estas son relativamente nuevas y pueden ser llamadas servicios por demanda.
 Requieren de
\end_layout

\begin_layout Standard
recursos computacionales conectados por redes con anchos de banda considerables.
\end_layout

\begin_layout Standard
Ejemplos:
\end_layout

\begin_layout Standard
1.
 Procesamiento de transacciones en línea.
\end_layout

\begin_layout Standard
2.
 Sistemas colaborativos.
\end_layout

\begin_layout Standard
3.
 Texto por demanda.
\end_layout

\begin_layout Standard
4.
 Vídeo por demanda.
\end_layout

\begin_layout Standard
5.
 Imágenes por demanda.
\end_layout

\begin_layout Standard
6.
 Simulación por demanda.
\end_layout

\begin_layout Standard
Obviamente que todas las aplicaciones anteriores dependen en cierto grado
 de cada uno de los
\end_layout

\begin_layout Standard
aspectos computacionales mencionados: poder de computo, capacidades de almacenam
iento y
\end_layout

\begin_layout Standard
eficientes canales de comunicación, sin embargo las podemos agrupar por
 su característica
\end_layout

\begin_layout Standard
dominante.
\end_layout

\begin_layout Standard
Sistemas de sistemas
\end_layout

\begin_layout Standard
Las aplicaciones en este grupo combinan en forma más compleja las característica
s anteriores
\end_layout

\begin_layout Standard
y dependen, en muchos casos, de sistemas computacionales integrados diseñados
\end_layout

\begin_layout Standard
primordialmente para ellas.
 
\end_layout

\begin_layout Standard
Ejemplos:
\end_layout

\begin_layout Standard
1.
 Soporte a decisiones corporativas y gubernamentales.
\end_layout

\begin_layout Standard
2.
 Control de sistemas a tiempo real.
\end_layout

\begin_layout Standard
3.
 Banca electrónica.
\end_layout

\begin_layout Standard
4.
 Compras electrónicas.
\end_layout

\begin_layout Standard
5.
 Educación.
\end_layout

\begin_layout Standard
Existe una alta correspondencia entre la evolución de tecnologías informaticas
 y el
\end_layout

\begin_layout Standard
desarrollo de aplicaciones; en particular el hardware tiene gran influencia
 en el éxito de ciertas
\end_layout

\begin_layout Standard
áreas.
 La aplicaciones intensivas en cálculo fueron estimuladas principalmente
 por máquinas
\end_layout

\begin_layout Standard
vectoriales y procesadores masivamente paralelos.
 Las aplicaciones de almacenamiento masivo
\end_layout

\begin_layout Standard
han sido guiadas por dispositivos de almacenamiento como RAID y robots de
 cintas.
 Las
\end_layout

\begin_layout Standard
aplicaciones exigentes comunicacionales como herramientas colaborativas
 basadas en WWW y
\end_layout

\begin_layout Standard
servicios por demanda en línea originalmente surgieron con las LAN y estan
 creciendo
\end_layout

\begin_layout Standard
drásticamente con Internet.
\end_layout

\begin_layout Standard
Organizacion de computadores
\end_layout

\begin_layout Standard
La organización de los procesadores o red se refiere a como se conectan
 o enlazan los
\end_layout

\begin_layout Standard
procesadores o nodos en un computador paralelo.
\end_layout

\begin_layout Standard
Existen varios criteriospara evaluar los distintos diseños de organizacion:
\end_layout

\begin_layout Standard
1) Diámetro: viene dado por la mayor distancia entre dos nodos.
 Mientras menor
\end_layout

\begin_layout Standard
sea el diámetro menor será el tiempo de comunicación entre nodos.
\end_layout

\begin_layout Standard
2) Ancho de bisección de la red: es el menor número de enlaces que deben
 ser removidos
\end_layout

\begin_layout Standard
para dividir la red por la mitad.
 Un ancho de bisección alto puede reducir el tiempo de comunicación cuando
 el movimiento de datos es sustancial, y un ancho
\end_layout

\begin_layout Standard
de bisección alto hace el sistema más tolerante a fallas debido a que defectos
 en un nodo no hacen inoperable a todo el sistema.
\end_layout

\begin_layout Standard
3) Es preferible que el número de enlaces por nodo sea una constante independien
te del
\end_layout

\begin_layout Standard
tamaño de la red, ya que hace más fácil incrementar el número de nodos.
\end_layout

\begin_layout Standard
4) Es preferible que la longitud máxima de los enlaces sea una constante
 independiente del
\end_layout

\begin_layout Standard
tamaño de la red, ya que hace más fácil añadir nodos.
 
\end_layout

\begin_layout Standard
5) Redes estáticas y dinámicas.
 En las redes estáticas la topología de interconexión se
\end_layout

\begin_layout Standard
define cuando se construye la máquina.
 Si la red es dinámica, la interconexión puede
\end_layout

\begin_layout Standard
variar durante la ejecución de un programa o entre la ejecución de programas.
\end_layout

\begin_layout Standard
A continuacion veremos algunos tipos de disenos de redes de procesadores.
\end_layout

\begin_layout Standard
BUS Y ETHERNET
\end_layout

\begin_layout Standard
En una red donde los procesadores comparten el mismo recurso de comunicación:
\end_layout

\begin_layout Standard
el bus.
 
\end_layout

\begin_layout Standard
Esta arquitectura es fácil y económica de implementar, pero es altamente
 no escalable ya
\end_layout

\begin_layout Standard
que solo un procesador puede usar el bus en un momento dado; a medida que
 se incrementa el
\end_layout

\begin_layout Standard
número de procesadores, el bus se convierte en un cuello de botella debido
 a la congestión.
 
\end_layout

\begin_layout Standard
Mallas
\end_layout

\begin_layout Standard
Al igual que el bus (o ethernet), las mallas son fáciles y económicas de
 implementar, sin
\end_layout

\begin_layout Standard
embargo el diámetro se incrementa al añadir nodos.
 En las mallas de dimensión 1, si los dos
\end_layout

\begin_layout Standard
nodos extremos también son conectados, entonces se tiene un anillo
\end_layout

\begin_layout Standard
Mallas de 2 dimensiones y de 3 dimensiones son comunes en computación paralela
\end_layout

\begin_layout Standard
y tiene la ventaja de que pueden ser construidas sin conexiones largas.
 El diámetro de las mallas
\end_layout

\begin_layout Standard
puede ser reducido a la mitad si se extiende la malla con conexiones toroidales
 de forma que los
\end_layout

\begin_layout Standard
procesadores en los bordes también estén conectados con vecinos.
 Esto sin embargo
\end_layout

\begin_layout Standard
presenta dos desventajas: a) conexiones más largas son requeridas y b) un
 subconjunto de un
\end_layout

\begin_layout Standard
torus no es un torus y los beneficios de esta interconexión se pierden si
 la máquina es
\end_layout

\begin_layout Standard
particionada entre varios usuarios.
\end_layout

\begin_layout Standard
MARIPOSA
\end_layout

\begin_layout Standard
Figura
\end_layout

\begin_layout Standard
ARBOLES BINARIOS
\end_layout

\begin_layout Standard
Son particularmente útiles en problemas de ordenamiento, multiplicación
\end_layout

\begin_layout Standard
de matrices, y algunos problemas en los que los tiempos sw solución crecen
 exponencialmente con el
\end_layout

\begin_layout Standard
tamaño del problema (NP-complejos).
 
\end_layout

\begin_layout Standard
PIRAMIDES
\end_layout

\begin_layout Standard
Estas redes intentan combinar las ventajas de las mallas y los arboles,
 se incrementa la tolerancia a fallas y el número de vias de comunicación.
\end_layout

\begin_layout Standard
HIPERCUBO
\end_layout

\begin_layout Standard
Un hipercubo puede ser considerado como una malla con conexiones largas
 adicionales, estas conexiones reducen el diámetro e incrementan el ancho
 de bisección.
 
\end_layout

\begin_layout Standard
Se puede definir recursibamente un hipercubo de la siguiente manera: un
 hipercubo de dimensión-cero es un único procesador
\end_layout

\begin_layout Standard
y un hipercubo de dimensión-uno conecta dos hipercubos de dimensión-cero.
 En general, un
\end_layout

\begin_layout Standard
hipercubo de dimensión d+1 con 2d+1 nodos, se construye conectando los procesado
res
\end_layout

\begin_layout Standard
respectivos de dos hipercubos de dimensión d.
 
\end_layout

\begin_layout Standard
OMEGA
\end_layout

\begin_layout Standard
La Figura 12 muestra una red omega de 3 etapas que conecta 8 procesadores).
 La red está
\end_layout

\begin_layout Standard
formada por crossbar switches 2x2.
 Los switches tiene cuatro estados posibles: recto, cruzado,
\end_layout

\begin_layout Standard
broadcast superior y broadcast inferior; que pueden ser configurados dependiendo
 de la
\end_layout

\begin_layout Standard
conexión que se desea (Figura 13).
 Debido a esto, estas topologías se llaman dinámicas o
\end_layout

\begin_layout Standard
reconfigurables.
 Los switches son unidireccionales y debemos ver la red plegada en donde
 los
\end_layout

\begin_layout Standard
procesadores de la izquierda y la derecha son los mismos.
 Estas redes reducen considerablemente
\end_layout

\begin_layout Standard
la competencia por ancho de banda, pero son altamente no escalables y costosas.
 El highperformance-
\end_layout

\begin_layout Standard
switch de la SP2 es una red omega.
\end_layout

\begin_layout Standard
FIBRAS DE INTERCONEXION
\end_layout

\begin_layout Standard
Estas so un conjunto de switches, llamados routers, enlazados por distintas
 configuraciones o topologías.
\end_layout

\begin_layout Standard
DISEÑO DE ALGORITMOS PARALELOS
\end_layout

\begin_layout Standard
El diseño de algoritmos paralelos involucra cuatro etapas, las cuales se
 presentan como
\end_layout

\begin_layout Standard
secuenciales pero que en la práctica no lo son.
\end_layout

\begin_layout Standard
1) Partición: El cómputo y los datos sobre los cuales se opera se descomponen
 en tareas.
 Se
\end_layout

\begin_layout Standard
ignoran aspectos como el número de procesadores de la máquina a usar y se
 concentra la
\end_layout

\begin_layout Standard
atención en explotar oportunidades de paralelismo.
\end_layout

\begin_layout Standard
2) Comunicación: Se determina la comunicación para coordinar las tareas.
 Se
\end_layout

\begin_layout Standard
definen estructuras y algoritmos de comunicación.
\end_layout

\begin_layout Standard
3) Agrupación: Se evalua en terminos de eficiencia y costos de implementacion
 a las dos etapas anteriores.
 
\end_layout

\begin_layout Standard
se agrupan tareas pequeñas en tareas más grandes.
\end_layout

\begin_layout Standard
4) Asignación: Cada tarea es asignada a un procesador tratando de maximizar
 la utilización
\end_layout

\begin_layout Standard
de los procesadores y de reducir el costo de comunicación.
 La asignación puede ser
\end_layout

\begin_layout Standard
estática (se establece antes de la ejecución del programa) o en tiempo de
 ejecución
\end_layout

\begin_layout Standard
mediante algoritmos de balanceo de carga.
\end_layout

\begin_layout Standard
Particion
\end_layout

\begin_layout Standard
En la etapa de partición se buscan oportunidades de paralelismo y se trata
 de subdividir el
\end_layout

\begin_layout Standard
problema lo más finamente posible.
 Se dividen tanto los cómputos como los datos.
 
\end_layout

\begin_layout Standard
Existen dos formas de descomposicion.
\end_layout

\begin_layout Standard
Descomposición del dominio: se centra en los datos.
 Se determina la partición
\end_layout

\begin_layout Standard
apropiada de los datos y luego se trabaja en los cómputos asociados con
 los datos.
\end_layout

\begin_layout Standard
Descomposición funcional: es el contrario al enfoque anterior, primero se
 descomponen los
\end_layout

\begin_layout Standard
cómputos y luego se ocupa de los datos.
\end_layout

\begin_layout Standard
En el proceso de particion existen aspectos a tener en cuenta: 
\end_layout

\begin_layout Standard
-el orden de tareas debe ser por lo menos superior al numero de procesadores
 para tener flexibilidad en etapas siguientes
\end_layout

\begin_layout Standard
-evitar cómputos y almacenamientos redundantes
\end_layout

\begin_layout Standard
-las tareas deben ser de tamaños equivalentes para facilitar el balanceo
\end_layout

\begin_layout Standard
de la carga de los procesadores.
\end_layout

\begin_layout Standard
-El número de tareas debe ser proporcional al tamaño del problema
\end_layout

\begin_layout Standard
Comunicación
\end_layout

\begin_layout Standard
El proceso de partición de tareas no es independiente del proceso de comunicació
n.
 En esta se especifica como los datos serán transferidos o compartidos entre
 tareas.
\end_layout

\begin_layout Standard
La comunicación puede ser definida en dos fases.
 Primero se definen los canales que conectan las tareas.
 Segundo
\end_layout

\begin_layout Standard
se especifica la información o mensajes que deben ser enviado y recibidos
 en estos canales.
\end_layout

\begin_layout Standard
Dependiendo de si estamos en el caso de memoria distribuida o memoria compartida
 dependera la forma de atacar la comunicación entre tareas varía.
\end_layout

\begin_layout Standard
En un ambientes de memoria distribuida, las tareas tiene una identificación
 única y interactuan enviando y recibiendo mensajes hacia y desde tareas
 específicas.
 Las librerías más conocidas para implementar el pase de mensajes en ambientes
 de memoria distribuida son: MPI (Message Passing Interface) y PVM (Parallel
 Virtual Machine).
\end_layout

\begin_layout Standard
En ambientes de memoria compartida no existe la noción de pertenencia y
 el envío de datos
\end_layout

\begin_layout Standard
no se dá como tal.
 Todas las tareas comparten la misma memoria.
\end_layout

\begin_layout Standard
Semáforos, semáforos binarios, barreras y otros mecanismos de sincronizacion
 son usados para controlar el acceso a la
\end_layout

\begin_layout Standard
memoria compartida y coordinar las tareas.
\end_layout

\begin_layout Standard
En esta etapa hay que tener en cuenta los siguientes aspectos:
\end_layout

\begin_layout Standard
-las tareas deben efectuar aproximadamente el mismo número de operaciones
 de
\end_layout

\begin_layout Standard
comunicación.
 De otra forma es muy probable que el algoritmo no sea extensible a
\end_layout

\begin_layout Standard
problemas mayores ya que habrán cuellos de botella.
\end_layout

\begin_layout Standard
-la comunicación entre tareas debe ser tan pequeña como sea posible.
\end_layout

\begin_layout Standard
-las operaciones de comunicación deben poder proceder concurrentemente.
\end_layout

\begin_layout Standard
-los cómputos de diferentes tareas deben poder proceder concurrentemente.
\end_layout

\begin_layout Standard
AGRUPACION
\end_layout

\begin_layout Standard
En las dos tareas anteriores el algoritmo resultante es aún abstracto en
 el sentido de que no se tomó en cuenta
\end_layout

\begin_layout Standard
la máquina sobre el cual correrá.
 En este proceso se busca un algoritmo concreto, que corra eficientemente
 sobre cierta clase
\end_layout

\begin_layout Standard
de computadores.
 
\end_layout

\begin_layout Standard
En particular se considera si es útil agrupar tareas y si vale la pena replicar
 datos y/o cómputos.
\end_layout

\begin_layout Standard
En el proceso de partición se trata de establecer el mayor número posible
 de tareas con la 
\end_layout

\begin_layout Standard
intensión de maximizar el paralelismo.
 
\end_layout

\begin_layout Standard
Esto no necesariamente produce un algoritmo eficiente ya que el costo de
 comunicación puede ser significativo.
 
\end_layout

\begin_layout Standard
Mediante la agrupación de tareas se puede reducir la cantidad de datos a
 enviar y así reducir el número de
\end_layout

\begin_layout Standard
mensajes y el costo de comunicación.
\end_layout

\begin_layout Standard
Se puede intentar replicar cómputos y/o datos para reducir los requerimientos
 de comunicación.
 
\end_layout

\begin_layout Standard
Aspectos a considerar en esta etapa:
\end_layout

\begin_layout Standard
-chequear si la agrupación redujo los costos de comunicación.
\end_layout

\begin_layout Standard
-si se han replicado cómputos y/o datos, se debe verificar que los beneficios
 son superiores
\end_layout

\begin_layout Standard
a los costos.
\end_layout

\begin_layout Standard
-se debe verificar que las tareas resultantes tengan costos de computo y
 comunicación
\end_layout

\begin_layout Standard
similares.
\end_layout

\begin_layout Standard
- revisar si el número de tareas es extensible con el tamaño del problema.
\end_layout

\begin_layout Standard
-si el agrupamiento ha reducido las oportunidades de ejecución concurrente,
 se debe
\end_layout

\begin_layout Standard
verificar que aun hay suficiente concurrencia y posiblemente considerar
 diseños
\end_layout

\begin_layout Standard
alternativos.
\end_layout

\begin_layout Standard
-nalizar si es posible reducir aun más el número de tareas sin introducir
 desbalances de
\end_layout

\begin_layout Standard
cargas o reducir la extensibilidad.
\end_layout

\begin_layout Standard
ASIGNACION
\end_layout

\begin_layout Standard
En este proceso se determina en que procesador se ejecutará cada tarea.
 Este problema
\end_layout

\begin_layout Standard
no se presenta en máquinas de memoria compartida tipo UMA.
 Estas proveen asignación
\end_layout

\begin_layout Standard
dinámica de procesos y los procesos que necesitan de un CPU están en una
 cola de procesos
\end_layout

\begin_layout Standard
listos.
 Cada procesador tiene acceso a esta cola y puede correr el próximo proceso.
 No
\end_layout

\begin_layout Standard
consideraremos más este caso.
\end_layout

\begin_layout Standard
Para el momento no hay mecanismos generales de asignación de tareas para
 máquinas
\end_layout

\begin_layout Standard
distribuidas.
 Esto continúa siendo un problema difícil y que debe ser atacado explícitamente
 a la
\end_layout

\begin_layout Standard
hora de diseñar algoritmos paralelos.
\end_layout

\begin_layout Standard
La asignación de tareas puede ser estática o dinámica.
 En la asignación estática, las tareas
\end_layout

\begin_layout Standard
son asignadas a un procesador al comienzo de la ejecución del algoritmo
 paralelo y corren ahí
\end_layout

\begin_layout Standard
hasta el final.
 La asignación estática en ciertos casos puede resultar en un tiempo de
 ejecución
\end_layout

\begin_layout Standard
menor respecto a asignaciones dinámicas y también puede reducir el costo
 de creación de
\end_layout

\begin_layout Standard
procesos, sincronización y terminación.
\end_layout

\begin_layout Standard
En la asignación dinámica se hacen cambios en la distribución de las tareas
 entre los
\end_layout

\begin_layout Standard
procesadores a tiempo de ejecución, o sea, hay migración de tareas a tiempo
 de ejecución.
 Esto
\end_layout

\begin_layout Standard
es con el fin de balancear la carga del sistema y reducir el tiempo de ejecución.
 Sin embargo, el
\end_layout

\begin_layout Standard
costo de balanceo puede ser significativo y por ende incrementar el tiempo
 de ejecución.
 Entre
\end_layout

\begin_layout Standard
los algoritmos de balanceo de carga están los siguientes:
\end_layout

\begin_layout Standard
Balanceo centralizado: un nodo ejecuta el algoritmo y mantiene el estado
 global del sistema.
\end_layout

\begin_layout Standard
Este método no es extensible a problemas más grandes ya que el nodo encargado
 del balanceo se
\end_layout

\begin_layout Standard
convierte en un cuello de botella.
\end_layout

\begin_layout Standard
Balanceo completamente distribuido: cada procesador mantiene su propia visión
 del
\end_layout

\begin_layout Standard
sistema intercambiando información con sus vecinos y así hacer cambios locales.
 El costo de
\end_layout

\begin_layout Standard
balanceo se reduce pero no es óptimo debido a que solo se dispone de información
 parcial.
\end_layout

\begin_layout Standard
Balanceo semi-distribuido: divide los procesadores en regiones, cada una
 con un algoritmo
\end_layout

\begin_layout Standard
centralizado local.
 Otro algoritmo balancea la carga entre las regiones.
\end_layout

\begin_layout Standard
El balanceo puede ser iniciado por envío o recibimiento.
 Si es balanceo iniciado por envío,
\end_layout

\begin_layout Standard
un procesador con mucha carga envía trabajo a otros.
 Si es balanceo iniciado por recibimiento,
\end_layout

\begin_layout Standard
un procesador con poca carga solicita trabajo de otros.
 Si la carga por procesador es baja o
\end_layout

\begin_layout Standard
mediana, es mejor el balanceo iniciado por envío.
 Si la carga es alta se debe usar balanceo
\end_layout

\begin_layout Standard
iniciado por recibimiento.
 De lo contrario, en ambos casos, se puede producir una fuerte
\end_layout

\begin_layout Standard
migración innecesaria de tareas.
\end_layout

\begin_layout Section*
Relevamiento de tecnologias
\end_layout

\begin_layout Section
SSI: OpenMosix, OpenSSI y Kerrighed
\end_layout

\begin_layout Standard
A diferencia de un cluster tradicional en SSI todas las computadoras vinculadas
 dependen de un sistema operativo identico en común.
\end_layout

\begin_layout Standard
Un SSI oculta la naturaleza heterogénea y distribuida de los recursos, y
 los presenta a los usuarios y a las aplicaciones como un recurso computacional
 unificado y sencillo.
 Una de las metas que mas diferencia un SSI de un cluster o un grid tradicional
 es su completa transparencia en la gestión de recursos.
 Es debido a esta completa transparencia que es posible migrar procesos
 de un nodo a otro.
 Tampoco es necesario programacion adicional para beneficiarse del paralelismo,
 de esta manera no es necesario re-escribir programas utilizando bibliotecas
 como PVM (Parallel Virtual Machine) o MPI (Message Passing Interface).
\end_layout

\begin_layout Standard
A pesar de las ventajas presentadas por los SSI a nivel de manejo de recursos,
 no creemos que se ajusten a los requerimientos planteados principalmente
 debido a que el cluster ya se encuentra en funcionamiento siguiendo un
 paradigma estilo beowulf mas clasico.
 Quizas sea interesante construir un pequeño cluster SSI e investigar mas
 a fondo este tipo de tecnologias de manera de poder compara ambas soluciones.
\end_layout

\begin_layout Section
Condor
\end_layout

\begin_layout Section
OpenPBS
\end_layout

\begin_layout Section
SLURM
\end_layout

\begin_layout Section
Sun grid engine 6.1
\end_layout

\begin_layout Paragraph
Sun Grid Engine es un software para administración de recursos distribuídos
 que dinámicamente asocia requerimientos de hardware y software de los usuarios
 con los recursos disponibles en la red (generalmente heterogéneos) de acuerdo
 a políticas predefinidas.
\end_layout

\begin_layout Standard
Sun Grid Engine actúa como el sistema nervioso central de un cluster de
 computadoras conectadas.
 A partir de algunos demoniso, el Sun Grid Master supervisa todos los recursos
 de la red 
\lang english
para permitir control completo y alcanzar la utilización óptima de los mismos.
\end_layout

\begin_layout Paragraph
Sun Grid Engine fue desarrollado como realce de Codine de Genias GmbH y
 Grisdware inc, de acuerdo a requerimientos de varios clientes tales como
 el laboratorio de investigación del ejército de Aberdeen y BMW.
 Con Sun Grid Engine, el uso medio de los recursos aumentó de menos del
 50% a más del 90% en ambos ambientes.
\end_layout

\begin_layout Paragraph
Sun grid Engine reune el poder de cálculo disponible en granjas de computadoras
 dedicadas, servidores conectados y computadoras de escritorio, y las presenta
 desde un único punto de acceso para el usuario que necesita ciclos de cómputo.
 Esto se logra distribuyendo la carga de trabajo entre los sistemas disponibles,
 aumentando la productividad de máquinas y del uso de licencias, mientras
 se maximiza el número de trabajos que pueden ser completados.
\end_layout

\begin_layout Paragraph
Los requerimientos de hardware son mínimos (100 MB de memoria disponible
 y 500MB de disco) y soporta la mayoría de los sistemas operativos populares.
 Soporta las plataformas SPARC Ultra III, SPARC Ultra IV, AMD64, x86 y Mac.
\end_layout

\begin_layout Standard
Permite control de usuarios, limitando tanto el número máximo de trabajos
 por usuario, grupo y proyecto, como recursos tales como colas, hosts, memoria
 y licencias de software.
\end_layout

\begin_layout Standard
Los recursos requeridos por cada trabajo se pueden indicar mediante expresiones
 lógicas, (por ejemplo, un usuario puede requerir que un trabajo corra en
 un host que cumpla la condición 
\begin_inset Quotes eld
\end_inset

Solaris o Linux pero no Linux en IA64
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Paragraph
Distributed Resource Management Application API (DRMAA) es un conjunto de
 APIs standard desarrollado por Global Grid Forum para application builders,
 portal builders e ISV's.
 La versión 6.1 soporta los últimos C y Java bindings de DRMAA 1.0.
 Adicionalmente, es provisto de compatibilidad hacia atrás para: DRMAA 0.5
 Java binding y DRMAA 0.95 C binding.
\end_layout

\begin_layout Paragraph
Guarda la información referida a la cuenta de cada trabajo en una base de
 datos relacional (soportando Oracle, MySQL y PostgreSQL).
\end_layout

\begin_layout Paragraph
Maneja aplicaciones paralelas (MPI o PVM habilitados) a travez de una interface
 dedicada.
 
\end_layout

\begin_layout Paragraph
Permite la distribución de recursos a equipos o departamentos, por ejemplo
 proporcionalmente a su contribución económica.
 
\end_layout

\begin_layout Paragraph
El código muestra una complejidad alta, además el sistema es integrado,
 no muestra facilidades a la hora de dividirlo en subsistemas.
 Estas carácterísticas hacen que en algunos aspectos se vea como un sistema
 cerrado, que no permite adaptaciones en caso de realidades no abarcadas
 por el mismo.
\end_layout

\begin_layout Subsection
Resumen
\end_layout

\begin_layout Standard
Sun grid engine 6.1 provee la mayoría de las funcionalidades requeridas para
 el proyecto.
 Por otro lado, tiene como desventaja el ser un único gran paquete indivisible,
 lo cual hace muy difícil cualquier adaptación o modificación de su funcionamien
to.
 
\end_layout

\begin_layout Standard
Si nos basamos en la historia reciente del producto, en una versión anterior
 se encontró un problema de seguridad (un salto en las restricciones) y
 quienes lo estaban utilizando en ese momento descartaron la idea de buscar
 el problema y solucionarlo, limitándose sólamente a esperar que los creadores
 de la herramienta lo corrigieran.
 
\end_layout

\begin_layout Standard
Esto se debe a que la herramienta cuenta con un código muy complejo y dificil
 de modificar, lo que lo coloca como una opción poco vialble para ser utilizada
 dentro del proyecto propuesto.
\end_layout

\begin_layout Section
TORQUE
\end_layout

\begin_layout Standard
TORQUE (Terascale Open-source Resource and QUEue manager) se trata de un
 'fork' open-source de la versión 2.3.12 de OpenPBS mantenido por Cluster
 Resources.
 
\end_layout

\begin_layout Standard
Incorpora muchas mejoras con respecto al proyecto PBS original provistas
 por NCSA (National Center of Supercomputing Applications), OSC (Ohio Supercompu
ter Center), Sandia, PNNL (Pacific Northwest National Laboratory), y otros
 centros HPC junto con las mejoras desarrolladas por Cluster Resources.
\end_layout

\begin_layout Subsection
Requerimientos
\end_layout

\begin_layout Standard
Plataformas soportadas:
\end_layout

\begin_layout Itemize
Linux
\end_layout

\begin_layout Itemize
UNIX
\end_layout

\begin_layout Subsection
Características generales
\end_layout

\begin_layout Standard
Los trabajos son manejados en el cluster de la misma manera que lo hace
 OpenPBS.
 Cuenta con un conjunto de colas de trabajos que definen propiedades generales
 para los trabajos que contienen (p.ej.
 recursos disponibles, etc.) y al ingresar un trabajo al sistema, este debe
 especificar características especificas de si mismo que ayuden al sistema
 a planificar la ejecución del mismo (p.ej.
 nodos a utilizar, memoria máxima a utilizar, etc.).
 Con esta información (los datos de la cola a la que pertenece el trabajo
 y los datos específicos del trabajo en cuestión) el scheduler decide el
 momento en que un trabajo comienza a ejecutarse y con que recursos cuenta
 en cada instante.
\end_layout

\begin_layout Standard
Existen tres tipos de nodos en el sistema:
\end_layout

\begin_layout Itemize
Nodo Maestro
\end_layout

\begin_deeper
\begin_layout Standard
Es necesario que exista un nodo maestro en el sistema, en este nodo se debe
 ejecutar pbs_server.
 Dependiendo del sistema este nodo maestro puede encontrarse dedicado únicamente
 a este rol o compartir otros roles.
\end_layout

\end_deeper
\begin_layout Itemize
Nodos Interactivos
\end_layout

\begin_deeper
\begin_layout Standard
Los nodos interactivos proveen un punto de entrada al sistema para los usuarios.
 Es en estos nodos en que los usuarios puede ingresar tareas al sistema
 y monitorear su progreso.
 Estos nodos deben tener comandos como qsub, qhold, etc disponibles.
\end_layout

\end_deeper
\begin_layout Itemize
Nodos de computo
\end_layout

\begin_deeper
\begin_layout Standard
Estos son los responsables de la ejecución de los trabajos encolados en
 el sistema.
 En cada uno de estos nodos debe ejecutarse pbs_mom (Machine Oriented Mini-serve
r) que es el encargado de iniciar, detener y manejar los trabajos encolados.
\end_layout

\end_deeper
\begin_layout Standard
Torque cuenta con una interfaz de usuario de linea de comando (CLI) como
 principal manera de interactuar con el sistema, también cuenta con una
 interfaz gráfica para X-Windows y una biblioteca para desarrollo en C.
 Es posible utilizar bibliotecas desarrolladas por 3eros para otros lenguajes
 (p.ej.
 Perl o Python).
\end_layout

\begin_layout Subsection
Bibliotecas para programación paralela
\end_layout

\begin_layout Subsubsection
MPI (Message Passing Interface)
\end_layout

\begin_layout Standard
El soporte para bibliotecas tipo MPI se encuentra integrado en TORQUE, el
 sistema puede correr con cualquier implementación de MPI.
 Particularmente para MPICH existe un reemplazo para el script mpirun (mpirun
 se encuentra incluido en el paquete de mpich) llamado mpiexec.
 Mpiexec es usado para inicializar trabajos paralelos dentro de un sistema
 PBS.
 Los recursos utilizados por procesos paralelos son correctamente registrados
 cuando se utiliza mpiexec y se reporta su uso en el log del PBS, a diferencia
 de lo que sucede cuando se utiliza el script mpirun original.
\end_layout

\begin_layout Subsubsection
PVM (Parallel Virtual Machine)
\end_layout

\begin_layout Standard
Una de las principales desventajas de TORQUE es la dificultad que plantea
 el uso de la biblioteca PVM.
 A diferencia de MPI, el soporte para PVM no se encuentra integrado al sistema
 por lo que debe manejarse cuidadosamente.
 Para ejecutar una aplicación paralela utilizando PVM se debe iniciar el
 demonio pvmd en el script del trabajo, configurar los nodos esclavos para
 luego iniciar la ejecución del trabajo (todo esto puede realizarse utilizando
 mpiexec).
\end_layout

\begin_layout Subsubsection
Interfaz de programación
\end_layout

\begin_layout Standard
Torque soporta el estandar Distributed Resource Management Application API
 (DRMAA) al igual que la mayoria de los manejadores de recursos.
\end_layout

\begin_layout Subsection
Maui
\end_layout

\begin_layout Standard
TORQUE contiene la lógica necesaria para llevar a cabo la planificación
 de trabajos, pero se trata de una lógica muy simple que no resulta adecuada
 para un ambiente de producción.
 Básicamente el planificador que se encuentra incorporado a TORQUE maneja
 los trabajos como una cola FIFO (First-In First-Out).
 Para mejorar esto se utiliza Maui, una aplicación especializada en la planifica
ción de trabajos.
\end_layout

\begin_layout Standard
Maui se enfoca en la planificación de trabajos y deja la problemática de
 iniciar los trabajos y la interacción con los usuarios a los manejadores
 de recursos (distributed resource managers, DRM) como OpenPBS, TORQUE,
 SGE, etc.
\end_layout

\begin_layout Subsubsection
Requerimientos
\end_layout

\begin_layout Itemize
Hardware
\end_layout

\begin_deeper
\begin_layout Itemize
20-50 MB de RAM (para clusters de hasta 10 teraflops)
\end_layout

\begin_layout Itemize
>20 MB de disco duro para los fuentes, binarios, estadísticas y archivos
 de logs
\end_layout

\end_deeper
\begin_layout Itemize
Plataformas soportadas
\end_layout

\begin_deeper
\begin_layout Itemize
Linux
\end_layout

\begin_layout Itemize
AIX
\end_layout

\begin_layout Itemize
OSF/Tru-64
\end_layout

\begin_layout Itemize
Solaris
\end_layout

\begin_layout Itemize
HP-UX
\end_layout

\begin_layout Itemize
IRIX
\end_layout

\begin_layout Itemize
FreeBSD
\end_layout

\begin_layout Itemize
Other UNIX platforms
\end_layout

\end_deeper
\begin_layout Itemize
DRM soportados
\end_layout

\begin_deeper
\begin_layout Itemize
OpenPBS
\end_layout

\begin_layout Itemize
'Scalable' Open PBS
\end_layout

\begin_layout Itemize
PBSPro
\end_layout

\begin_layout Itemize
Sun Grid Engine (SGE)
\end_layout

\begin_layout Itemize
SGE Enterprise Edition
\end_layout

\begin_layout Itemize
LoadLeveler
\end_layout

\begin_layout Itemize
LSF
\end_layout

\begin_layout Itemize
BProc/Scyld
\end_layout

\begin_layout Itemize
Scalable System Software (SSSRM)
\end_layout

\end_deeper
\begin_layout Subsubsection
Características generales
\end_layout

\begin_layout Standard
El algoritmo de planificación de Maui soporta fairness, preemption, backfill,
 etc.
 y tiene una interfaz para la interacción con un allocation management externo.
 Un allocation manager (también conocido como allocation bank o cpu bank)
 funciona como un banco en el cual la moneda son los recursos del sistema
 (p.ej.
 procesadores, memoria, etc.) autorizando a los trabajos cierta cantidad
 de recursos.
\end_layout

\begin_layout Paragraph
Backfill
\end_layout

\begin_layout Standard
Backfill es un acercamiento en la planificación que permite ejecutar algunos
 trabajos 'desordenadamente' siempre y cuando estos no retrasen los trabajos
 de prioridad superior de la cola.
 Para determinar si un trabajo será retrasado, cada trabajo debe proveer
 una estimación de cuánto tiempo necesitará para su ejecución.
 Esta estimación, conocida como límite wallclock, es una valoración del
 tiempo desde el comienzo del trabajo hasta su final.
 Es a menudo sabio sobrestimar levemente este límite porque el planificador
 se puede configurar para matar a los trabajos que exceden sus límites del
 wallclock.
 Sin embargo, la sobrestimación demasiado grande del tiempo del wallclock
 de un trabajo evitará que el planificador pueda optimizar correctamente
 la cola de trabajo.
 Cuanto más exacto el límite del wallclock, mayor sera la posibilidad de
 que Maui encuentre agujeros en la planificación para comenzar a ejecutar
 su trabajo con mayor anticipación.
 
\end_layout

\begin_layout Paragraph
Gerenciamiento de asignación
\end_layout

\begin_layout Standard
Maui posee interfaces para sistemas de gerenciamiento de asignación tales
 como Gold de PNNL.
 Estos sistemas permiten que a cada usuario le sea asignada una porción
 de los recursos totales de cálculo disponibles en el sistema.
 Estos sistemas trabajan asociando a cada usuario a unas o más cuentas.
 Cuando se envía un trabajo, el usuario especifica a que cuenta se debe
 cargar los recursos consumidos por el trabajo.
\end_layout

\begin_layout Paragraph
Reservas anticipadas
\end_layout

\begin_layout Standard
Las reservas anticipadas permiten que un sitio disponga ciertos recursos
 a un lado para el uso específico de de ciertas aplicaciones durante cierto
 tiempo.
 El acceso a una reserva dada es controlado por un Access Control List (ACL)
 que determina quién puede utilizar los recursos reservados.
 Es importante observar que mientras que un ACL permite que trabajos particulare
s utilicen recursos reservados, no fuerzan al trabajo a utilizar estos recursos.
 Maui procurará utilizar la mejor combinación posible de recursos disponibles
 sean éstos reservados o no.
 Maui puede ser configurado para que ciertos trabajos sean restringidos
 y que funcionen utilizando solamente recursos reservados, aplicando restriccion
es a nivel de trabajo o especificando ciertas restricciones especiales de
 QoS.
 
\end_layout

\begin_layout Paragraph
Quality of Service (QoS)
\end_layout

\begin_layout Standard
Las funciones de QoS permiten otorgar ciertos privilegios especiales a usuarios,
 estos beneficios pueden incluir acceso a recursos adicionales, exclusiones
 de determinadas políticas, acceso a capacidades especiales, y mejoras en
 la priorizacion de trabajos.
\end_layout

\begin_layout Paragraph
Faireshare
\end_layout

\begin_layout Standard
Este componente permite favorecer trabajos en base al uso histórico a corto
 plazo.
 Es posible así ajustar la prioridad de un trabajo dependiendo de la utilización
 porcentual del sistema de usuarios, grupos, o QoS.
 Dada una ventana de tiempo determinado sobre la cual se evalúa la utilización
 de recursos del sistema se determina si esta siendo mantenido un cierto
 balanceo o no.
\end_layout

\begin_layout Subsubsection
Interfaz de programación
\end_layout

\begin_layout Itemize
Interfaz de extensión (Extension Interface)
\end_layout

\begin_deeper
\begin_layout Itemize
Esta interfaz permite que bibliotecas externas sean 'linkeadas' al servidor
 de Maui brindando acceso a todos los datos y objetos utilizados por el
 planificador.
 Además, permite que estas bibliotecas realicen override de las principales
 funciones de Maui.
\end_layout

\end_deeper
\begin_layout Itemize
Interfaz local
\end_layout

\begin_deeper
\begin_layout Itemize
Se trata de una interfaz en C que permite el desarrollo de nuevos algoritmos.
\end_layout

\end_deeper
\begin_layout Subsubsection
Estadísticas
\end_layout

\begin_layout Standard
Maui almacena tres diferentes clases de estadísticas:
\end_layout

\begin_layout Itemize
Estadísticas de tiempo real
\end_layout

\begin_deeper
\begin_layout Itemize
Estas estadísticas son mantenidas en memoria y pueden ser consultadas mediante
 comandos.
 El comando 'showstats' provee información detallada por usuario, por grupo,
 por cuenta o por nodo.
 Además en cualquier momento estas estadísticas pueden resetearse utilizando
 el comando 'resetstats'.
 
\end_layout

\end_deeper
\begin_layout Itemize
Histórico
\end_layout

\begin_deeper
\begin_layout Itemize
Estas estadísticas pueden ser obtenidas para un lapso de tiempo, un tipo
 de trabajo y/o una porción de recursos utilizando el comando 'profiler'.
 Este comando trabaja con la traza de información detallada de un trabajo,
 que es guardada al dar por finalizado un trabajo.
 Estas trazas son almacenadas en el directorio configurado por el parámetro
 STATDIR (por defecto $(MAUIHOMEDIR)/stats) en archivos utilizando el formato
 WWW_MMM_DD_YYYY (p.
 ej.
 Mon_Jul_16_2001), siendo esta fecha la fecha de finalización del trabajo.
 La traza de un trabajo se almacena en texto plano utilizando espacios como
 separadores, por lo que puede ser analizado directamente con cualquier
 editor de texto.
 
\end_layout

\end_deeper
\begin_layout Itemize
Fairshare
\end_layout

\begin_deeper
\begin_layout Itemize
Este tipo de estadísticas son mantenidas sin importar si fairshare se encuentra
 habilitado.
 Al igual que las trazas de los trabajos, estas son almacenadas en archivos
 utilizando texto plano en el directorio configurado por el parámetro STATDIR
 y utilizando el formato FS.<EPOCHTIME> (p.ej., FS.982713600) por cada ventana
 de fairshare.
 Se puede obtener información de estos archivos utilizando el comando 'diagnose
 -f'.
 
\end_layout

\end_deeper
\begin_layout Subsection
Gold
\end_layout

\begin_layout Standard
Gold es un sistema de contaduría open-source desarrollado en PNNL bajo el
 proyecto Scalable Systems Software (SSS) que lleva registro y maneja el
 uso de recursos en clusters de alto desempeño.
 Actúa de la misma forma que un banco en el que son depositados créditos
 en cuentas, estos créditos representan recursos en el sistema.
 A medida que se finalizan trabajos o que son consumidos recursos en el
 sistema se debitan créditos de sus respectivas cuentas.
\end_layout

\begin_layout Standard
Es posible realizar operaciones como depósitos, retiros, transferencias
 o reembolsos sobre las cuentas del sistema, además, provee listados de
 balances a usuarios y administradores.
\end_layout

\begin_layout Subsubsection
Características generales
\end_layout

\begin_layout Paragraph
Reservas
\end_layout

\begin_layout Standard
Previo al inicio de un trabajo se realiza una estimación del total de recursos
 que este consumirá, en base a esta estimación se reservan créditos en la
 cuenta correspondiente.
 Estos créditos 'reservados' se debitan una vez que el trabajo es terminado,
 de esta manera se evita que un proyecto consuma mas recursos de los que
 tiene asignados.
\end_layout

\begin_layout Paragraph
Vencimiento de créditos
\end_layout

\begin_layout Standard
Puede especificarse un lapso de validez a los créditos en el sistema, permitiend
o que se implemente una política de use-it-or-lose-it previniendo el uso
 exhaustivo de créditos acumulados y estableciendo ciclos a un proyecto.
\end_layout

\begin_layout Paragraph
Interfaz web
\end_layout

\begin_layout Standard
Permitiendo acceso remoto a usuarios y administradores.
 
\end_layout

\begin_layout Paragraph
Interfaz de programación
\end_layout

\begin_layout Standard
Existen diferentes formas de integrar Gold al sistema: Perl API, Java API
 o directamente utilizando el protocolo SSSRMAP (basado en XML).
\end_layout

\begin_layout Subsection
Casos de estudio
\end_layout

\begin_layout Paragraph
Oregon State University, Laboratorio de fisica
\end_layout

\begin_layout Standard
Actualmente cuenta con 34 computadoras Dell Optiplex GX620's con procesadores
 Intel Pentium D 830 (3.0 GHz) y 1 GB of RAM con sistema operativo Suse GNU/Linux
 10.1 64-bit.
\end_layout

\begin_layout Standard
Estas computadoras cuentan con compiladores Intel para C, C++ y Fortran
 compilers, la bilbioteca Math Kernel Library (cluster edition).
 Para el procesamiento paralelo se utilizan las bibliotecas MPI.
 El cluster utiliza Torque como manejador de recursos, y desde el 4 de febrero
 del 2007 se utiliza Maui para la planificacion de tareas.
 
\end_layout

\begin_layout Paragraph
University of Glasgow
\end_layout

\begin_layout Standard
El cluster utiliza Torque y Maui sobre Redhat Enterprise GNU/Linux 3.
 Actualmente cuenta con 60 nodos de procesamiento disponibles, cada uno
 con procesadores dual opteron 248 y 2GB RAM.
 Ademas de los componentes estandares de incluidos en la distribucion (p.ej.
 compilador gcc, g++, g77, etc.) se han instalado las bibliotecas mpich 1.2.6
 (MPI) para procesamiento paralelo.
\end_layout

\begin_layout Paragraph
University of Heidelberg
\end_layout

\begin_layout Standard
El cluster fue instalado a principios de 2002 y consisten de 512 procesadores
 AMD Athlon MP, instalados en 256 nodos de procesamiento SMP con 2 GB de
 memoria RAM cada uno.
 Los procesadores funcionan cada uno a 1.4GHz y alcanzan un maximo teorico
 de 2.4 billiones de operaciones de punto flotante por segundo (Gflops).
 El sistema total indica un maximo teorico de desempeño de mas de 1.4 Teraflops.
 Las primeras mediciones de desempeño utilizando Linpack Benchmark mostraron
 una rendimiento de 825 Gflops, ubicando el cluster en la posicion 35va
 del top 500 de supercomputadoras del mundo en Junio del 2002.
\end_layout

\begin_layout Standard
Como base del cluster se utiliza un sistema Debian GNU/Linux.
 Para procesamiento paralelo se utiliza la biblioteca mpich (MPI) y como
 manejador de recursos se utiliza Torque.
 Por sobre Torque se encuentra instalado Moab, el sucesor de Maui.
 
\end_layout

\begin_layout Paragraph
Stony Brook University
\end_layout

\begin_layout Standard
El cluster tiene 235 nodos de procesamiento dual (470 procesadores individuales).
 Cada procesador es un Dell Pentium IV Xeon de 3.4Ghz con 2GB de memoria
 RAM y 40GB de disco duro.
 Las computadores operan con Debian GNU/Linux, utilizan Torque y Maui para
 manejar los trabajos y la version 1.4 de MPI para el procesamiento paralelo.
\end_layout

\begin_layout Paragraph
Dansk Center for Scientific Computing
\end_layout

\begin_layout Standard
El cluster consta de 200 nodos Dell PowerEdge 1950 1U con dos procesadores
 Intel Woodcrest de 2,66Ghz.
 De estos 200 nodos 160 cuentan con 4 GB de RAM y 40 cuentan con 8GB de
 RAM.
 Como sistema se utiliza OpenSuSE GNU/Linux 10.1 con kernel 2.6 y para la
 planificacion y manejo de trabajos de utiliza Torque 2.1.2 y MAUI 3.2.6.
 Para el procesamiento paralelo se disponen de bibliotecas tanto de tipo
 MPI como de tipo PVM.
\end_layout

\begin_layout Subsection
Conclusiones
\end_layout

\begin_layout Standard
Torque junto con Maui y Gold satisfacen los requerimientos planteados para
 el proyecto.
 Su consumo de recursos del cluster es relativamente bajo, se adapta muy
 bien al manejo de clusters pequeños y posee un manejo eficiente de trabajos
 paralelos (sobre todo si estos son homogéneos).
 
\end_layout

\begin_layout Standard
Pero existen ciertos aspectos en los que Torque no se desempeña de la mejor
 manera.
 Si bien se encuentra integrado el soporte para bibliotecas tipo MPI, no
 existe un correcto soporte para PVM.
 Además existen ciertas carencias en sus funcionalidades de Resource Management;
 p.
 ej.
 no es capaz de realizar CPU Harvesting o migración de procesos.
 
\end_layout

\begin_layout Standard
Si bien existe un buen soporte por parte de la comunidad de usuarios de
 Torque y Maui por medio de listas de correo, la documentación disponible
 en el sitio web se encuentra incompleta en algunos aspectos (p.
 ej.
 PVM, interfaces de programación, etc.).
 Como es de esperar esta documentación se encuentra mas completa para Moab
 (la versión comercial de Maui), y si bien son productos diferentes son
 similares en muchos aspectos.
\end_layout

\begin_layout Bibliography

\bibitem [1]{key-1}
B.
 Radic, E.
 Imamagic: Benchmarking the Performance of JMS on Computer Clusters, CARNet
 Users' Conference, 28.
 9.
 2004.
\end_layout

\begin_layout Bibliography

\bibitem [2]{key-2}
E.
 Imamagic, B.
 Radic, D.
 Dobrenic: Job Management Systems Analysis, CARNet Users' Conference, 28.
 9.
 2004.
\end_layout

\begin_layout Bibliography

\bibitem [3]{key-3}
Maui Cluster Scheduler, Cluster Resources.
 http://www.clusterresources.com/products/maui/
\end_layout

\begin_layout Bibliography

\bibitem [4]{key-4}
Torque Resource Manager, Cluster Resources.
 http://www.clusterresources.com/products/torque/
\end_layout

\begin_layout Bibliography

\bibitem [5]{key-5}
William Gropp, Ewing Lusk and Thomas Sterling.
 Beowulf Cluster Computing with Linux, Second Edition.
 The MIT press, 2003 ISBN:0262692929 
\end_layout

\begin_layout Bibliography

\bibitem [6]{key-6}
M.
 Michels, W.
 Borremans: Clustering with openMosix, University of Amsterdam, February
 2005.
\end_layout

\begin_layout Bibliography

\bibitem {key-7}
http://es.wikipedia.org/wiki/Single_System_Image
\end_layout

\begin_layout Bibliography

\bibitem {key-8}
Dansk Center for Scientific Computing, http://www.dcsc.sdu.dk/
\end_layout

\begin_layout Bibliography

\bibitem {key-9}
Stony Brook University, http://www.sunysb.edu/seawulfcluster/
\end_layout

\begin_layout Bibliography

\bibitem {key-10}
University of Heidelberg, http://helics.uni-hd.de/
\end_layout

\begin_layout Bibliography

\bibitem {key-11}
University of Glasgow, http://www.gla.ac.uk/services/it/whatwedo/computecluster/
\end_layout

\begin_layout Bibliography

\bibitem {key-12}
Oregon State University, Laboratorio de fisica, http://physics.oregonstate.edu/~el
serj/support/cluster_use.php
\end_layout

\begin_layout Bibliography

\bibitem {key-13}
Distributed Resource Management Application API (DRMAA), http://en.wikipedia.org/w
iki/DRMAA/
\end_layout

\end_body
\end_document
