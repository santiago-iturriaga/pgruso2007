#LyX 1.4.3 created this file. For more info see http://www.lyx.org/
\lyxformat 245
\begin_document
\begin_header
\textclass article
\language spanish
\inputencoding auto
\fontscheme default
\graphics default
\paperfontsize default
\spacing single
\papersize default
\use_geometry false
\use_amsmath 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language swedish
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\end_header

\begin_body

\begin_layout Title
Estado del Arte
\end_layout

\begin_layout Title
Cluster de Computadores de Alto Desempeño con Acceso Remoto
\end_layout

\begin_layout Title
Carrera de Ingeniería en Computación
\end_layout

\begin_layout Title
Facultad de Ingeniería
\end_layout

\begin_layout Author
Santiago Iturriaga, Paulo Maya, Damian Pintos
\end_layout

\begin_layout Section*
Resumen
\end_layout

\begin_layout Section*
Introducción
\end_layout

\begin_layout Paragraph

\lang english
En el principio de la historia de las computadoras, se tenía la idea de
 que un servidor tenia que ser necesariamente una máquina, conocida como
 super computador o mainframe, que contaba con un súper procesador que daba
 servicio a varias terminales tontas.
 Asi, para asegurar mayor poder de cálculo es necesario tener un procesador
 más poderoso.
 De este modo, cuando los requerimientos de procesamiento aumentan, es necesario
, en la mayoría de los casos, cambiar la super computadora actual por una
 nueva mejor, pasando a subutilizar la super computadora anterior, con un
 alto costo asociado.
\end_layout

\begin_layout Paragraph

\lang english
Esta idea encontró oposición en 1994, cuando Donald Becker y Thomas Sterling
 crearon el proyecto Beowulf, en el que lograron un elevado nivel de procesamien
to poniendo a trabajar varias computadoras en paralelo con procesadores
 16 DX4, interconectadas en una red de 10 Mbit Ethernet.
\end_layout

\begin_layout Paragraph

\lang english
A partir de ese momento, y debido al éxito alcanzado en el proyecto, muchos
 otros proyectos siguieron la investigaci´n del tema, bajo la premisa de
 convertir hardware de relativo bajo costo en clusters que logren equiparar
 o superar la performance alcanzada por las supercomputadoras.
\end_layout

\begin_layout Paragraph

\lang english
Actualmente, el cluster de computadoras se ha utilizado para varias y diferentes
 tareas como Data Mining, Servidores de Archivos, Servidores de Base de
 Datos, Servidores Web, Simuladores de Vuelo, Graphics Rendering, Modeladores
 Climáticos o incluso para CD Ripping alcanzando velocidades realmente altas.
\end_layout

\begin_layout Paragraph

\lang english
La idea es simplemente reunir el poder de cómputo de una serie de nodos
 para proveer alta escabilidad, poder de cómputo, o construir redundancia
 y de esta manera proveer alta disponibilidad.
 Entonces en vez de un simple cliente haciendo peticiones de uno o más servidore
s, los cluster utilizan múltiples máquinas para proveer un ambiente más
 poderoso de cómputo, a través de una sola imagen de sistema, esto es que
 el procesamiento sea visible al usuario como una sola unidad, aunque se
 componga de muchas computadoras trabajando en paralelo para el mismo fin.
\end_layout

\begin_layout Paragraph

\lang english
El alto poder de procesamiento de varias computadoras trabajando en paralelo,
 (o cluster de computadoras), se perfila como una solución viable a las
 empresas, universidades y escuelas a un bajo costo de implementación.
 
\end_layout

\begin_layout Paragraph

\lang english
Con el tiempo, la investigación se ha ido especializando en áreas específicas.
 El Cluster de Alto Rendimiento se aplica principalmente a aplicaciones
 científicas mientras que el Cluster de Alta Disponibilidad y Cluster de
 Balanceo de Carga son más utilizado en el ambiente de negocios.
 En este trabajo nos enfocaremos en el primero.
\end_layout

\begin_layout Subsection*

\lang english
Trabajos previos
\end_layout

\begin_layout Standard

\lang english
No son pocos los trabajos realizados sobre investigación, desarrollo, construcci
ón y aplicaciones de clusters, pero es relativamente poca la bibliografía
 asociada, dado que el tema ha ganado auge principalmente en la última década,
 más alla de que la idea no es nueva.
 Se remontan estudios desde 1970, pero se puede decir que IBM fue la primera
 en establecer una teoría formal con respecto a los clusters.
 Este trabajo fue realizado por Gene Myron Amdahl (arquitecto de computadoras
 estadounidense noruego), que en el momento de realizar el estudio donde
 demostró la ley de Amdal (llamada axial en honor a su nombre).
 Esta ley se usa para encontrar la máxima mejora esperada a un sistema en
 su totalidad, cuando solo parte del sistema se mejora.
 Esto es usado muy a menudo en computación paralela para predecir la máxima
 mejora de velocidad usando múltiples procesadores.
 
\end_layout

\begin_layout Itemize

\lang english
Ley de amdahl: 
\begin_inset Quotes eld
\end_inset

La mejora obtenida en el rendimiento de un sistema debido a la alteración
 de uno de sus componentes está limitada por la fracción de tiempo que se
 utiliza dicho componente
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\lang english
\begin_inset Formula $A=F_{a}*((1-F_{m})+F_{m}/A_{m}$
\end_inset

) donde 
\begin_inset Formula $F_{a}$
\end_inset

 es el tiempo de ejecución antiguo, 
\begin_inset Formula $A$
\end_inset

 es la aceleración obtenida, 
\begin_inset Formula $A_{m}$
\end_inset

 es el factor de mejora introducido y 
\begin_inset Formula $F_{m}$
\end_inset

 es la fracción de tiempo que el sistema utiliza el subsistema mejorado
\end_layout

\end_deeper
\begin_layout Standard

\lang english
Hoy en día, hay muchísimos trabajos de diferente índole con respecto a los
 clusters de computadoras, debido a que en elámbito de computación paralela,
 el cluster es la nueva modalidad de construcción de supercomputadoras,
 por su bajo costo y accesibilidad.
\end_layout

\begin_layout Subsection*

\lang english
Proyectos / Productos de Clusters
\end_layout

\begin_layout Subsubsection*

\lang english
Clusters de Alta Disponibilidad
\end_layout

\begin_layout Itemize

\lang english
Oracle Real Application Clusters
\end_layout

\begin_layout Itemize

\lang english
IBM DB2 Integrated Cluster Environment
\end_layout

\begin_layout Itemize

\lang english
Linux HA
\end_layout

\begin_layout Itemize

\lang english
FreeNAS
\end_layout

\begin_layout Itemize

\lang english
HA-OSCAR: The High Availability Linux Project
\end_layout

\begin_layout Itemize

\lang english
Kimberlite 
\end_layout

\begin_layout Itemize

\lang english
Lifekeeper
\end_layout

\begin_layout Itemize

\lang english
Linux FailSafe
\end_layout

\begin_layout Itemize

\lang english
Linux Replicated High Availability Manager
\end_layout

\begin_layout Itemize

\lang english
MC/Serviceguard
\end_layout

\begin_layout Subsubsection*

\lang english
Clusters de Alto Rendimiento
\end_layout

\begin_layout Itemize

\lang english
PVM
\end_layout

\begin_layout Itemize

\lang english
pvmsync
\end_layout

\begin_layout Itemize

\lang english
MPI
\end_layout

\begin_layout Itemize

\lang english
LAM/MPI
\end_layout

\begin_layout Itemize

\lang english
PLUS: MPI&PVM integration
\end_layout

\begin_layout Itemize

\lang english
Beowulf (for Linux)
\end_layout

\begin_layout Itemize

\lang english
Berkeley NOW (for Solaris)
\end_layout

\begin_layout Itemize

\lang english
Parallel Knoppix
\end_layout

\begin_layout Subsubsection*

\lang english
Mallas (Grids) de computadoras
\end_layout

\begin_layout Itemize

\lang english
SGE (Sun Grid Engine)
\end_layout

\begin_layout Itemize

\lang english
GridSim toolkit for Simulation of Application Scheduling
\end_layout

\begin_layout Itemize

\lang english
Gridbus: Toolkit for service-oriented cluster and grid computing 
\end_layout

\begin_layout Subsubsection*

\lang english
Balanceo de Carga
\end_layout

\begin_layout Itemize

\lang english
Alinka Oranges 
\end_layout

\begin_layout Itemize

\lang english
Keepalived
\end_layout

\begin_layout Itemize

\lang english
Linux Virtual Server
\end_layout

\begin_layout Itemize

\lang english
LVSM
\end_layout

\begin_layout Itemize

\lang english
Red Hat Cluster Suite
\end_layout

\begin_layout Itemize

\lang english
Turbolinux Cluster Server
\end_layout

\begin_layout Itemize

\lang english
Ultra Monkey
\end_layout

\begin_layout Itemize

\lang english
Condor
\end_layout

\begin_layout Itemize

\lang english
openMosix
\end_layout

\begin_layout Itemize

\lang english
Cluster Knoppix
\end_layout

\begin_layout Section*
Computación de Alto Rendimiento
\end_layout

\begin_layout Paragraph
El campo de la computación de alto rendimiento es muy importante en la resolució
n a problemas complejos.
 
\end_layout

\begin_layout Paragraph
La computación de alto rendimiento se apoya en tecnologías como los clusters,
 supercomputadores o mediante el uso de la computación paralela.
 
\end_layout

\begin_layout Subsection*
Computación Distribuida
\end_layout

\begin_layout Paragraph
La computación distribuida es un modelo para resolver problemas de computación
 masiva utilizando un gran número de computadoras en una infraestructura
 de telecomunicaciones distribuida.
\end_layout

\begin_layout Paragraph
La informática distribuida consiste en compartir recursos heterogéneos (basadas
 en distintas plataformas, arquitecturas de equipos y programas, lenguajes
 de programación), situados en distintos lugares y pertenecientes a diferentes
 dominios de administración sobre una red que utiliza estándares abiertos.
 
\end_layout

\begin_layout Paragraph
La computación distribuida ha sido diseñada para resolver problemas demasiado
 grandes para cualquier supercomputadora y main-frame, manteniendose la
 flexibilidad de trabajar en múltiples problemas más pequeños.
 Por lo tanto, la computación en grid es naturalmente un entorno multi-usuario;
 por ello, las técnicas de autorización segura son esenciales antes de permitir
 que los recursos informáticos sean controlados por usuarios remotos.
\end_layout

\begin_layout Section*
Programación Paralela
\end_layout

\begin_layout Paragraph
La programación paralela es una técnica de programación basada en la ejecución
 simultánea, bien sea en un mismo ordenador (con uno o varios procesadores)
 o en un cluster de ordenadores, en cuyo caso se denomina computación distribuid
a.
 Al contrario que en la programación concurrente, esta técnica enfatiza
 la verdadera simultaneidad en el tiempo de la ejecución de las tareas.
\end_layout

\begin_layout Paragraph
Los sistemas con multiprocesador y multicomputadores consiguen un aumento
 del rendimiento si se utilizan estas técnicas.
 En los sistemas monoprocesador el beneficio en rendimiento no es tan evidente,
 ya que la CPU es compartida por múltiples procesos en el tiempo, lo que
 se denomina multiplexación o multiprogramación.
\end_layout

\begin_layout Paragraph
El mayor problema de la computación paralela radica en la complejidad de
 sincronizar unas tareas con otras, ya sea mediante secciones críticas,
 semáforos o paso de mensajes, para garantizar la exclusión mutua en las
 zonas del código en las que sea necesario.
\end_layout

\begin_layout Subsubsection*
Aplicaciones demandantes de recursos
\end_layout

\begin_layout Paragraph
Podemos hacer una division de las aplicaciones que requieren una demanda
 de recusrsos computaionales importantes de la siguiente manera.
\end_layout

\begin_layout Subsubsection*
Aplicacions de calculo intencivas
\end_layout

\begin_layout Paragraph
Son aplicaciones que requieren un alto numero de ciclos de máquinas, estas
 son las que han impulsado el desarrollo de supercomputadores.
 Son típicas en ciencias e ingeniería, aunque recientemente han aparecido
 en otras áreas como simulación financiera y económica.
 Dependen grandemente de la velocidad y el procesamiento de punto flotantes
 de los supercomputadores.
 
\end_layout

\begin_layout Paragraph
Algunos ejemplos son:
\end_layout

\begin_layout Itemize
Dinámica de fluidos computacional.
\end_layout

\begin_layout Itemize
Simulaciones electromagnéticas.
\end_layout

\begin_layout Itemize
Modelado ambiental.
\end_layout

\begin_layout Itemize
Dinámica estructural.
\end_layout

\begin_layout Itemize
Modelado biológico.
\end_layout

\begin_layout Itemize
Dinámica molecular.
\end_layout

\begin_layout Itemize
Simulación de redes.
\end_layout

\begin_layout Itemize
Modelado financiero y económico.
\end_layout

\begin_layout Subsubsection*
Aplicaciones de almacenemiento masivo
\end_layout

\begin_layout Paragraph
Dependen de la capacidad para almacenar y procesar grandes cantidades de
 información.
 Requieren de un acceso rápido y seguro a una masa considerable de datos
 almacenados.
 
\end_layout

\begin_layout Paragraph
Algunas de ellas son:
\end_layout

\begin_layout Itemize
Análisis de data sísmica.
\end_layout

\begin_layout Itemize
Procesamiento de imágenes.
\end_layout

\begin_layout Itemize
Minería de datos.
\end_layout

\begin_layout Itemize
Análisis estadístico de datos.
\end_layout

\begin_layout Itemize
Análisis de mercados.
\end_layout

\begin_layout Subsubsection*
Aplicaciones exigentes comunicacionalmente
\end_layout

\begin_layout Paragraph
Estas son relativamente nuevas y pueden ser llamadas servicios por demanda.
 Requieren de recursos computacionales conectados por redes con anchos de
 banda considerables.
 
\end_layout

\begin_layout Paragraph
Ejemplos:
\end_layout

\begin_layout Itemize
Procesamiento de transacciones en línea.
\end_layout

\begin_layout Itemize
Sistemas colaborativos.
\end_layout

\begin_layout Itemize
Texto por demanda.
\end_layout

\begin_layout Itemize
Vídeo por demanda.
\end_layout

\begin_layout Itemize
Imágenes por demanda.
\end_layout

\begin_layout Itemize
Simulación por demanda.
\end_layout

\begin_layout Paragraph
Obviamente que todas las aplicaciones anteriores dependen en cierto grado
 de cada uno de los aspectos computacionales mencionados: poder de computo,
 capacidades de almacenamiento y eficientes canales de comunicación, sin
 embargo las podemos agrupar por su característica dominante.
\end_layout

\begin_layout Subsubsection*
Sistemas de sistemas
\end_layout

\begin_layout Paragraph
Las aplicaciones en este grupo combinan en forma más compleja las característica
s anteriores y dependen, en muchos casos, de sistemas computacionales integrados
 diseñados primordialmente para ellas.
 
\end_layout

\begin_layout Paragraph
Ejemplos:
\end_layout

\begin_layout Itemize
Soporte a decisiones corporativas y gubernamentales.
\end_layout

\begin_layout Itemize
Control de sistemas a tiempo real.
\end_layout

\begin_layout Itemize
Banca electrónica.
\end_layout

\begin_layout Itemize
Compras electrónicas.
\end_layout

\begin_layout Itemize
Educación.
\end_layout

\begin_layout Paragraph
Existe una alta correspondencia entre la evolución de tecnologías informaticas
 y el desarrollo de aplicaciones; en particular el hardware tiene gran influenci
a en el éxito de ciertas áreas.
 La aplicaciones intensivas en cálculo fueron estimuladas principalmente
 por máquinas vectoriales y procesadores masivamente paralelos.
 Las aplicaciones de almacenamiento masivo han sido guiadas por dispositivos
 de almacenamiento como RAID y robots de cintas.
 Las aplicaciones exigentes comunicacionales como herramientas colaborativas
 basadas en WWW y servicios por demanda en línea originalmente surgieron
 con las LAN y estan creciendo drásticamente con Internet.
\end_layout

\begin_layout Subsection*
Organizacion de computadores
\end_layout

\begin_layout Paragraph
La organización de los procesadores o red se refiere a como se conectan
 o enlazan los procesadores o nodos en un computador paralelo.
\end_layout

\begin_layout Paragraph
Existen varios criteriospara evaluar los distintos diseños de organizacion:
 
\end_layout

\begin_layout Paragraph
1) Diámetro: viene dado por la mayor distancia entre dos nodos.
 Mientras menor sea el diámetro menor será el tiempo de comunicación entre
 nodos.
\end_layout

\begin_layout Paragraph
2) Ancho de bisección de la red: es el menor número de enlaces que deben
 ser removidos para dividir la red por la mitad.
 Un ancho de bisección alto puede reducir el tiempo de comunicación cuando
 el movimiento de datos es sustancial, y un ancho de bisección alto hace
 el sistema más tolerante a fallas debido a que defectos en un nodo no hacen
 inoperable a todo el sistema.
\end_layout

\begin_layout Paragraph
3) Es preferible que el número de enlaces por nodo sea una constante independien
te del tamaño de la red, ya que hace más fácil incrementar el número de
 nodos.
\end_layout

\begin_layout Paragraph
4) Es preferible que la longitud máxima de los enlaces sea una constante
 independiente del tamaño de la red, ya que hace más fácil añadir nodos.
 
\end_layout

\begin_layout Paragraph
5) Redes estáticas y dinámicas.
 En las redes estáticas la topología de interconexión se define cuando se
 construye la máquina.
 Si la red es dinámica, la interconexión puede variar durante la ejecución
 de un programa o entre la ejecución de programas.
\end_layout

\begin_layout Paragraph
A continuacion veremos algunos tipos de disenos de redes de procesadores.
\end_layout

\begin_layout Subsubsection*
BUS Y ETHERNET
\end_layout

\begin_layout Paragraph
En una red donde los procesadores comparten el mismo recurso de comunicación:
 el bus.
 
\end_layout

\begin_layout Subsubsection*
Esta arquitectura es fácil y económica de implementar, pero es altamente
 no escalable ya que solo un procesador puede usar el bus en un momento
 dado; a medida que se incrementa el número de procesadores, el bus se convierte
 en un cuello de botella debido a la congestión.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Caption
C:
\backslash
Documents and Settings
\backslash
Paulo
\backslash
Mis documentos
\backslash
Facultad
\backslash
Proyecto
\backslash
Source
\backslash
Documentacion
\backslash
Images
\backslash
bus.png
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Mallas
\end_layout

\begin_layout Paragraph
Al igual que el bus (o ethernet), las mallas son fáciles y económicas de
 implementar, sin embargo el diámetro se incrementa al añadir nodos.
 En las mallas de dimensión 1, si los dos nodos extremos también son conectados,
 entonces se tiene un anillo.
\end_layout

\begin_layout Paragraph
Mallas de 2 dimensiones y de 3 dimensiones son comunes en computación paralela
 y tiene la ventaja de que pueden ser construidas sin conexiones largas.
 El diámetro de las mallas puede ser reducido a la mitad si se extiende
 la malla con conexiones toroidales de forma que los procesadores en los
 bordes también estén conectados con vecinos.
 Esto sin embargo presenta dos desventajas: a) conexiones más largas son
 requeridas y b) un subconjunto de un torus no es un torus y los beneficios
 de esta interconexión se pierden si la máquina es particionada entre varios
 usuari os.
\end_layout

\begin_layout Subsubsection*
Mariposa
\end_layout

\begin_layout Paragraph
Simaleres a las mallas pero presentan menor diametro.
\end_layout

\begin_layout Subsubsection*
Arboles binarios
\end_layout

\begin_layout Paragraph
Son particularmente útiles en problemas de ordenamiento, multiplicación
 de matrices, y algunos problemas en los que los tiempos sw solución crecen
 exponencialmente con el tamaño del problema (NP-complejos).
 
\end_layout

\begin_layout Subsubsection*
Piramides
\end_layout

\begin_layout Paragraph
Estas redes intentan combinar las ventajas de las mallas y los arboles,
 se incrementa la tolerancia a fallas y el número de vias de comunicación.
\end_layout

\begin_layout Subsubsection*
Hipercubo
\end_layout

\begin_layout Paragraph
Un hipercubo puede ser considerado como una malla con conexiones largas
 adicionales, estas conexiones reducen el diámetro e incrementan el ancho
 de bisección.
 
\end_layout

\begin_layout Paragraph
Se puede definir recursibamente un hipercubo de la siguiente manera: un
 hipercubo de dimensión-cero es un único procesador y un hipercubo de dimensión-
uno conecta dos hipercubos de dimensión-cero.
 En general, un hipercubo de dimensión d+1 con 2d+1 nodos, se construye
 conectando los procesadores respectivos de dos hipercubos de dimensión
 d.
 
\end_layout

\begin_layout Subsubsection*
Omega
\end_layout

\begin_layout Paragraph
Es una red formada por crossbar switches 2x2.
 Estos tiene cuatro estados posibles: recto, cruzado, broadcast superior
 y broadcast inferior que son configurados segun la conexión que se tien.
 Estas topologías se llaman dinámicas o reconfigurables.
 
\end_layout

\begin_layout Paragraph
Estas redes reducen considerablemente la competencia por ancho de banda,
 pero son altamente no escalables y costosas.
 El highperformance-switch de la SP2 es una red omega.
\end_layout

\begin_layout Paragraph
Lasig siguiente figura muestra una red omega de 3 etapas que conecta 8 procesado
res.
\end_layout

\begin_layout Subsubsection*
Fibras de interconexion
\end_layout

\begin_layout Paragraph
Estas so un conjunto de switches, llamados routers, enlazados por distintas
 configuraciones o topologías.
\end_layout

\begin_layout Subsection*
Diseno de algoritmos paraleleos
\end_layout

\begin_layout Paragraph
El diseño de algoritmos paralelos involucra cuatro etapas, las cuales se
 presentan como secuenciales pero que en la práctica no lo son.
\end_layout

\begin_layout Paragraph
1) Partición: El cómputo y los datos sobre los cuales se opera se descomponen
 en tareas.
 Se ignoran aspectos como el número de procesadores de la máquina a usar
 y se concentra la atención en explotar oportunidades de paralelismo.
\end_layout

\begin_layout Paragraph
2) Comunicación: Se determina la comunicación para coordinar las tareas.
 Se definen estructuras y algoritmos de comunicación.
\end_layout

\begin_layout Paragraph
3) Agrupación: Se evalua en terminos de eficiencia y costos de implementacion
 a las dos etapas anteriores.
 
\end_layout

\begin_layout Paragraph
se agrupan tareas pequeñas en tareas más grandes.
\end_layout

\begin_layout Paragraph
4) Asignación: Cada tarea es asignada a un procesador tratando de maximizar
 la utilización de los procesadores y de reducir el costo de comunicación.
 La asignación puede ser estática (se establece antes de la ejecución del
 programa) o en tiempo de ejecución mediante algoritmos de balanceo de carga.
\end_layout

\begin_layout Subsubsection*
Particion
\end_layout

\begin_layout Paragraph
En la etapa de partición se buscan oportunidades de paralelismo y se trata
 de subdividir el problema lo más finamente posible.
 Se dividen tanto los cómputos como los datos.
 
\end_layout

\begin_layout Paragraph
Existen dos formas de descomposicion.
 Descomposición del dominio: se centra en los datos.
 Se determina la partición
\end_layout

\begin_layout Paragraph
apropiada de los datos y luego se trabaja en los cómputos asociados con
 los datos.
\end_layout

\begin_layout Paragraph
Descomposición funcional: es el contrario al enfoque anterior, primero se
 descomponen los cómputos y luego se ocupa de los datos.
 
\end_layout

\begin_layout Paragraph
En el proceso de particion existen aspectos a tener en cuenta: 
\end_layout

\begin_layout Itemize
el orden de tareas debe ser por lo menos superior al numero de procesadores
 para tener flexibilidad en etapas siguientes
\end_layout

\begin_layout Itemize
evitar cómputos y almacenamientos redundantes
\end_layout

\begin_layout Itemize
las tareas deben ser de tamaños equivalentes para facilitar el balanceo
 de la carga de los procesadores.
\end_layout

\begin_layout Itemize
el número de tareas debe ser proporcional al tamaño del problema
\end_layout

\begin_layout Subsubsection*
Comunicación
\end_layout

\begin_layout Paragraph
El proceso de partición de tareas no es independiente del proceso de comunicació
n.
 En esta se especifica como los datos serán transferidos o compartidos entre
 tareas.
\end_layout

\begin_layout Paragraph
La comunicación puede ser definida en dos fases.
 Primero se definen los canales que conectan las tareas.
 Segundo
\end_layout

\begin_layout Paragraph
se especifica la información o mensajes que deben ser enviado y recibidos
 en estos canales.
\end_layout

\begin_layout Paragraph
Dependiendo de si estamos en el caso de memoria distribuida o memoria compartida
 dependera la forma de atacar la comunicación entre tareas varía.
\end_layout

\begin_layout Paragraph
En un ambientes de memoria distribuida, las tareas tiene una identificación
 única y interactuan enviando y recibiendo mensajes hacia y desde tareas
 específicas.
 Las librerías más conocidas para implementar el pase de mensajes en ambientes
 de memoria distribuida son: MPI (Message Passing Interface) y PVM (Parallel
 Virtual Machine).
\end_layout

\begin_layout Paragraph
En ambientes de memoria compartida no existe la noción de pertenencia y
 el envío de datos no se dá como tal.
 Todas las tareas comparten la misma memoria.
\end_layout

\begin_layout Paragraph
Semáforos, semáforos binarios, barreras y otros mecanismos de sincronizacion
 son usados para controlar el acceso a la memoria compartida y coordinar
 las tareas.
\end_layout

\begin_layout Paragraph
En esta etapa hay que tener en cuenta los siguientes aspectos:
\end_layout

\begin_layout Itemize
las tareas deben efectuar aproximadamente el mismo número de operaciones
 de comunicación.
 De otra forma es muy probable que el algoritmo no sea extensible a problemas
 mayores ya que habrán cuellos de botella.
\end_layout

\begin_layout Itemize
la comunicación entre tareas debe ser tan pequeña como sea posible.
\end_layout

\begin_layout Itemize
las operaciones de comunicación deben poder proceder concurrentemente.
\end_layout

\begin_layout Itemize
los cómputos de diferentes tareas deben poder proceder concurrentemente.
\end_layout

\begin_layout Subsubsection*
Agrupacion
\end_layout

\begin_layout Paragraph
En las dos tareas anteriores el algoritmo resultante es aún abstracto en
 el sentido de que no se tomó en cuenta la máquina sobre el cual correrá.
 En este proceso se busca un algoritmo concreto, que corra eficientemente
 sobre cierta clase de computadores.
 
\end_layout

\begin_layout Paragraph
En particular se considera si es útil agrupar tareas y si vale la pena replicar
 datos y/o cómputos.
\end_layout

\begin_layout Paragraph
En el proceso de partición se trata de establecer el mayor número posible
 de tareas con la intensión de maximizar el paralelismo.
 
\end_layout

\begin_layout Paragraph
Esto no necesariamente produce un algoritmo eficiente ya que el costo de
 comunicación puede ser significativo.
 
\end_layout

\begin_layout Paragraph
Mediante la agrupación de tareas se puede reducir la cantidad de datos a
 enviar y así reducir el número de
\end_layout

\begin_layout Paragraph
mensajes y el costo de comunicación.
\end_layout

\begin_layout Paragraph
Se puede intentar replicar cómputos y/o datos para reducir los requerimientos
 de comunicación.
 
\end_layout

\begin_layout Paragraph
Aspectos a considerar en esta etapa:
\end_layout

\begin_layout Itemize
chequear si la agrupación redujo los costos de comunicación.
\end_layout

\begin_layout Itemize
si se han replicado cómputos y/o datos, se debe verificar que los beneficios
 son superiores a los costos.
\end_layout

\begin_layout Itemize
se debe verificar que las tareas resultantes tengan costos de computo y
 comunicación similares.
\end_layout

\begin_layout Itemize
revisar si el número de tareas es extensible con el tamaño del problema.
\end_layout

\begin_layout Itemize
si el agrupamiento ha reducido las oportunidades de ejecución concurrente,
 se debe verificar que aun hay suficiente concurrencia y posiblemente considerar
 diseños alternativos.
\end_layout

\begin_layout Itemize
nalizar si es posible reducir aun más el número de tareas sin introducir
 desbalances de cargas o reducir la extensibilidad.
\end_layout

\begin_layout Subsubsection*
Asignacion
\end_layout

\begin_layout Paragraph
En este proceso se determina en que procesador se ejecutará cada tarea.
 
\end_layout

\begin_layout Paragraph
En maquinas de memoria compartida tipo UMA no se presenta este problema
 ya que proveen asignación dinámica de procesos y los procesos.
 
\end_layout

\begin_layout Paragraph
Actualmente no hay mecanismos generales de asignación de tareas para máquinas
 distribuidas por lo que este problema debe ser atacado explícitamente en
 el diseno de algoritmos paralelos.
\end_layout

\begin_layout Paragraph
La asignación de tareas puede ser estática o dinámica.
 En la asignación estática, las tareas son asignadas a un procesador al
 comienzo de la ejecución del algoritmo paralelo y corren ahí hasta el final.
 La asignación estática en ciertos casos puede resultar en un tiempo de
 ejecución menor respecto a asignaciones dinámicas y también puede reducir
 el costo de creación de procesos, sincronización y terminación.
\end_layout

\begin_layout Paragraph
En la asignación dinámica se hacen cambios en la distribución de las tareas
 entre los procesadores en tiempo de ejecución Esto es con el fin de balancear
 la carga del sistema y reducir el tiempo de ejecución.
 Sin embargo, el
\end_layout

\begin_layout Paragraph
costo de balanceo puede ser significativo y por ende incrementar el tiempo
 de ejecución.
 Entre los algoritmos de balanceo de carga están los siguientes:
\end_layout

\begin_layout Itemize
Balanceo centralizado: un nodo ejecuta el algoritmo y mantiene el estado
 global del sistema.
\end_layout

\begin_layout Itemize
Balanceo completamente distribuido: cada procesador mantiene su propia visión
 del sistema intercambiando información con sus vecinos y así hacer cambios
 locales.
\end_layout

\begin_layout Itemize
Balanceo semi-distribuido: divide los procesadores en regiones, cada una
 con un algoritmo centralizado local.
 Otro algoritmo balancea la carga entre las regiones.
\end_layout

\begin_layout Subsection*
Modelos de computacion paralela
\end_layout

\begin_layout Subsubsection*
Arreglos de procesadores
\end_layout

\begin_layout Paragraph
Una maquina cuyo conjunto de instruccuines permite operaciones tanto sobre
 vectores como escalares lo denominamos computador vectorial.
\end_layout

\begin_layout Subsubsection*
Procesador Vectorial Pipelined
\end_layout

\begin_layout Paragraph
En estas máquinas los vectores fluyen a través de las unidades aritméticas
 pipelined.
\end_layout

\begin_layout Paragraph
Las unidades consisten de una cascada de etapas de procesamiento compuestas
 de circuitos que efectúan operaciones aritméticas o lógicas sobre el flujo
 de datos que pasan a través de ellas.
\end_layout

\begin_layout Paragraph
La etapas están separadas por registros de alta velocidad usados para guardar
 resultados intermedios.
 La información que fluye entre las etapas adyacentes esta bajo el control
 de un reloj R que se aplica a todos los registros simultáneamente.
 
\end_layout

\begin_layout Paragraph
En esta categoría tenemos maquinas como la Cray-1 y la Cyber-205.
\end_layout

\begin_layout Subsubsection*
Arreglos de Procesadores
\end_layout

\begin_layout Paragraph
Son máquinas que constan de un computador secuencial conectado a un arreglo
 de elementos de procesamiento sincronizados e idénticos capaces de ejecutar
 las mismas operaciones sobre datos diferentes.
 El computador secuencial generalmente es un CPU de propósito general que
 almacena el programa y los datos que serán operados en paralelo, además
 de ejecutar la porción del programa que es secuencial.
 Los elementos de procesamiento se asemejan a CPUs pero no tienen unidades
 de control propias; el computador secuencial genera todas las señales de
 control para la unidades de procesamiento en el computador.
\end_layout

\begin_layout Paragraph
Los arreglos de procesamiento difieren fundamentalmente en la complejidad
 y la topología de interconexión de sus elementos de procesamiento.
 
\end_layout

\begin_layout Paragraph
Ejemplos de estas máquinas son: IILIAC IV, Goodyear MPP y Connection Machine
 CM-200.
\end_layout

\begin_layout Subsubsection*
Multiprocesadores
\end_layout

\begin_layout Paragraph
Son equipos formados por un número de procesadores completamente programables
 capaces de ejecutar su propio programa.
\end_layout

\begin_layout Subsubsection*
UMA: Multiprocesadores de Acceso Uniforme a Memoria
\end_layout

\begin_layout Paragraph
\paragraph_spacing single
\align left
Estos computadores tienen sus procesadores interconectados a través de un
 mecanismo de switches a una memoria compartida centralizada.
 Entre estos mecanismos estan: un bus común, crossbar switches, packet-switched
 networks9.
\end_layout

\begin_layout Paragraph
\paragraph_spacing single
\align left
Encore Multimax y Sequent Symetry S81 son ejemplos comerciales de este tipo
 de multiprocesadores.
\end_layout

\begin_layout Subsubsection*
NUMA: Multiprocesadores de Acceso No-Uniforme a Memoria
\end_layout

\begin_layout Paragraph
Estos multiprocesadores tienen el espacio de direccionamiento compartido
 y la memoria distribuida.
 La memoria compartida esta formada por la memoria local de los procesadores.
 El tiempo de acceso a memoria depende de si el acceso es local al procesador
 o no.
 La BBN TC2000 y la SGI Origin 2000 son ejemplos de este modelo de computación
 paralela.
\end_layout

\begin_layout Paragraph
El principal problema que presentan los multiprocesadores (máquinas de memoria
 compartida) radica en que no se pueden agregar procesadores indefinidamente
 ya que a partir de cierto número y dependiendo de la aplicación, el mecanismo
 de switches o enrutamiento se satura , en otras palabras, tienen poca extensibi
lidad.
\end_layout

\begin_layout Paragraph
MULTICOMPUTADORES
\end_layout

\begin_layout Paragraph
En esta categoría, los procesadores no comparten memoria.
 Cada procesador tiene su propia memoria privada (máquinas de memoria distribuid
a) y la interacción entre ellos es a través de pase de mensajes.
 Ejemplos son: Intel ParagonXP/S, Meikos Computing Surface, nCUBE 2, Parsytec
 SuperCluster, Thinking Machine CM-5 y la IBM SP2.
\end_layout

\begin_layout Subsubsection*
MAQUINAS DE MEMORIA COMPARTIDA DISTRIBUIDA
\end_layout

\begin_layout Paragraph
Actualmente las máquinas paralelas tienden a aprovechar las facilidades
 de programación que ofrecen los ambientes de memoria compartida y la escalabili
dad de los ambientes de memoria distribuida.
 
\end_layout

\begin_layout Paragraph
Este modelo conecta entre si módulos de multiprocesadores manteniendo la
 visión global de la memoria.
 
\end_layout

\begin_layout Paragraph
Este tipo de maquinas entra dentro de la categoría de NUMA y un ejemplo
 es la SGI Origin 2000.
\end_layout

\begin_layout Subsubsection*
MULTIPROCESADORES MULTI-HEBRADOS
\end_layout

\begin_layout Paragraph
En estas máquinas cada procesador tiene cierto número de flujos de instrucciones
 implementados en hardware, incluyendo el contador de programa y registros,
 cada uno destinado a ejecutar una hebra.
 En cada ciclo el procesador ejecuta instrucciones de una de la hebras.
 En el ciclo siguiente, el procesador hace un cambio de contexto y ejecuta
 instrucciones de otra hebra
\end_layout

\begin_layout Paragraph
La Tera MTA (multithreaded architecture), es la primera de estas máquinas
 , cada procesador tiene 128 flujos de instrucciones.
\end_layout

\begin_layout Paragraph
Un acceso a memoria dura aproximadamente 100 ciclos por lo que en en la
 próxima ejecución de la hebra se tendrán los datos requeridos.
 Este mecanismo permite a la MTA tolerar la latencia a memoria y por lo
 tanto no requiere de memorias cache.
\end_layout

\begin_layout Paragraph
Cada instrucción de 64-bits codifica 3 operaciones(una de memoria y dos
 que pueden ser aritméticas o de control).
 Cuenta con in sistema operativo de versión distribuida completamente simétrica
 de UNIX.
\end_layout

\begin_layout Paragraph
El sistema cuenta con 1 a 256 procesadores que comparten una enorme memoria.
 A su ves cada procesador tiene 1 o 2 Gb de memoria, un mapeo aleatorio
 de la memoria y una red altamente interconectada proveen acceso casi uniforme
 de cualquier procesador a cualquier memoria.
\end_layout

\begin_layout Paragraph
Los estudios realizados muestran que el costo del multi-hebrado es pequeño,
 con un rendimiento comparable con el de la T90 y los códigos de la MTA
 son significativamente más fáciles de optimizar que en máquinas masivamente
 paralelas o estaciones de trabajo de alto rendimiento.
\end_layout

\begin_layout Subsubsection*
TAXONOMIA DE FLYNN
\end_layout

\begin_layout Paragraph
Esta es otra forma de clasificar computadores seriales y paralelos.
\end_layout

\begin_layout Paragraph
Es basada en la multiplicidad del flujo de instrucciones y del flujo de
 datos en un computador.
 
\end_layout

\begin_layout Paragraph
Un flujo de instrucciones es una secuencia de instrucciones ejecutadas por
 el computador y un flujo de datos es la secuencia de datos sobre los cuales
 operan las instrucciones.
\end_layout

\begin_layout Paragraph
Dentro de la clasificacion de Flynn tenemos la siguientes categorías.
\end_layout

\begin_layout Subsubsection*
SISD Single instruction stream, single data stream
\end_layout

\begin_layout Paragraph
La mayor parte de computadores seriales son SISD.
 Estas tienen un CPU que ejecuta una instrucciones, busca o guarda datos
 en un momento dado.
\end_layout

\begin_layout Standard
Figura
\end_layout

\begin_layout Subsubsection*
SIMD Single instruction stream, multiple data stream
\end_layout

\begin_layout Paragraph
En esta categoria entrarn los arreglos de procesadores.
\end_layout

\begin_layout Paragraph
Hay un conjunto de unidades de procesamiento cada una ejecutando la misma
 operación sobre datos distintos
\end_layout

\begin_layout Standard
Figura
\end_layout

\begin_layout Subsubsection*
MISD Multiple instruction stream, single data stream
\end_layout

\begin_layout Paragraph
Existen n procesadores, cada uno recibiendo una instrucción diferente y
 operando sobre el mismo flujo de datos.
\end_layout

\begin_layout Paragraph
Actualmente no hay máquinas de este tipo por ser poco practicas, a pesar
 de que ciertas MIMD puedan ser usadas de esta forma.
\end_layout

\begin_layout Paragraph
MIMD Multiple instruction stream, multiple data stream La mayoría de los
 multiprocesadores y multicomputadores pueden ser clasificados bajo esta
 categoría.
 Estos tienen más de un procesador independiente, y cada uno puede ejecutar
 un programa diferente sobre sus propios datos.
\end_layout

\begin_layout Paragraph
Podemos categorizarlos tambien según como esté organizada su memoria: memoria
 compartida o memoria distribuida.
\end_layout

\begin_layout Subsubsection*
SPMD Single program, multiple data
\end_layout

\begin_layout Paragraph
En esta categoria cada procesador ejecuta una copia exacta del mismo programa,
 pero opera sobre datos diferentes.
 
\end_layout

\begin_layout Paragraph
Pese a no ser una categoría definida por Flynn es muy usada y pueder ser
 considerada como un caso particular de MIMD.
\end_layout

\begin_layout Subsubsection*
EL IBM SCALABLE POWER PARALLEL SYSTEM (SP2)
\end_layout

\begin_layout Paragraph
Si queremos clasificar a la SP2 dentro de las categorías vistas podemos
 decir que es un multicomputador (memoria distribuida) MIMD.
 
\end_layout

\begin_layout Paragraph
Cada nodo puede ser usado como un computador independiente, pero también
 pueden cooperar en la solución del mismo problema en paralelo.
 
\end_layout

\begin_layout Paragraph
Tenemos tres tipos de nodos en la SP2:
\end_layout

\begin_layout Itemize
anchos: se configuran como servidores para proveer servicios necesarios
 para la ejecución de tareas y permiten conectar directamente dispositivos
 de almacenamiento.
 
\end_layout

\begin_layout Itemize
finos: son preferibles para ejecutar tareas.
\end_layout

\begin_layout Itemize
altos constan de hasta 8 procesadores que comparten la misma memoria.
 
\end_layout

\begin_layout Paragraph
Cada módulo de una SP2 puede tener hasta 16 nodos finos, u 8 anchos o 4
 altos.
 
\end_layout

\begin_layout Paragraph
Un switch de alta eficiencia (high-performance-switch) conecta los procesadores
 para proveer altas velocidades de comunicación.
 
\end_layout

\begin_layout Paragraph
En particular consiste de una red omega, de mutiples etapas, buffers y un
 mecanismo de enrutamiento packet-switched.
 
\end_layout

\begin_layout Paragraph
Los buffers permiten el almacenamiento temporal de paquetes entre etapas
 en caso de que la etapa siguiente esté ocupada.
 
\end_layout

\begin_layout Paragraph
En los protocolos packet-switched, cada mensaje contiene información de
 enrutamiento
\end_layout

\begin_layout Paragraph
que es interpretada por cada elemento del switch.
\end_layout

\begin_layout Paragraph
La SP2 esta basada en el sistema operativo AIX/6000 y en la tecnología RISC
 System/6000 POWER2.
 
\end_layout

\begin_layout Subsubsection*
LA SGI ORIGIN 2000
\end_layout

\begin_layout Paragraph
La SGI Origin 2000 es un MIMD con memoria distribuida compartida.
\end_layout

\begin_layout Paragraph
En cada nodo hay uno o dos procesadores R10000 con memoria distribuida a
 través del sistema.
 
\end_layout

\begin_layout Paragraph
Esta provista con hardware para la migración de paginas.
\end_layout

\begin_layout Paragraph
La memoria es universalmente accesible y compartida entre todos los procesadores.
\end_layout

\begin_layout Paragraph
De igual forma los dispositivos de I/O están distribuidos entre los nodos
 pero cada uno esta
\end_layout

\begin_layout Paragraph
disponible a todos los procesadores.
\end_layout

\begin_layout Paragraph
FIGURA
\end_layout

\begin_layout Paragraph
El Hub es el controlador de la memoria distribuida/compartida y es el encargado
 de que todos los procesadores y los dispositivos de I/O tengan acceso transpare
nte a toda la memoria.
\end_layout

\begin_layout Paragraph
El Crossbow es un crossbar chip encargado de conectar dos nodos controladores
 de I/O.
\end_layout

\begin_layout Paragraph
La fibra de interconexión es una malla de múltiples enlaces punto-a-punto
 conectados por los enrutadores y provee a cada par de nodos con un mínimo
 de dos trayectorias distintas.
 Esta redundancia le permite al sistema eludir enrutadores o enlaces de
 la fibra que estén fallando.
\end_layout

\begin_layout Paragraph
La Origin 2000 puede tener hasta 128 procesadores, 4GB de memoria por nodo
 (256GB en total) y 64 interfaces de I/O con 192 controladores de I/O.
 
\end_layout

\begin_layout Paragraph
A medida que se agregan nodos a la fibra de interconexión, el ancho de banda
 y la eficiencia escalan linealmente sin afectar significativamente las
 latencias del sistema.
 
\end_layout

\begin_layout Subsubsection*
CLUSTERS DE PC'S
\end_layout

\begin_layout Paragraph
El término cluster se aplica a los conjuntos de computadoras que se comportan
 como si fuesen una única computadora.
 Hoy en día tiene un papel importante en aplicaciones científicas y de ingenierí
a, comerciales, simulaciones, etc.
\end_layout

\begin_layout Paragraph
La tecnología de clusters ha evolucionado apoyándose en actividades que
 van desde aplicaciones de supercómputo y software de misiones críticas,
 servidores Web y comercio electrónico, hasta bases de datos de alto rendimiento.
\end_layout

\begin_layout Paragraph
El uso de clusters surge gracias a la convergencia de varias tendencias
 actuales como la disponibilidad de microprocesadores económicos de alto
 rendimiento y redes de alta velocidad, la existentica de herramientas para
 cómputo distribuido de alto rendimiento, así como la creciente necesidad
 de potencia computacional para aplicaciones que la requieran.
\end_layout

\begin_layout Paragraph
Un cluster es un grupo de múltiples ordenadores unidos mediante una red
 de alta velocidad, de tal forma que el conjunto es visto como un único
 ordenador, más potente que los comunes de escritorio.
 
\end_layout

\begin_layout Paragraph
Se espera de un cluster que presente combinaciones de los siguientes característ
icas:
\end_layout

\begin_layout Itemize
Alto rendimiento
\end_layout

\begin_layout Itemize
Alta disponibilidad
\end_layout

\begin_layout Itemize
Equilibrio de carga 
\end_layout

\begin_layout Itemize
Escalabilidad 
\end_layout

\begin_layout Paragraph
La construcción de los cluster es muy fácil y económica, debido a su flexibilida
d: pueden tener todos la misma configuración de hardware y sistema operativo
 (cluster homogéneo), diferente rendimiento pero con arquitecturas y sistemas
 operativos similares (cluster semi-homogéneo), o tener diferente hardware
 y sistema operativo (cluster heterogéneo).
\end_layout

\begin_layout Paragraph
Se pueden construir cluster con ordenadores personales desechados por "anticuado
s" que consiguen competir en capacidad de cálculo con superordenadores carísimos.
\end_layout

\begin_layout Paragraph
Es necesario proveer un sistema para el manejo del cluster, el cual se encargue
 de interactuar con el usuario y los procesos que corren en él para optimizar
 el funcionamiento.
\end_layout

\begin_layout Subsubsection*
Cluster de alto rendimiento
\end_layout

\begin_layout Subparagraph
Un cluster de alto rendimiento está diseñado para dar altas prestaciones
 en cuanto a capacidad de cálculo.
\end_layout

\begin_layout Subparagraph
Los motivos para utilizar un cluster de alto rendimiento son:
\end_layout

\begin_layout Itemize
el tamaño del problema por resolver y
\end_layout

\begin_layout Itemize
el precio de la máquina necesaria para resolverlo.
\end_layout

\begin_layout Subparagraph
Por medio de un cluster se pueden conseguir capacidades de cálculo superiores
 a las de un ordenador más caro.
\end_layout

\begin_layout Subparagraph
Para garantizar esta capacidad de cálculo, los problemas necesitan ser paraleliz
ables, ya que el método con el que los clusters agilizan el procesamiento
 es dividir el problema en problemas más pequeños y calcularlos en los nodos.
\end_layout

\begin_layout Subsubsection*
Cluster de alta disponibilidad
\end_layout

\begin_layout Subparagraph
Un cluster de alta disponibilidad se caracterizan por compartir los discos
 de almacenamiento de datos y por estar constantemente monitorizándose entre
 sí.
 
\end_layout

\begin_layout Subparagraph
Podemos dividirlo en dos clases:
\end_layout

\begin_layout Itemize
Alta disponibilidad de infrestructura
\end_layout

\begin_layout Subsubsection*
Alta disponibilidad de aplicación
\end_layout

\begin_layout Subsubsection*
Cluster de balanceo de carga
\end_layout

\begin_layout Paragraph
Un cluster de equilibrio de carga o de cómputo adaptativo está compuesto
 por uno o más ordenadores que actúan como frontend del cluster, y que se
 ocupan de repartir las peticiones de servicio que reciba el cluster, a
 otros ordenadores del cluster que forman el back-end de éste.
 
\end_layout

\begin_layout Subsubsection*
Escalabilidad
\end_layout

\begin_layout Paragraph
La escalabilidad es la propiedad deseable de un sistema, una red o un proceso,
 que indica su habilidad para, o bien manejar el crecimiento continuo de
 trabajo de manera fluida, o bien para estar preparado para hacerse más
 grande sin perder calidad en los servicios ofrecidos.
\end_layout

\begin_layout Paragraph
Las características más destacadas de este tipo de cluster son:
\end_layout

\begin_layout Itemize
Se puede ampliar su capacidad fácilmente añadiendo más ordenadores al cluster.
\end_layout

\begin_layout Itemize
Robustez.
 Ante la caída de alguno de los ordenadores del cluster el servicio se puede
 ver mermado, pero mientras haya ordenadores en funcionamiento, éstos seguirán
 dando servicio.
\end_layout

\begin_layout Section*
Relevamiento de tecnologias
\end_layout

\begin_layout Section
SSI: OpenMosix, OpenSSI y Kerrighed
\end_layout

\begin_layout Standard
A diferencia de un cluster tradicional en SSI todas las computadoras vinculadas
 dependen de un sistema operativo identico en común.
\end_layout

\begin_layout Standard
Un SSI oculta la naturaleza heterogénea y distribuida de los recursos, y
 los presenta a los usuarios y a las aplicaciones como un recurso computacional
 unificado y sencillo.
 Una de las metas que mas diferencia un SSI de un cluster o un grid tradicional
 es su completa transparencia en la gestión de recursos.
 Es debido a esta completa transparencia que es posible migrar procesos
 de un nodo a otro.
 Tampoco es necesario programacion adicional para beneficiarse del paralelismo,
 de esta manera no es necesario re-escribir programas utilizando bibliotecas
 como PVM (Parallel Virtual Machine) o MPI (Message Passing Interface).
\end_layout

\begin_layout Standard
A pesar de las ventajas presentadas por los SSI a nivel de manejo de recursos,
 no creemos que se ajusten a los requerimientos planteados principalmente
 debido a que el cluster ya se encuentra en funcionamiento siguiendo un
 paradigma estilo beowulf mas clasico.
 Quizas sea interesante construir un pequeño cluster SSI e investigar mas
 a fondo este tipo de tecnologias de manera de poder compara ambas soluciones.
\end_layout

\begin_layout Section
Condor
\end_layout

\begin_layout Subsection
Caracteristicas generales
\end_layout

\begin_layout Standard
Condor es un proyecto open-source de la universidad de Wisconsin que fue
 especificamente diseñado para High-Throughput Computing (HTC).
 Si bien Condor puede utilizarse en High-Performance Computing (HPC) existen
 importantes diferencias entre HPC y HTC.
 Fundamentalmente los trabajos ejecutados en un entorno HPC se caracterizan
 por necesitar grandes cantidades de poder computacional de manera de resolver
 un problema en el menor tiempo posible, en cambio en entorno HTC las tareas
 requieren tiempos de ejecucion considerablemente superiores (meses o años
 en lugar de horas o días).
 Como consecuencia en HPC el poder computacional de un cluster normalmente
 es medido en operaciones de punto flotante por segundo (FLOPS), la comunidad
 HTC en cambio se enfoca en cuantos trabajos es posible completar en un
 largo periodo de tiempo y no le interesa que tan rapido puede terminarse
 un trabajo individual.
 Debido a este perfil de tipo HTC, Condor se enfoca en la robustez y la
 confiabilidad brindando funcionalidades como: CPU harvesting, migración
 de procesos, tolerancia a fallas y process checkpoint.
\end_layout

\begin_layout Standard
Paradojicamente si bien se trata de un proyecto open-source su codigo fuente
 no se distribuye libremente.
 Para obtener el codigo fuente de Condor es necesario enviar un correo electroni
co explicando la razón por la que se quiere obtener el código fuente y cada
 petición es analizada puntualmente.
\end_layout

\begin_layout Subsection
Bibliotecas para programación paralela
\end_layout

\begin_layout Standard
Condor se encuentra diseñado principalmente para la ejecucion de trabajos
 seriales.
 Si bien provee soporte limitado para las bibliotecas PVM y MPI para procesamien
to paralelo su desempeño en esta area se encuentra por debajo de otros DRM's
 como Torque o SGE.
\end_layout

\begin_layout Subsection
Interfaz de programación
\end_layout

\begin_layout Standard
Condor brinda una interfaz de programacion propietaria, pero no soporta
 el estandar DRMAA.
\end_layout

\begin_layout Subsection
Conclusión
\end_layout

\begin_layout Standard
Condor no parece ser el software ideal para el proyecto.
 Si bien puede adaptarse a los requerimientos planteados y brinda funcionalidade
s interesantes (p.ej.
 migracion de procesos, cpu harvesting, etc.) existen areas en las que se
 encuentra claramente en desventaja con respecto a otros productos.
\end_layout

\begin_layout Section
OpenPBS
\end_layout

\begin_layout Itemize
No hay soporte
\end_layout

\begin_layout Itemize
No hay nuevas versiones desde hace anios
\end_layout

\begin_layout Section
SLURM
\end_layout

\begin_layout Itemize
No soporta PVM
\end_layout

\begin_layout Section
Sun grid engine 6.1
\end_layout

\begin_layout Paragraph
Sun Grid Engine es un software para administración de recursos distribuídos
 que dinámicamente asocia requerimientos de hardware y software de los usuarios
 con los recursos disponibles en la red (generalmente heterogéneos) de acuerdo
 a políticas predefinidas.
\end_layout

\begin_layout Standard
Sun Grid Engine actúa como el sistema nervioso central de un cluster de
 computadoras conectadas.
 A partir de algunos demoniso, el Sun Grid Master supervisa todos los recursos
 de la red 
\lang english
para permitir control completo y alcanzar la utilización óptima de los mismos.
\end_layout

\begin_layout Paragraph
Sun Grid Engine fue desarrollado como realce de Codine de Genias GmbH y
 Grisdware inc, de acuerdo a requerimientos de varios clientes tales como
 el laboratorio de investigación del ejército de Aberdeen y BMW.
 Con Sun Grid Engine, el uso medio de los recursos aumentó de menos del
 50% a más del 90% en ambos ambientes.
\end_layout

\begin_layout Paragraph
Sun grid Engine reune el poder de cálculo disponible en granjas de computadoras
 dedicadas, servidores conectados y computadoras de escritorio, y las presenta
 desde un único punto de acceso para el usuario que necesita ciclos de cómputo.
 Esto se logra distribuyendo la carga de trabajo entre los sistemas disponibles,
 aumentando la productividad de máquinas y del uso de licencias, mientras
 se maximiza el número de trabajos que pueden ser completados.
\end_layout

\begin_layout Paragraph
Los requerimientos de hardware son mínimos (100 MB de memoria disponible
 y 500MB de disco) y soporta la mayoría de los sistemas operativos populares.
 Soporta las plataformas SPARC Ultra III, SPARC Ultra IV, AMD64, x86 y Mac.
\end_layout

\begin_layout Standard
Permite control de usuarios, limitando tanto el número máximo de trabajos
 por usuario, grupo y proyecto, como recursos tales como colas, hosts, memoria
 y licencias de software.
\end_layout

\begin_layout Standard
Los recursos requeridos por cada trabajo se pueden indicar mediante expresiones
 lógicas, (por ejemplo, un usuario puede requerir que un trabajo corra en
 un host que cumpla la condición 
\begin_inset Quotes eld
\end_inset

Solaris o Linux pero no Linux en IA64
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Paragraph
Distributed Resource Management Application API (DRMAA) es un conjunto de
 APIs standard desarrollado por Global Grid Forum para application builders,
 portal builders e ISV's.
 La versión 6.1 soporta los últimos C y Java bindings de DRMAA 1.0.
 Adicionalmente, es provisto de compatibilidad hacia atrás para: DRMAA 0.5
 Java binding y DRMAA 0.95 C binding.
\end_layout

\begin_layout Paragraph
Guarda la información referida a la cuenta de cada trabajo en una base de
 datos relacional (soportando Oracle, MySQL y PostgreSQL).
\end_layout

\begin_layout Paragraph
Maneja aplicaciones paralelas (MPI o PVM habilitados) a travez de una interface
 dedicada.
 
\end_layout

\begin_layout Paragraph
Permite la distribución de recursos a equipos o departamentos, por ejemplo
 proporcionalmente a su contribución económica.
 
\end_layout

\begin_layout Paragraph
El código muestra una complejidad alta, además el sistema es integrado,
 no muestra facilidades a la hora de dividirlo en subsistemas.
 Estas carácterísticas hacen que en algunos aspectos se vea como un sistema
 cerrado, que no permite adaptaciones en caso de realidades no abarcadas
 por el mismo.
\end_layout

\begin_layout Subsection
Resumen
\end_layout

\begin_layout Standard
Sun grid engine 6.1 provee la mayoría de las funcionalidades requeridas para
 el proyecto.
 Por otro lado, tiene como desventaja el ser un único gran paquete indivisible,
 lo cual hace muy difícil cualquier adaptación o modificación de su funcionamien
to.
 
\end_layout

\begin_layout Standard
Si nos basamos en la historia reciente del producto, en una versión anterior
 se encontró un problema de seguridad (un salto en las restricciones) y
 quienes lo estaban utilizando en ese momento descartaron la idea de buscar
 el problema y solucionarlo, limitándose sólamente a esperar que los creadores
 de la herramienta lo corrigieran.
 
\end_layout

\begin_layout Standard
Esto se debe a que la herramienta cuenta con un código muy complejo y dificil
 de modificar, lo que lo coloca como una opción poco vialble para ser utilizada
 dentro del proyecto propuesto.
\end_layout

\begin_layout Section
TORQUE
\end_layout

\begin_layout Standard
TORQUE (Terascale Open-source Resource and QUEue manager) se trata de un
 'fork' open-source de la versión 2.3.12 de OpenPBS mantenido por Cluster
 Resources.
 
\end_layout

\begin_layout Standard
Incorpora muchas mejoras con respecto al proyecto PBS original provistas
 por NCSA (National Center of Supercomputing Applications), OSC (Ohio Supercompu
ter Center), Sandia, PNNL (Pacific Northwest National Laboratory), y otros
 centros HPC junto con las mejoras desarrolladas por Cluster Resources.
\end_layout

\begin_layout Subsection
Requerimientos
\end_layout

\begin_layout Standard
Plataformas soportadas:
\end_layout

\begin_layout Itemize
Linux
\end_layout

\begin_layout Itemize
UNIX
\end_layout

\begin_layout Subsection
Características generales
\end_layout

\begin_layout Standard
Los trabajos son manejados en el cluster de la misma manera que lo hace
 OpenPBS.
 Cuenta con un conjunto de colas de trabajos que definen propiedades generales
 para los trabajos que contienen (p.ej.
 recursos disponibles, etc.) y al ingresar un trabajo al sistema, este debe
 especificar características especificas de si mismo que ayuden al sistema
 a planificar la ejecución del mismo (p.ej.
 nodos a utilizar, memoria máxima a utilizar, etc.).
 Con esta información (los datos de la cola a la que pertenece el trabajo
 y los datos específicos del trabajo en cuestión) el scheduler decide el
 momento en que un trabajo comienza a ejecutarse y con que recursos cuenta
 en cada instante.
\end_layout

\begin_layout Standard
Existen tres tipos de nodos en el sistema:
\end_layout

\begin_layout Itemize
Nodo Maestro
\end_layout

\begin_deeper
\begin_layout Standard
Es necesario que exista un nodo maestro en el sistema, en este nodo se debe
 ejecutar pbs_server.
 Dependiendo del sistema este nodo maestro puede encontrarse dedicado únicamente
 a este rol o compartir otros roles.
\end_layout

\end_deeper
\begin_layout Itemize
Nodos Interactivos
\end_layout

\begin_deeper
\begin_layout Standard
Los nodos interactivos proveen un punto de entrada al sistema para los usuarios.
 Es en estos nodos en que los usuarios puede ingresar tareas al sistema
 y monitorear su progreso.
 Estos nodos deben tener comandos como qsub, qhold, etc disponibles.
\end_layout

\end_deeper
\begin_layout Itemize
Nodos de computo
\end_layout

\begin_deeper
\begin_layout Standard
Estos son los responsables de la ejecución de los trabajos encolados en
 el sistema.
 En cada uno de estos nodos debe ejecutarse pbs_mom (Machine Oriented Mini-serve
r) que es el encargado de iniciar, detener y manejar los trabajos encolados.
\end_layout

\end_deeper
\begin_layout Standard
Torque cuenta con una interfaz de usuario de linea de comando (CLI) como
 principal manera de interactuar con el sistema, también cuenta con una
 interfaz gráfica para X-Windows y una biblioteca para desarrollo en C.
 Es posible utilizar bibliotecas desarrolladas por 3eros para otros lenguajes
 (p.ej.
 Perl o Python).
\end_layout

\begin_layout Subsection
Bibliotecas para programación paralela
\end_layout

\begin_layout Subsubsection
MPI (Message Passing Interface)
\end_layout

\begin_layout Standard
El soporte para bibliotecas tipo MPI se encuentra integrado en TORQUE, el
 sistema puede correr con cualquier implementación de MPI.
 Particularmente para MPICH existe un reemplazo para el script mpirun (mpirun
 se encuentra incluido en el paquete de mpich) llamado mpiexec.
 Mpiexec es usado para inicializar trabajos paralelos dentro de un sistema
 PBS.
 Los recursos utilizados por procesos paralelos son correctamente registrados
 cuando se utiliza mpiexec y se reporta su uso en el log del PBS, a diferencia
 de lo que sucede cuando se utiliza el script mpirun original.
\end_layout

\begin_layout Subsubsection
PVM (Parallel Virtual Machine)
\end_layout

\begin_layout Standard
Una de las principales desventajas de TORQUE es la dificultad que plantea
 el uso de la biblioteca PVM.
 A diferencia de MPI, el soporte para PVM no se encuentra integrado al sistema
 por lo que debe manejarse cuidadosamente.
 Para ejecutar una aplicación paralela utilizando PVM se debe iniciar el
 demonio pvmd en el script del trabajo, configurar los nodos esclavos para
 luego iniciar la ejecución del trabajo (todo esto puede realizarse utilizando
 mpiexec).
\end_layout

\begin_layout Subsubsection
Interfaz de programación
\end_layout

\begin_layout Standard
Torque soporta el estandar Distributed Resource Management Application API
 (DRMAA) al igual que la mayoria de los manejadores de recursos.
\end_layout

\begin_layout Subsection
Maui
\end_layout

\begin_layout Standard
TORQUE contiene la lógica necesaria para llevar a cabo la planificación
 de trabajos, pero se trata de una lógica muy simple que no resulta adecuada
 para un ambiente de producción.
 Básicamente el planificador que se encuentra incorporado a TORQUE maneja
 los trabajos como una cola FIFO (First-In First-Out).
 Para mejorar esto se utiliza Maui, una aplicación especializada en la planifica
ción de trabajos.
\end_layout

\begin_layout Standard
Maui se enfoca en la planificación de trabajos y deja la problemática de
 iniciar los trabajos y la interacción con los usuarios a los manejadores
 de recursos (distributed resource managers, DRM) como OpenPBS, TORQUE,
 SGE, etc.
\end_layout

\begin_layout Subsubsection
Requerimientos
\end_layout

\begin_layout Itemize
Hardware
\end_layout

\begin_deeper
\begin_layout Itemize
20-50 MB de RAM (para clusters de hasta 10 teraflops)
\end_layout

\begin_layout Itemize
>20 MB de disco duro para los fuentes, binarios, estadísticas y archivos
 de logs
\end_layout

\end_deeper
\begin_layout Itemize
Plataformas soportadas
\end_layout

\begin_deeper
\begin_layout Itemize
Linux
\end_layout

\begin_layout Itemize
AIX
\end_layout

\begin_layout Itemize
OSF/Tru-64
\end_layout

\begin_layout Itemize
Solaris
\end_layout

\begin_layout Itemize
HP-UX
\end_layout

\begin_layout Itemize
IRIX
\end_layout

\begin_layout Itemize
FreeBSD
\end_layout

\begin_layout Itemize
Other UNIX platforms
\end_layout

\end_deeper
\begin_layout Itemize
DRM soportados
\end_layout

\begin_deeper
\begin_layout Itemize
OpenPBS
\end_layout

\begin_layout Itemize
'Scalable' Open PBS
\end_layout

\begin_layout Itemize
PBSPro
\end_layout

\begin_layout Itemize
Sun Grid Engine (SGE)
\end_layout

\begin_layout Itemize
SGE Enterprise Edition
\end_layout

\begin_layout Itemize
LoadLeveler
\end_layout

\begin_layout Itemize
LSF
\end_layout

\begin_layout Itemize
BProc/Scyld
\end_layout

\begin_layout Itemize
Scalable System Software (SSSRM)
\end_layout

\end_deeper
\begin_layout Subsubsection
Características generales
\end_layout

\begin_layout Standard
El algoritmo de planificación de Maui soporta fairness, preemption, backfill,
 etc.
 y tiene una interfaz para la interacción con un allocation management externo.
 Un allocation manager (también conocido como allocation bank o cpu bank)
 funciona como un banco en el cual la moneda son los recursos del sistema
 (p.ej.
 procesadores, memoria, etc.) autorizando a los trabajos cierta cantidad
 de recursos.
\end_layout

\begin_layout Paragraph
Backfill
\end_layout

\begin_layout Standard
Backfill es un acercamiento en la planificación que permite ejecutar algunos
 trabajos 'desordenadamente' siempre y cuando estos no retrasen los trabajos
 de prioridad superior de la cola.
 Para determinar si un trabajo será retrasado, cada trabajo debe proveer
 una estimación de cuánto tiempo necesitará para su ejecución.
 Esta estimación, conocida como límite wallclock, es una valoración del
 tiempo desde el comienzo del trabajo hasta su final.
 Es a menudo sabio sobrestimar levemente este límite porque el planificador
 se puede configurar para matar a los trabajos que exceden sus límites del
 wallclock.
 Sin embargo, la sobrestimación demasiado grande del tiempo del wallclock
 de un trabajo evitará que el planificador pueda optimizar correctamente
 la cola de trabajo.
 Cuanto más exacto el límite del wallclock, mayor sera la posibilidad de
 que Maui encuentre agujeros en la planificación para comenzar a ejecutar
 su trabajo con mayor anticipación.
 
\end_layout

\begin_layout Paragraph
Gerenciamiento de asignación
\end_layout

\begin_layout Standard
Maui posee interfaces para sistemas de gerenciamiento de asignación tales
 como Gold de PNNL.
 Estos sistemas permiten que a cada usuario le sea asignada una porción
 de los recursos totales de cálculo disponibles en el sistema.
 Estos sistemas trabajan asociando a cada usuario a unas o más cuentas.
 Cuando se envía un trabajo, el usuario especifica a que cuenta se debe
 cargar los recursos consumidos por el trabajo.
\end_layout

\begin_layout Paragraph
Reservas anticipadas
\end_layout

\begin_layout Standard
Las reservas anticipadas permiten que un sitio disponga ciertos recursos
 a un lado para el uso específico de de ciertas aplicaciones durante cierto
 tiempo.
 El acceso a una reserva dada es controlado por un Access Control List (ACL)
 que determina quién puede utilizar los recursos reservados.
 Es importante observar que mientras que un ACL permite que trabajos particulare
s utilicen recursos reservados, no fuerzan al trabajo a utilizar estos recursos.
 Maui procurará utilizar la mejor combinación posible de recursos disponibles
 sean éstos reservados o no.
 Maui puede ser configurado para que ciertos trabajos sean restringidos
 y que funcionen utilizando solamente recursos reservados, aplicando restriccion
es a nivel de trabajo o especificando ciertas restricciones especiales de
 QoS.
 
\end_layout

\begin_layout Paragraph
Quality of Service (QoS)
\end_layout

\begin_layout Standard
Las funciones de QoS permiten otorgar ciertos privilegios especiales a usuarios,
 estos beneficios pueden incluir acceso a recursos adicionales, exclusiones
 de determinadas políticas, acceso a capacidades especiales, y mejoras en
 la priorizacion de trabajos.
\end_layout

\begin_layout Paragraph
Faireshare
\end_layout

\begin_layout Standard
Este componente permite favorecer trabajos en base al uso histórico a corto
 plazo.
 Es posible así ajustar la prioridad de un trabajo dependiendo de la utilización
 porcentual del sistema de usuarios, grupos, o QoS.
 Dada una ventana de tiempo determinado sobre la cual se evalúa la utilización
 de recursos del sistema se determina si esta siendo mantenido un cierto
 balanceo o no.
\end_layout

\begin_layout Subsubsection
Interfaz de programación
\end_layout

\begin_layout Itemize
Interfaz de extensión (Extension Interface)
\end_layout

\begin_deeper
\begin_layout Itemize
Esta interfaz permite que bibliotecas externas sean 'linkeadas' al servidor
 de Maui brindando acceso a todos los datos y objetos utilizados por el
 planificador.
 Además, permite que estas bibliotecas realicen override de las principales
 funciones de Maui.
\end_layout

\end_deeper
\begin_layout Itemize
Interfaz local
\end_layout

\begin_deeper
\begin_layout Itemize
Se trata de una interfaz en C que permite el desarrollo de nuevos algoritmos.
\end_layout

\end_deeper
\begin_layout Subsubsection
Estadísticas
\end_layout

\begin_layout Standard
Maui almacena tres diferentes clases de estadísticas:
\end_layout

\begin_layout Itemize
Estadísticas de tiempo real
\end_layout

\begin_deeper
\begin_layout Itemize
Estas estadísticas son mantenidas en memoria y pueden ser consultadas mediante
 comandos.
 El comando 'showstats' provee información detallada por usuario, por grupo,
 por cuenta o por nodo.
 Además en cualquier momento estas estadísticas pueden resetearse utilizando
 el comando 'resetstats'.
 
\end_layout

\end_deeper
\begin_layout Itemize
Histórico
\end_layout

\begin_deeper
\begin_layout Itemize
Estas estadísticas pueden ser obtenidas para un lapso de tiempo, un tipo
 de trabajo y/o una porción de recursos utilizando el comando 'profiler'.
 Este comando trabaja con la traza de información detallada de un trabajo,
 que es guardada al dar por finalizado un trabajo.
 Estas trazas son almacenadas en el directorio configurado por el parámetro
 STATDIR (por defecto $(MAUIHOMEDIR)/stats) en archivos utilizando el formato
 WWW_MMM_DD_YYYY (p.
 ej.
 Mon_Jul_16_2001), siendo esta fecha la fecha de finalización del trabajo.
 La traza de un trabajo se almacena en texto plano utilizando espacios como
 separadores, por lo que puede ser analizado directamente con cualquier
 editor de texto.
 
\end_layout

\end_deeper
\begin_layout Itemize
Fairshare
\end_layout

\begin_deeper
\begin_layout Itemize
Este tipo de estadísticas son mantenidas sin importar si fairshare se encuentra
 habilitado.
 Al igual que las trazas de los trabajos, estas son almacenadas en archivos
 utilizando texto plano en el directorio configurado por el parámetro STATDIR
 y utilizando el formato FS.<EPOCHTIME> (p.ej., FS.982713600) por cada ventana
 de fairshare.
 Se puede obtener información de estos archivos utilizando el comando 'diagnose
 -f'.
 
\end_layout

\end_deeper
\begin_layout Subsection
Gold
\end_layout

\begin_layout Standard
Gold es un sistema de contaduría open-source desarrollado en PNNL bajo el
 proyecto Scalable Systems Software (SSS) que lleva registro y maneja el
 uso de recursos en clusters de alto desempeño.
 Actúa de la misma forma que un banco en el que son depositados créditos
 en cuentas, estos créditos representan recursos en el sistema.
 A medida que se finalizan trabajos o que son consumidos recursos en el
 sistema se debitan créditos de sus respectivas cuentas.
\end_layout

\begin_layout Standard
Es posible realizar operaciones como depósitos, retiros, transferencias
 o reembolsos sobre las cuentas del sistema, además, provee listados de
 balances a usuarios y administradores.
\end_layout

\begin_layout Subsubsection
Características generales
\end_layout

\begin_layout Paragraph
Reservas
\end_layout

\begin_layout Standard
Previo al inicio de un trabajo se realiza una estimación del total de recursos
 que este consumirá, en base a esta estimación se reservan créditos en la
 cuenta correspondiente.
 Estos créditos 'reservados' se debitan una vez que el trabajo es terminado,
 de esta manera se evita que un proyecto consuma mas recursos de los que
 tiene asignados.
\end_layout

\begin_layout Paragraph
Vencimiento de créditos
\end_layout

\begin_layout Standard
Puede especificarse un lapso de validez a los créditos en el sistema, permitiend
o que se implemente una política de use-it-or-lose-it previniendo el uso
 exhaustivo de créditos acumulados y estableciendo ciclos a un proyecto.
\end_layout

\begin_layout Paragraph
Interfaz web
\end_layout

\begin_layout Standard
Permitiendo acceso remoto a usuarios y administradores.
 
\end_layout

\begin_layout Paragraph
Interfaz de programación
\end_layout

\begin_layout Standard
Existen diferentes formas de integrar Gold al sistema: Perl API, Java API
 o directamente utilizando el protocolo SSSRMAP (basado en XML).
\end_layout

\begin_layout Subsection
Casos de estudio
\end_layout

\begin_layout Paragraph
Oregon State University, Laboratorio de fisica
\end_layout

\begin_layout Standard
Actualmente cuenta con 34 computadoras Dell Optiplex GX620's con procesadores
 Intel Pentium D 830 (3.0 GHz) y 1 GB of RAM con sistema operativo Suse GNU/Linux
 10.1 64-bit.
\end_layout

\begin_layout Standard
Estas computadoras cuentan con compiladores Intel para C, C++ y Fortran
 compilers, la bilbioteca Math Kernel Library (cluster edition).
 Para el procesamiento paralelo se utilizan las bibliotecas MPI.
 El cluster utiliza Torque como manejador de recursos, y desde el 4 de febrero
 del 2007 se utiliza Maui para la planificacion de tareas.
 
\end_layout

\begin_layout Paragraph
University of Glasgow
\end_layout

\begin_layout Standard
El cluster utiliza Torque y Maui sobre Redhat Enterprise GNU/Linux 3.
 Actualmente cuenta con 60 nodos de procesamiento disponibles, cada uno
 con procesadores dual opteron 248 y 2GB RAM.
 Ademas de los componentes estandares de incluidos en la distribucion (p.ej.
 compilador gcc, g++, g77, etc.) se han instalado las bibliotecas mpich 1.2.6
 (MPI) para procesamiento paralelo.
\end_layout

\begin_layout Paragraph
University of Heidelberg
\end_layout

\begin_layout Standard
El cluster fue instalado a principios de 2002 y consisten de 512 procesadores
 AMD Athlon MP, instalados en 256 nodos de procesamiento SMP con 2 GB de
 memoria RAM cada uno.
 Los procesadores funcionan cada uno a 1.4GHz y alcanzan un maximo teorico
 de 2.4 billiones de operaciones de punto flotante por segundo (Gflops).
 El sistema total indica un maximo teorico de desempeño de mas de 1.4 Teraflops.
 Las primeras mediciones de desempeño utilizando Linpack Benchmark mostraron
 una rendimiento de 825 Gflops, ubicando el cluster en la posicion 35va
 del top 500 de supercomputadoras del mundo en Junio del 2002.
\end_layout

\begin_layout Standard
Como base del cluster se utiliza un sistema Debian GNU/Linux.
 Para procesamiento paralelo se utiliza la biblioteca mpich (MPI) y como
 manejador de recursos se utiliza Torque.
 Por sobre Torque se encuentra instalado Moab, el sucesor de Maui.
 
\end_layout

\begin_layout Paragraph
Stony Brook University
\end_layout

\begin_layout Standard
El cluster tiene 235 nodos de procesamiento dual (470 procesadores individuales).
 Cada procesador es un Dell Pentium IV Xeon de 3.4Ghz con 2GB de memoria
 RAM y 40GB de disco duro.
 Las computadores operan con Debian GNU/Linux, utilizan Torque y Maui para
 manejar los trabajos y la version 1.4 de MPI para el procesamiento paralelo.
\end_layout

\begin_layout Paragraph
Dansk Center for Scientific Computing
\end_layout

\begin_layout Standard
El cluster consta de 200 nodos Dell PowerEdge 1950 1U con dos procesadores
 Intel Woodcrest de 2,66Ghz.
 De estos 200 nodos 160 cuentan con 4 GB de RAM y 40 cuentan con 8GB de
 RAM.
 Como sistema se utiliza OpenSuSE GNU/Linux 10.1 con kernel 2.6 y para la
 planificacion y manejo de trabajos de utiliza Torque 2.1.2 y MAUI 3.2.6.
 Para el procesamiento paralelo se disponen de bibliotecas tanto de tipo
 MPI como de tipo PVM.
\end_layout

\begin_layout Subsection
Conclusiones
\end_layout

\begin_layout Standard
Torque junto con Maui y Gold satisfacen los requerimientos planteados para
 el proyecto.
 Su consumo de recursos del cluster es relativamente bajo, se adapta muy
 bien al manejo de clusters pequeños y posee un manejo eficiente de trabajos
 paralelos (sobre todo si estos son homogéneos).
 
\end_layout

\begin_layout Standard
Pero existen ciertos aspectos en los que Torque no se desempeña de la mejor
 manera.
 Si bien se encuentra integrado el soporte para bibliotecas tipo MPI, no
 existe un correcto soporte para PVM.
 Además existen ciertas carencias en sus funcionalidades de Resource Management;
 p.
 ej.
 no es capaz de realizar CPU Harvesting o migración de procesos.
 
\end_layout

\begin_layout Standard
Si bien existe un buen soporte por parte de la comunidad de usuarios de
 Torque y Maui por medio de listas de correo, la documentación disponible
 en el sitio web se encuentra incompleta en algunos aspectos (p.
 ej.
 PVM, interfaces de programación, etc.).
 Como es de esperar esta documentación se encuentra mas completa para Moab
 (la versión comercial de Maui), y si bien son productos diferentes son
 similares en muchos aspectos.
\end_layout

\begin_layout Bibliography

\bibitem [1]{key-1}
B.
 Radic, E.
 Imamagic: Benchmarking the Performance of JMS on Computer Clusters, CARNet
 Users' Conference, 28.
 9.
 2004.
\end_layout

\begin_layout Bibliography

\bibitem [2]{key-2}
E.
 Imamagic, B.
 Radic, D.
 Dobrenic: Job Management Systems Analysis, CARNet Users' Conference, 28.
 9.
 2004.
\end_layout

\begin_layout Bibliography

\bibitem [3]{key-3}
Maui Cluster Scheduler, Cluster Resources.
 http://www.clusterresources.com/products/maui/
\end_layout

\begin_layout Bibliography

\bibitem [4]{key-4}
Torque Resource Manager, Cluster Resources.
 http://www.clusterresources.com/products/torque/
\end_layout

\begin_layout Bibliography

\bibitem [5]{key-5}
William Gropp, Ewing Lusk and Thomas Sterling.
 Beowulf Cluster Computing with Linux, Second Edition.
 The MIT press, 2003 ISBN:0262692929 
\end_layout

\begin_layout Bibliography

\bibitem [6]{key-6}
M.
 Michels, W.
 Borremans: Clustering with openMosix, University of Amsterdam, February
 2005.
\end_layout

\begin_layout Bibliography

\bibitem {key-7}
http://es.wikipedia.org/wiki/Single_System_Image
\end_layout

\begin_layout Bibliography

\bibitem {key-8}
Dansk Center for Scientific Computing, http://www.dcsc.sdu.dk/
\end_layout

\begin_layout Bibliography

\bibitem {key-9}
Stony Brook University, http://www.sunysb.edu/seawulfcluster/
\end_layout

\begin_layout Bibliography

\bibitem {key-10}
University of Heidelberg, http://helics.uni-hd.de/
\end_layout

\begin_layout Bibliography

\bibitem {key-11}
University of Glasgow, http://www.gla.ac.uk/services/it/whatwedo/computecluster/
\end_layout

\begin_layout Bibliography

\bibitem {key-12}
Oregon State University, Laboratorio de fisica, http://physics.oregonstate.edu/~el
serj/support/cluster_use.php
\end_layout

\begin_layout Bibliography

\bibitem {key-13}
Distributed Resource Management Application API (DRMAA), http://en.wikipedia.org/w
iki/DRMAA/
\end_layout

\end_body
\end_document
