\documentclass[a4paper,10pt,spanish]{article}

\usepackage[latin1]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{color}

%opening

\begin{document}
\title{Manual de instalaci\'{o}n\\
Cluster de Computadores de Alto Desempe\~{n}o con Acceso Remoto (CCADAR)\\
Facultad de Ingenier\'{i}a\\
Universidad de la Rep\'{u}blica}

\author{Santiago Iturriaga, Paulo Maya, Dami\'{a}n Pintos}
\date{}

\maketitle
\begin{abstract}
Que contiene el documento, en que consiste el proyecto...
\end{abstract}
\newpage{}\tableofcontents{}\newpage{}

\section{Sistema operativo}

Se recomienda utilizar Linux 2.6 (o superior), con una arquitectura de 32-bits (x86) o 64-bits (x64). Si bien el sistema fue probado en SuSE, Ubuntu y Fedora, esto no quita que pueda realizarse una instalaci\'{o}n en otras distribuciones (Debian, Slackware, etc.) o hasta otro sistema operativo de la familia POSIX (FreeBSD, Solaris, etc).

Durante le resto del documento se asumir\'{a} que se utiliza Linux.

\section{Pre-instalaci\'{o}n}

\paragraph{Usuario y grupo del sistema.}

El sistema necesita contar con un usuario y un grupo para ejecutar. Este usuario del sistema debe ser configurado como administrador en algunos de los programas utilizados (p.ej.: Maui, TORQUE, etc.). Por defecto este usuario y este grupo se llamar\'{a}n fenton.

Para crearlos es debemos ejecutar:

\begin{verbatim}
root:# addgroup fenton
root:# adduser fenton
root:# adduser fenton fenton
\end{verbatim}

\section{Requerimientos de software}

A continuaci\'{o}n detallaremos el software requerido por el sistema. Por simplicidad es recomendable realizar la instalaci\'{o}n del software utilizando los paquetes de la distribuci\'{o}n de Linux sobre la que se est\'{e} trabajando (apt-get, yast2, etc.).

A continuaci\'{o}n veremos un breve instructivo de como realizar la instalaci\'{o}n compilando e instalando manualmente utilizando los fuentes.

\subsection{PostgreSQL 8.2.5 o superior \small{[requerido]}}

PostgreSQL\cite{postgres} es un servidor de base de datos relacional liberado bajo la licencia BSD. El sistema utiliza PostgreSQL para persistir la informaci\'{o}n sobre usuarios, trabajos, etc.

Para compilar e instalar PostgreSQL en \mbox{\texttt{/usr/local/postgresql-8.2.5}} se deben seguir los siguientes pasos:

\begin{verbatim}
$ ./configure --prefix=/usr/local/postgresql-8.2.5
$ make
root:# make install
\end{verbatim}

Luego se debe crear el usuario de PostgreSQL y se debe asignar espacio f\'{i}sico para los datos y logs.

\begin{verbatim}
root:# adduser postgres 
root:# mkdir /usr/local/postgresql-8.2.5/data 
root:# chown postgres /usr/local/postgresql-8.2.5/data 
postgres:$ cd /usr/local/postgresql-8.2.5/bin
postgres:$ ./initdb -D /usr/local/postgresql-8.2.5/data 
\end{verbatim}

Finalmente iniciamos el servicio:

\begin{verbatim}
postgres:$ cd /usr/local/postgresql-8.2.5/bin
postgres:$ ./pg_ctl -D /usr/local/postgresql-8.2.5/data
\end{verbatim}

El sistema requiere que la base de datos debe sea accesible desde todos los nodos del cluster (tanto el nodo maestro como los nodos de computo). Para habilitar el acceso externo a la base de datos se deben editar los siguientes archivos de configuraci\'{o}n:
\begin{verbatim}
/net/local/postgresql-8.2.5/data/pg_hba.conf
/net/local/postgresql-8.2.5/data/postgresql.conf
\end{verbatim}

\subsection{Apache 2.0 o superior \small{[requerido]}}

El servidor HTTP Apache\cite{apache} es un servidor HTTP de c\'{o}digo abierto. Fenton utiliza un interfaz de usuario web para la ejecuci\'{o}n de trabajos y la administraci\'{o}n del cluster por lo que es necesario contar con un servidor HTTP.

En la actualidad todas las distribuciones de Linux disponen de un paquete de Apache 2.0 o superior. Recomendamos utilizar este paquete para la instalaci\'{o}n.

\subsection{PHP 5.2.5 o superior \small{[requerido]}}

PHP\cite{php} es un lenguaje de programaci\'{o}n interpretado, dise\~{n}ado originalmente para la creaci\'{o}n de p\'{a}ginas web din\'{a}micas. Fenton se encuentra casi completamente implementando en PHP y requiere que este se encuentre instalado en todos los nodos del cluster. En el nodo maestro es utilizado para la ejecuci\'{o}n de la interfaz web y la l\'{o}gica del sistema. En los nodos de computo es utilizado para ejecutar scripts de pre-ejecuci\'{o}n y post-ejecuci\'{o}n de trabajos en el cluster.

Para compilar e instalar PHP en el directorio \mbox{\texttt{/usr/local/php-5.2.5}}:

\begin{verbatim}
$ ./configure --prefix=/usr/local/php-5.2.5 \
> --with-pgsql=/usr/local/postgresql-8.2.5 --with-gd 
$ make 
root:# make install
\end{verbatim}

Es necesario incluir soporte para PostgreSQL y para la biblioteca gr\'{a}fica GD (utilizada por Ganglia). Por \'{u}ltimo es necesario configurar el servidor HTTP Apache para poder ejecutar archivos PHP.

\subsection{Ganglia 3.0.5 o superior \small{[requerido]}}

Ganglia es utilizado para monitorear el estado del cluster: carga de memoria de los nodos, tr\'{a}fico de red, utilizaci\'{o}n de CPU, etc. 

Ganglia cuenta con tres componentes que deben ser instalados:

\subsubsection{gmond}

Gmond es un demonio encargado de recolectar y enviar informaci\'{o}n del estado del nodo sobre el que esta ejecutando. Por esta raz\'{o}n debe correr en cada una de los nodos del cluster que se desea monitorear.

Para compilar e instalar gmond se debe ejecutar:

\begin{verbatim}
$ ./configure --prefix=/usr/local/ganglia-3.0.7
$ make
root:# make install
\end{verbatim}

Dentro del directorio \mbox{\texttt{ganglia-3.0.7-src/gmond}} podremos encontrar algunos scripts de ejemplo para ejecutar gmond al inicio del sistema como un servicio. 

Por \'{u}ltimo es necesario configurar gmond. Para esto creamos el archivo de configuraci\'{o}n \mbox{\texttt{/etc/gmond.conf}} con valores por defecto ejecutando:

\begin{verbatim}
root:# cd /usr/local/ganglia-3.0.7/sbin
root:# ./gmond --default_config > /etc/gmond.conf
\end{verbatim}

Una vez creado con valores por defecto podemos configurarlo a gusto.

\subsubsection{gmetad}

Gmetad es un demonio al igual que gmond. Pero en lugar de enviar datos, oficia de servidor recolectando la informaci\'{o}n enviada por gmond desde los nodos del cluster. Solamente una instancia de este demonio debe
estar en ejecuci\'{o}n. Para compilar e instalar gmetad y gmond:

\begin{verbatim}
$ ./configure --prefix=/usr/local/ganglia-3.0.7 --with-gmetad 
$ make
root:# make install
\end{verbatim}

Dentro del directorio \mbox{\texttt{ganglia-3.0.7-src/gmetad}} podremos encontrar algunos scripts de ejemplo para iniciar gmetad como servicio al inicio del sistema. A diferencia de gmond, gmetad viene adem\'{a}s con un archivo de configuraci\'{o}n por defecto que se encuentra en \mbox{\texttt{ganglia-3.0.7-src/gmetad/gmetad.conf}} y que debemos copiar a \mbox{\texttt{/etc/gmetad.conf}} y luego editar.

Finalmente debemos asignar espacio en disco en donde gmetad guardar\'{a} la informaci\'{o}n que recopile. La ubicaci\'{o}n por defecto es \mbox{\texttt{/var/lib/ganglia/rrds}} y su owner debe ser el usuario que fue configurado en \texttt{gmetad.conf} como usuario de ejecuci\'{o}n.

\subsubsection{Interfaz web}

Ganglia cuenta con una interfaz web implementada en PHP donde se presenta visualmente la informaci\'{o}n del cluster. La interfaz web de Ganglia debe estar alojada en el mismo nodo que ejecuta el demonio gmetad.

Para realizar la instalaci\'{o}n de la interfaz web es necesario copiar el directorio \mbox{\texttt{ganglia-3.0.7-src/web}} al directorio \texttt{DocumentRoot} de su instalaci\'{o}n del servidor Apache, por ejemplo: \mbox{\texttt{/var/www/ganglia}}.

\subsection{TORQUE 2.3.0 o superior \small{[requerido]}}

TORQUE\cite{torque} es un manejador de recursos distribuidos (DRM) y es utilizado para administrar los recursos disponibles en el cluster: procesadores, memoria, tiempo de c\'{o}mputo, etc. La arquitectura de un cluster TORQUE cuenta con un nodo maestro y muchos nodos de c\'{o}mputo. El nodo maestro debe ejecutar el demonio \texttt{qserverd} y los nodos de c\'{o}mputo deben ejecutar el demonio \texttt{qnoded}. A continuaci\'{o}n veremos como instalar estos demonios.

\subsubsection{Nodo maestro: qserverd}

Para instalar el servidor de TORQUE en \mbox{\texttt{/usr/local/torque-2.3.0}}:

\begin{verbatim}
$ ./configure --prefix=/usr/local/torque-2.3.0 
$ make 
root:# make install
\end{verbatim}

Una vez que termina la instalaci\'{o}n del servidor, es necesario configurar el demonio qserverd:

\begin{verbatim}
root:# export PATH=$PATH:/usr/local/torque-2.3.0/bin
root:# export PATH=$PATH:/usr/local/torque-2.3.0/sbin
root:# torque.setup fenton
\end{verbatim}

Donde \mbox{\texttt{fenton}} es el usuario que se desempe\~{n}ar\'{a} como administrador de TORQUE. Este script se encarga de crear un entorno b\'{a}sico de trabajo con una configuraci\'{o}n por defecto para poder iniciar el servidor. Finalmente se deben configurar los nodos de computo editando \mbox{\texttt{/var/spool/torque/server\_priv/nodes}}, p. ej.:

\begin{verbatim}
nodo001.fing.edu.uy
nodo002.fing.edu.uy
nodo003.fing.edu.uy
\end{verbatim}

O mediante la interfaz de consola de torque:

\begin{verbatim}
$ qmgr
qmgr: create node nodo001.fing.edu.uy
\end{verbatim}

\subsubsection{Nodo de c\'{o}mputo: qnoded}

Una vez compilado torque es posible crear paquetes de instalaci\'{o}n distribuibles para instalar en los nodos de c\'{o}mputo. Esto simplifica la instalaci\'{o}n de los nodos de computo si se trata de un cluster homog\'{e}neo, en cambio si los nodos del cluster cuentan con sistemas operativos diferentes o arquitecturas diferentes ser\'{a} necesario compilar torque para cada nodo. Asumiremos que se trata de un cluster homog\'{e}neo, para crear los paquetes de instalaci\'{o}n distribuibles se debe ejecutar:

\begin{verbatim}
$ make packages
\end{verbatim}

Y luego en cada nodo de computo:

\begin{verbatim}
root:# ./torque-package-clients-linux-i686.sh --install
root:# ./torque-package-mom-linux-i686.sh --install
\end{verbatim}

El servidor MOM para cada nodo de computo debe ser configurado para confiar en el servidor maestro de TORQUE editando el archivo:

\begin{verbatim}
/var/spool/torque/mom_priv/config
\end{verbatim}

\paragraph{Ejecuci\'{o}n de los demonios.}

Es necesario que el servidor de TORQUE (qserverd) y los demonios qnoded se encuentren en ejecuci\'{o}n permanente en el cluster. Para esto es recomendable que sean agregados como demonios en el sistema creando scripts de ejecuci\'{o}n en \mbox{\texttt{/etc/init.d}}. Para esto existen dos scripts que pueden tomarse de ejemplo:

\begin{verbatim}
scripts/qserverd.example.sh
scripts/qnoded.example.sh
\end{verbatim}

\subsection{Maui 3.2.6p19 o superior \small{[requerido]}}

Maui\cite{maui} es un despachador (scheduler) de c\'{o}digo abierto de trabajos para clusters que soporta varias pol\'{i}ticas de despacho, prioridades din\'{a}micas, reservas de recursos, etc.

\paragraph{Compilaci\'{o}n, instalaci\'{o}n y configuraci\'{o}n.}

Existe un bug en el script de configuraci\'{o}n e instalaci\'{o}n de Maui por lo que recomendamos realizar la instalaci\'{o}n en \mbox{\texttt{/usr/local/maui}} (su ubicaci\'{o}n por defecto). Adem\'{a}s es necesario estar logueado con el usuario de Linux que se desempe\~{n}ar\'{a} como administrador del scheduler al momento de la compilaci\'{o}n. Este usuario debe ser tambi\'{e}n administrador de TORQUE, por lo que recomendamos utilizar el mismo usuario se asign\'{o} durante el paso anterior como administrador de TORQUE (fenton en nuestro ejemplo). Una vez con este usuario, se debe ejecutar:

\begin{verbatim}
fenton:$ ./configure --prefix=/usr/local/maui \
> --with-pbs=/usr/local/torque-2.3.0
fenton:$ make 
root:# mkdir -p /usr/local/maui
root:# chown fenton.fenton /usr/local/maui
fenton:$ make install
\end{verbatim}

Es posible que ocurra un error durante la creaci\'{o}n de directorios en la instalaci\'{o}n. Si sucede esto es necesario crear manualmente los directorios: 'log', 'traces', 'stats', 'spool' y 'tools' en el directorio de instalaci\'{o}n y luego volver a ejecutar \mbox{\texttt{make install}}. 

Debemos asegurarnos que el usuario administrador de Maui tenga permisos totales sobre el directorio de instalaci\'{o}n. Una posibilidad para asegurar esto es asignarlo como owner ejecutando:

\begin{verbatim}
root:# chown -R fenton /usr/local/maui
\end{verbatim}

Luego de esto damos por finalizada la instalaci\'{o}n. Ahora es necesario configurar el scheduler editando \mbox{\texttt{/usr/local/maui/maui.cfg}}:

\begin{verbatim}
RMCFG[SERVIDOR.FING.EDU.UY] TYPE=PBS
\end{verbatim}

Para clusters peque\~{n}os quiz\'{a}s sea recomendable disminuir el tiempo de polling para tener una respuesta mas r\'{a}pida por parte del scheduler modificando el par\'{a}metro \texttt{RMPOLLINTERVAL}, p.ej.:

\begin{verbatim}
RMPOLLINTERVAL 00:00:05
\end{verbatim}

\paragraph{Ejecuci\'{o}n del demonio.}

Al igual que TORQUE, tambi\'{e}n es necesario que Maui se encuentre en ejecuci\'{o}n constante en el cluster. Y de la misma forma que antes, tambi\'{e}n existe un script que puede tomarse de ejemplo para esto en: \mbox{\texttt{scripts/maui.example.sh}}.

\subsection{MPI \small{[opcional]}}

Para ejecutar programas paralelos utilizando MPI es necesario contar con una implementaci\'{o}n del est\'{a}ndar. A continuaci\'{o}n veremos las implementaciones que fueron probadas en el sistema.

Cualquiera sea la implementaci\'{o}n elegida, es necesario que sea instalada (o se encuentra accesible) en todos los nodos del cluster.

\subsubsection{MPICH 1.2.7p1 o superior}

MPICH1\cite{mpich1} es una implementaci\'{o}n del est\'{a}ndar MPI versi\'{o}n 1. Para compilar e instalar MPICH1 en \mbox{\texttt{/usr/local/mpich-1.2.7p1}} se debe ejecutar:

\begin{verbatim}
$ ./configure --with-device=ch_p4 \
> --prefix=/usr/local/mpich-1.2.7p1 \
> --with-common-prefix=/usr/local/mpich-1.2.7p1 
$ make 
root:# make install
\end{verbatim}

\subsubsection{Mpiexec 0.83 o superior}

Mpiexec\cite{mpiexec} es un programa que reemplaza al script mpirun del paquete MPICH y es utilizado para inicializar un trabajo paralelo desde TORQUE. Si bien no es obligatorio utilizar Mpiexec, este facilita la integraci\'{o}n de MPICH con TORQUE y es muy recomendable utilizarlo.

\begin{verbatim}
$ ./configure --prefix=/usr/local/mpiexec-0.83 \
> --with-default-comm=mpich-p4 \
> --with-pbs=/usr/local/torque-2.3.0 \
> --with-mpicc=/usr/local/mpich-1.2.7p1/bin/mpicc 
$ make 
root:# make install
\end{verbatim}

\begin{quote}
NOTA: Mpiexec no es necesario si se utiliza OpenMPI.
\end{quote}

\subsubsection{OpenMPI 1.2.6 o superior \small{[recomendado]}}

OpenMPI\cite{openmpi} es una implementaci\'{o}n del est\'{a}ndar MPI versi\'{o}n 2. Para compilar e instalar OpenMPI se debe ejecutar:

\begin{verbatim}
$ ./configure --prefix=/usr/local/openmpi-1.2.6 \
> --with-tm=/usr/local/torque-2.3.0
> --disable-shared --enable-static
$ make
root:# make install
\end{verbatim}

Compilar OpenMPI de forma est\'{a}tica tiene la ventaja de que evitamos problemas de bibliotecas en los diferentes nodos. Aunque tiene dos desventajas: los ejecutables tienen un tama\~{n}o mayor y si las bibliotecas de OpenMPI son actualizadas es necesario re-compilar los proyectos y generar nuevos ejecutables para utilizar la nueva versi\'{o}n.

\section{Instalaci\'{o}n de Fenton}

\subsection{Repositorio del sistema}

Es necesario crear el repositorio ra\'{i}z donde el sistema almacenar\'{a} la estructura de clientes y trabajos para que los usuarios realicen ejecuciones y almacenen sus resultados. Por ejemplo, creamos el repositorio en \mbox{\texttt{/home/fenton/repositorio}} y luego le asignamos los permisos adecuados.

\begin{verbatim}
fenton:$ mkdir -p /home/fenton/repositorio
fenton:$ chmod g+w+r /home/fenton/repositorio
\end{verbatim}

El grupo del directorio debe ser \mbox{\texttt{fenton}}. En caso de que sea otro debemos cambiarlo con el comando:

\begin{verbatim}
root:# chgrp fenton /home/fenton/repositorio
\end{verbatim}

Finalmente debemos crear un espacio para el repositorio interno del sistema. Aqu\'{i} el sistema almacenar\'{a} logs de ejecuciones y otros datos que no estar\'{a}n directamente disponibles a los usuarios. La ubicaci\'{o}n recomendada para este directorio es \mbox{\texttt{/home/fenton/repositorio/sistema}}. Para crearlo:

\begin{verbatim}
fenton:$ mkdir -p /home/fenton/repositorio/sistema
fenton:$ chmod g+w+r /home/fenton/repositorio/sistema
\end{verbatim}

Al igual que en el directorio anterior, debemos asegurarnos que el grupo del directorio sea \mbox{\texttt{fenton}}.

El usuario y el grupo \mbox{\texttt{fenton}} deben existir en todos los nodos del cluster. Y ambos directorios tambi\'{e}n deben ser accesibles desde cualquier nodo del cluster.

\subsection{Acceso SSH con autenticaci\'{o}n RSA}

En el nodo de ejecuci\'{o}n de la aplicaci\'{o}n web, debe ser posible realizar un SSH sin password desde el usuario que ejecuta la aplicaci\'{o}n web (el usuario configurado en el servidor Apache) al usuario del sistema (por defecto \mbox{\texttt{fenton}}) en el nodo maestro. 

El primer paso para configurar este acceso es averiguar el usuario de ejecuci\'{o}n de los scripts en Apache. Este usuario var\'{i}a de distribuci\'{o}n en distribuci\'{o}n. En nuestro sistema es \mbox{\texttt{www-data}}.

Luego debemos asignar a este usuario un directorio home. Para esto es necesario editar \mbox{\texttt{/etc/passwd}}:

\begin{verbatim}
www-data:x:33:33:www-data:/home/www-data:/bin/sh
\end{verbatim}

De esta manera asignamos el directorio \mbox{\texttt{/home/www-data}} como home del usuario. Seguramente este directorio no exista, por lo que deberemos crearlo y asignarle \mbox{\texttt{www-data}} como owner.

A continuaci\'{o}n creamos la clave publica y la privada, estas claves son necesarias para la autenticaci\'{o}n:

\begin{verbatim}
root:# su - www-data
www-data:$ ssh-keygen -t dsa
\end{verbatim}

Finalmente debemos agregar a las claves publicas del usuario \mbox{\texttt{fenton}} la reci\'{e}n creada clave de \mbox{\texttt{www-data}}.

\begin{verbatim}
fenton:$ mkdir -p /home/fenton/.ssh
root:# cd /home/fenton/.ssh
root:# cat /home/www-data/.ssh/id_dsa.pub > authorized_keys2
root:# chown fenton.fenton /home/fenton/.ssh/authorized\_keys2
\end{verbatim}

Es necesario probar al menos una vez el acceso SSH al nodo maestro para agregar el nodo maestro a los hosts conocidos de SSH. Suponiendo que el servidor apache se encuentra en el nodo maestro ejecutamos:

\begin{verbatim}
www-data:$ ssh fenton@localhost
\end{verbatim}

\subsection{Base de datos}

Es necesario crear un usuario para el sistema en la base de datos, asignarle una contrase\~{n}a y crear las tablas de datos. A continuaci\'{o}n detallaremos como crear todo esto desde la linea de comando. Como primer paso debemos crear el usuario de la base de datos:

\begin{verbatim}
root:# su - postgres 
postgres:$ createuser fentondb 
\end{verbatim}

Luego creamos el esquema de datos y las tablas:

\begin{verbatim}
$ createdb fentondb 
$ psql fentondb
psql=# \i postgres/database.sql
\end{verbatim}

Finalmente asignamos una contrase\~{n}a al nuevo usuario:

\begin{verbatim}
psql=# alter user fentondb password 'fentondb';
\end{verbatim}

\subsection{Aplicaci\'{o}n web}

Para instalar la aplicaci\'{o}n web simplemente debemos copiar el directorio \mbox{\texttt{web/}} a la ubicaci\'{o}n deseada. Por ejemplo \mbox{\texttt{/home/fenton/web}}, y luego crear un alias para la aplicaci\'{o}n. En la mayor\'{i}a de los casos basta con copiar el archivo \mbox{\texttt{apache/fenton.conf}} a \mbox{\texttt{/etc/apache2/conf.d}}.

La aplicaci\'{o}n web debe ser accesible desde cualquier nodo del cluster. Esto es necesario debido a que los scripts de prologo y epilogo (ver siguiente paso) requieren ejecutar scripts PHP de la aplicaci\'{o}n.

Por \'{u}ltimo debemos configurar el script de control de cuota para que ejecute periodicamente en el nodo maestro. Para esto podemos utilizar el comando cron de Linux. 

El script de control se encuentra en \mbox{\texttt{/home/fenton/web/lib/Vigilante.php}} y para ejecutarlo peri\'{o}dicamente debemos editar la confguraci\'{o}n del cron ejecutando:

\begin{verbatim}
root:# crontab -e
\end{verbatim}

Este comando nos abrir\'{a} para edici\'{o}n el archivo de configuraci\'{o}n del cron del usuario. Supongamos que se quiere ejecutar el script de control una vez cada hora, para esto editamos la configuraci\'{o}n agregando la siguiente linea:

\begin{verbatim}
01  * * * * php /home/fenton/web/lib/Vigilante.php
\end{verbatim}

\subsection{Configuraci\'{o}n de TORQUE}

Es necesario que TORQUE notifique al sistema cuando un trabajo es sacado de la cola de espera para iniciar su ejecuci\'{o}n, y cuando un trabajo termina de ejecutar (ya sea de forma exitosa o no). Para esto debemos editar y copiar el script de pre-ejecuci\'{o}n (prologo) y el script de post-ejecuci\'{o}n (epilogo). Estos scripts deben estar disponibles en todos los nodos del cluster (tanto el maestro como los nodos de c\'{o}mputo). 

Los scripts se encuentran en \mbox{\texttt{torque/prologue}} y \mbox{\texttt{torque/epilogue}}, y se deben copiar a \mbox{\texttt{/var/spool/torque/mom\_priv}}. Luego de copiados es necesario editar estos scripts configurando la variable \mbox{\texttt{PATH\_PHP}} seg\'{u}n nuestra instalaci\'{o}n, p. ej.: \mbox{\texttt{/home/fenton/web/bin}}.

\subsection{Configuraci\'{o}n final del sistema}

Como \'{u}ltimo paso, debemos realizar los ajustes finales y configurar la aplicaci\'{o}n web con la ubicaci\'{o}n de los diferentes componentes del sistema. Para esto debemos editar el archivo de configuraci\'{o}n \mbox{\texttt{/home/fenton/web/lib/Constantes.php}}.

Si toda la instalaci\'{o}n fue realizada utilizando las ubicaciones sugeridas las modificaciones al archivo de configuraci\'{o}n deber\'{i}an ser m\'{i}nimas. A continuaci\'{o}n veremos algunos de los par\'{a}metros de configuraci\'{o}n:

\begin{itemize}
\item Ubicaci\'{o}n del repositorio y de los archivos temporales.
\begin{verbatim}
define("RAIZ","/home/fenton/repositorio\char");
define("RAIZ_SISTEMA","/home/fenton/repositorio/sistema");
define("TMP", "/tmp");
\end{verbatim}

\item Conexi\'{o}n a la base de datos.
\begin{verbatim}
define("CONEXION_HOST","localhost");
define("CONEXION_PORT","5432");
define("CONEXION_USUARIO","fentondb");
define("CONEXION_PASSWORD","fentondb");
define("CONEXION_BASE","fentondb");
\end{verbatim}

\item Configuraci\'{o}n del launcher MPI.
\begin{verbatim}
define("MPIEXEC","/usr/local/openmpi-1.2.6/bin/mpiexec");
\end{verbatim}

\item URL de la instalaci\'{o}n de Ganglia.
\begin{verbatim}
define("GANGLIA_URL","http://localhost/ganglia");
\end{verbatim}

\item Configuraci\'{o}n del usuario y el host utilizado por el sistema para ejecutar tareas administrativas.
\begin{verbatim}
define("SSH","/usr/bin/ssh"); 
define("USERNAME","fenton"); 
define("HOST","localhost");
\end{verbatim}

\item Opciones generales del sistema.
\begin{verbatim}
define("GRUPOFENTON","fenton");
\end{verbatim} 
\begin{quote}
Grupo al que pertenecen todos los usuarios del sistema.
\end{quote}
\begin{verbatim}
define("REDIRECCION_SALIDA",
 "/home/fenton/web/bin/redireccion_salida.php");
define("OUTPUT","salida\_extra");
define("EJECUTABLE","plantillas/archivos/qsub.script");
define("LOG_EJECUCIONES","../log/ejecuciones.log");
define("TIEMPO_REFRESH_RESULTADOS","5");

define("COMANDOS_EJECUCION",
 "make=make&mpicc=/usr/local/openmpi-1.2.6/bin/mpicc");
\end{verbatim}
\begin{quote}
Lista de comandos disponibles para la ejecuci\'{o}n por parte del usuario del sistema. Deben estar separados entre ellos por un '\&', y cada uno esta compuesto de dos partes separadas por un '=': la primer parte es la descripci\'{o}n y la segunda parte es ubicaci\'{o}n del comando.
\end{quote}

\item Configuraci\'{o}n de TORQUE y Maui.
\begin{verbatim}
define("PATH_TORQUE","/usr/local/torque-2.3.0");
define("PATH_MAUI","/usr/local/maui");
define("QSUB",PATH_TORQUE."/bin/qsub");
etc...
\end{verbatim}

\end{itemize}

\paragraph{Algunas de las opciones que seguramente siempre debamos configurar son:}
\begin{verbatim}
define("CONEXION_HOST","servidor.fing.edu.uy");
\end{verbatim}
\begin{quote}
Ubicaci\'{o}n del host ejecutando la base de datos.
\end{quote}
\begin{verbatim}
define("GANGLIA_URL","http://servidor.fing.edu.uy/ganglia");
\end{verbatim}
\begin{quote}
URL base de la instalaci\'{o}n de Ganglia.
\end{quote}

\begin{thebibliography}{99}
\bibitem{postgres}
PostgreSQL\\\url{http://www.postgresql.org/}

\bibitem{apache}
Apache\\\url{http://httpd.apache.org/}

\bibitem{php}
PHP\\\url{http://www.php.net/}

\bibitem{ganglia}
Ganglia\\\url{http://ganglia.info/}

\bibitem{torque}
TORQUE\\\url{http://www.clusterresources.com/pages/products/torque-resource-manager.php}

\bibitem{maui}
Maui\\\url{http://www.clusterresources.com/pages/products/maui-cluster-scheduler.php}

\bibitem{mpich1}
MPICH1\\\url{http://www-unix.mcs.anl.gov/mpi/mpich1/}

\bibitem{mpiexec}
Mpiexec\\\url{http://www.osc.edu/~pw/mpiexec/index.php}

\bibitem{openmpi}
OpenMPI\\\url{http://www.open-mpi.org/}

\end{thebibliography}
\end{document}
