\documentclass[a4paper,10pt,spanish]{article}

\usepackage[latin1]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{color}
\usepackage{fancyhdr}
\pagestyle{fancy}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}
%% Special footnote code from the package 'stblftnt.sty'
%% Author: Robin Fairbairns -- Last revised Dec 13 1996
\let\SF@@footnote\footnote
\def\footnote{\ifx\protect\@typeset@protect
    \expandafter\SF@@footnote
  \else
    \expandafter\SF@gobble@opt
  \fi
}
\expandafter\def\csname SF@gobble@opt \endcsname{\@ifnextchar[%]
  \SF@gobble@twobracket
  \@gobble
}
\edef\SF@gobble@opt{\noexpand\protect
  \expandafter\noexpand\csname SF@gobble@opt \endcsname}
\def\SF@gobble@twobracket[#1]#2{}

\makeatother

\usepackage{babel}
\deactivatetilden

\begin{document}

\title{\chead{}
\rhead{Página \thepage}
\lfoot{}
\cfoot{}
\rfoot{} 
\date{}{\small Facultad de Ingeniería}\\
{\small Universidad de la República}{\large }\\
{\large \bigskip{}
}{\Large Proyecto Fenton - Cluster de Computadores de Alto Desempeño
con Acceso Remoto (CCADAR)}{\large }\\
{\large \bigskip{}
}{\Large Manual de instalación}{\large }\\
{\large \bigskip{}
Julio 2008\bigskip{}
}}


\author{{\normalsize Estudiantes:}\texttt{\normalsize{} }~\\
\texttt{\normalsize Santiago Iturriaga, Paulo Maya, Damián Pintos\medskip{}
}~\\
{\normalsize Supervisor:}\texttt{\normalsize{} }~\\
\texttt{\normalsize Mg. Ing. Sergio Nesmachnow}}

\maketitle
\newpage{}\tableofcontents{}\newpage{}

\section{Sistema operativo}

Es recomendado instalar el sistema Fenton sobre alguna distribuci\'{o}n Linux 2.6 (o superior), con una arquitectura de 32-bits (x86) o 64-bits (x64). Si bien el sistema Fenton fue probado en SUSE, Ubuntu y Fedora, esto no quita que pueda realizarse una instalaci\'{o}n en otras distribuciones (e.g., Debian, Slackware, etc.) o hasta otro sistema operativo de la familia POSIX (e.g., FreeBSD, Solaris, etc.).

Durante el resto del documento se asumir\'{a} que el sistema Fenton se instalar\'{a} en una distribuci\'{o}n Linux.

\section{Pre-instalaci\'{o}n}

\paragraph{Usuario y grupo del sistema.}

Se necesita crear un usuario y un grupo del sistema para ejecutar el sistema Fenton. Este usuario del sistema debe ser configurado como administrador en TORQUE y Maui. Por defecto este usuario y este grupo se llamar\'{a}n fenton.

Para crear el usuario y grupo del sistema se deben ejecutar los siguientes comandos:

\begin{verbatim}
root:# addgroup fenton
root:# adduser fenton
root:# adduser fenton fenton
\end{verbatim}

\section{Requerimientos de software}

A continuaci\'{o}n se detallar\'{a} el software base requerido por el sistema Fenton. Por simplicidad se recomienda realizar la instalaci\'{o}n del software utilizando los paquetes de la distribuci\'{o}n de Linux sobre la que se est\'{e} trabajando.

Se vera un breve instructivo de como realizar la instalaci\'{o}n compilando e instalando manualmente el software utilizando el c\'{o}digo fuente.

\subsection{PostgreSQL 8.2.5 o superior \small{[requerido]}}

PostgreSQL\cite{postgres} es un servidor de base de datos relacional liberado bajo la licencia BSD. El sistema Fenton utiliza PostgreSQL para persistir la informaci\'{o}n sobre usuarios, trabajos del cluster, etc.

Para la compilaci\'{o}n e instalaci\'{o}n de PostgreSQL en \mbox{\texttt{/usr/local/postgresql-8.2.5}} se deben seguir los siguientes pasos:

\begin{verbatim}
$ ./configure --prefix=/usr/local/postgresql-8.2.5
$ make
root:# make install
\end{verbatim}

Luego se debe crear el usuario de PostgreSQL y asignar espacio f\'{i}sico para los datos y logs.

\begin{verbatim}
root:# adduser postgres 
root:# mkdir /usr/local/postgresql-8.2.5/data 
root:# chown postgres /usr/local/postgresql-8.2.5/data 
postgres:$ cd /usr/local/postgresql-8.2.5/bin
postgres:$ ./initdb -D /usr/local/postgresql-8.2.5/data 
\end{verbatim}

Finalmente se inicia el servicio:

\begin{verbatim}
postgres:$ cd /usr/local/postgresql-8.2.5/bin
postgres:$ ./pg_ctl -D /usr/local/postgresql-8.2.5/data
\end{verbatim}

El sistema Fenton requiere que la base de datos sea accesible desde todos los nodos del cluster (tanto el nodo maestro como los nodos de computo). Para habilitar el acceso externo a la base de datos se deben editar los siguientes archivos de configuraci\'{o}n:
\begin{verbatim}
/net/local/postgresql-8.2.5/data/pg_hba.conf
/net/local/postgresql-8.2.5/data/postgresql.conf
\end{verbatim}

\subsection{Apache 2.0 o superior \small{[requerido]}}

El servidor Apache\cite{apache} es un servidor HTTP de c\'{o}digo abierto y es utilizado para la publicación de la interfaz web de acceso remoto al sistema Fenton.

En la actualidad todas las distribuciones de Linux disponen de un paquete de Apache 2.0 o superior y se recomienda utilizar esta versi\'{o}n de Apache en la publicaci\'{o}n de la interfaz web. 

\subsection{PHP 5.2.5 o superior \small{[requerido]}}

PHP\cite{php} es un lenguaje de programaci\'{o}n interpretado, dise\~{n}ado originalmente para la creaci\'{o}n de p\'{a}ginas web din\'{a}micas. Fenton se encuentra casi completamente implementando en PHP y requiere que este lenguaje sea instalado en todos los nodos del cluster. En el nodo maestro PHP es utilizado para la ejecuci\'{o}n de la interfaz web y la l\'{o}gica del sistema. En los nodos de c\'{o}mputo PHP es utilizado para ejecutar scripts de pre-ejecuci\'{o}n y post-ejecuci\'{o}n de trabajos en el cluster.

Compilaci\'{o}n e instalaci\'{o}n de PHP en el directorio \mbox{\texttt{/usr/local/php-5.2.5}}:

\begin{verbatim}
$ ./configure --prefix=/usr/local/php-5.2.5 \
> --with-pgsql=/usr/local/postgresql-8.2.5 --with-gd 
$ make 
root:# make install
\end{verbatim}

Es necesario incluir soporte para PostgreSQL y para la biblioteca gr\'{a}fica GD utilizada por Ganglia. Por \'{u}ltimo es necesario configurar el servidor HTTP Apache para poder ejecutar archivos PHP.

\subsection{Ganglia 3.0.5 o superior \small{[requerido]}}

Ganglia es utilizado para monitorear el estado del cluster y nos brinda reportes de carga de memoria de los nodos, tr\'{a}fico de red, utilizaci\'{o}n de CPU, entre otros. 

Ganglia cuenta con tres componentes que deben ser instalados:

\subsubsection{gmond}

Gmond es un demonio encargado de recolectar y enviar informaci\'{o}n del estado del nodo sobre el que esta ejecutando. Por esta raz\'{o}n se debe ejecutar en cada una de los nodos del cluster que se desea monitorear.

Compilaci\'{o}n e instalaci\'{o}n de gmond se debe ejecutar:

\begin{verbatim}
$ ./configure --prefix=/usr/local/ganglia-3.0.7
$ make
root:# make install
\end{verbatim}

En el directorio \mbox{\texttt{ganglia-3.0.7-src/gmond}} se pueden encontrar algunos scripts de ejemplo para ejecutar gmond al inicio del sistema como un servicio. 

Por \'{u}ltimo es necesario configurar gmond. Para esto se debe crear el archivo de configuraci\'{o}n \mbox{\texttt{/etc/gmond.conf}} con valores por defecto ejecutando:

\begin{verbatim}
root:# cd /usr/local/ganglia-3.0.7/sbin
root:# ./gmond --default_config > /etc/gmond.conf
\end{verbatim}


\subsubsection{gmetad}

Gmetad es un demonio al igual que gmond, pero en lugar de enviar datos oficia de servidor recolectando la informaci\'{o}n enviada por gmond desde los nodos del cluster. Solamente una instancia de este demonio debe
estar en ejecuci\'{o}n. 

Compilaci\'{o}n e instalaci\'{o}n de gmetad y gmond:

\begin{verbatim}
$ ./configure --prefix=/usr/local/ganglia-3.0.7 --with-gmetad 
$ make
root:# make install
\end{verbatim}

Dentro del directorio \mbox{\texttt{ganglia-3.0.7-src/gmetad}} se encuentran algunos scripts de ejemplo para iniciar gmetad como servicio al inicio del sistema. A diferencia de gmond, gmetad contiene adem\'{a}s con un archivo de configuraci\'{o}n por defecto que se encuentra en \mbox{\texttt{ganglia-3.0.7-src/gmetad/gmetad.conf}} y que se debe copiar a \mbox{\texttt{/etc/gmetad.conf}} y luego editar.

Finalmente se debe asignar espacio en disco en donde gmetad guardar\'{a} la informaci\'{o}n que recopile. La ubicaci\'{o}n por defecto es \mbox{\texttt{/var/lib/ganglia/rrds}} y su owner debe ser el usuario que fue configurado en \texttt{gmetad.conf} como usuario de ejecuci\'{o}n.

\subsubsection{Interfaz web}

Ganglia cuenta con una interfaz Web implementada en PHP donde se presenta visualmente la informaci\'{o}n del cluster. La interfaz Web de Ganglia debe estar alojada en el mismo nodo que ejecuta el demonio gmetad.

Para realizar la instalaci\'{o}n de la interfaz Web es necesario copiar el directorio \mbox{\texttt{ganglia-3.0.7-src/web}} al directorio \texttt{DocumentRoot} de instalaci\'{o}n del servidor Apache, e.g.: \mbox{\texttt{/var/www/ganglia}}.

\subsection{TORQUE 2.3.0 o superior \small{[requerido]}}

TORQUE\cite{torque} es un manejador de recursos distribuidos (DRM) y es utilizado para administrar los recursos disponibles en el cluster: procesadores, memoria, tiempo de c\'{o}mputo, etc. La arquitectura de un cluster TORQUE cuenta con un nodo maestro y muchos nodos de c\'{o}mputo. El nodo maestro debe ejecutar el demonio \texttt{qserverd} y los nodos de c\'{o}mputo deben ejecutar el demonio \texttt{qnoded}. A continuaci\'{o}n se describir\'{a} como instalar estos demonios.

\subsubsection{Nodo maestro: qserverd}

Instalaci\'{o}n de el servidor de TORQUE en \mbox{\texttt{/usr/local/torque-2.3.0}}:

\begin{verbatim}
$ ./configure --prefix=/usr/local/torque-2.3.0 
$ make 
root:# make install
\end{verbatim}

Una vez terminada la instalaci\'{o}n del servidor, es necesario configurar el demonio qserverd:

\begin{verbatim}
root:# export PATH=$PATH:/usr/local/torque-2.3.0/bin
root:# export PATH=$PATH:/usr/local/torque-2.3.0/sbin
root:# torque.setup fenton
\end{verbatim}

Donde \mbox{\texttt{fenton}} es el usuario que se desempe\~{n}ar\'{a} como administrador de TORQUE. Este script se encarga de crear un entorno b\'{a}sico de trabajo con una configuraci\'{o}n por defecto para poder iniciar el servidor. Finalmente se deben configurar los nodos de c\'{o}mputo editando \mbox{\texttt{/var/spool/torque/server\_priv/nodes}}, e.g.:

\begin{verbatim}
nodo001.fing.edu.uy
nodo002.fing.edu.uy
nodo003.fing.edu.uy
\end{verbatim}

O mediante la interfaz de consola de TORQUE:

\begin{verbatim}
$ qmgr
qmgr: create node nodo001.fing.edu.uy
\end{verbatim}

\subsubsection{Nodo de c\'{o}mputo: qnoded}

Una vez compilado torque es posible crear paquetes de instalaci\'{o}n distribuibles para instalar en los nodos de c\'{o}mputo. Esto simplifica la instalaci\'{o}n de los nodos de c\'{o}mputo si se trata de un cluster homog\'{e}neo, en cambio si los nodos del cluster cuentan con sistemas operativos diferentes o arquitecturas diferentes ser\'{a} necesario compilar TORQUE para cada nodo. Se asumir\'{a} que se trata de un cluster homog\'{e}neo. Para crear los paquetes de instalaci\'{o}n distribuibles se debe ejecutar:

\begin{verbatim}
$ make packages
\end{verbatim}

Luego en cada nodo de c\'{o}mputo:

\begin{verbatim}
root:# ./torque-package-clients-linux-i686.sh --install
root:# ./torque-package-mom-linux-i686.sh --install
\end{verbatim}

El servidor MOM para cada nodo de c\'{o}mputo debe ser configurado para confiar en el servidor maestro de TORQUE editando el archivo:

\begin{verbatim}
/var/spool/torque/mom_priv/config
\end{verbatim}

\paragraph{Ejecuci\'{o}n de los demonios.}

Es necesario que el servidor de TORQUE (qserverd) y los demonios qnoded se encuentren en ejecuci\'{o}n permanente en el cluster. Para esto es recomendable que sean agregados como demonios en el sistema creando scripts de ejecuci\'{o}n en \mbox{\texttt{/etc/init.d}}. Para esto existen dos scripts que pueden tomarse de ejemplo:

\begin{verbatim}
scripts/qserverd.example.sh
scripts/qnoded.example.sh
\end{verbatim}

\subsection{Maui 3.2.6p19 o superior \small{[requerido]}}

Maui\cite{maui} es un despachador (scheduler) de trabajos para clusters de c\'{o}digo abierto que soporta varias pol\'{i}ticas de despacho, prioridades din\'{a}micas, reservas de recursos, etc.

\paragraph{Compilaci\'{o}n, instalaci\'{o}n y configuraci\'{o}n.}

Existe un bug en el script de configuraci\'{o}n e instalaci\'{o}n de Maui por lo que se recomienda realizar la instalaci\'{o}n en \mbox{\texttt{/usr/local/maui}} (su ubicaci\'{o}n por defecto). Adem\'{a}s es necesario estar logueado con el usuario de Linux que se desempe\~{n}ar\'{a} como administrador del scheduler al momento de la compilaci\'{o}n. Este usuario debe ser tambi\'{e}n administrador de TORQUE, por lo que recomendamos utilizar el mismo usuario que se asign\'{o} durante el paso anterior como administrador de TORQUE (fenton en este ejemplo). Una vez logeado con el usuario fenton, se deben ejecutar los siguientes comandos:

\begin{verbatim}
fenton:$ ./configure --prefix=/usr/local/maui \
> --with-pbs=/usr/local/torque-2.3.0
fenton:$ make 
root:# mkdir -p /usr/local/maui
root:# chown fenton.fenton /usr/local/maui
fenton:$ make install
\end{verbatim}

Es posible que ocurra un error durante la creaci\'{o}n de directorios en la instalaci\'{o}n. Si sucede esto es necesario crear manualmente los directorios: 'log', 'traces', 'stats', 'spool' y 'tools' en el directorio de instalaci\'{o}n y luego volver a ejecutar \mbox{\texttt{make install}}. 

Se debera asegurar que el usuario administrador de Maui tenga permisos totales sobre el directorio de instalaci\'{o}n. Una posibilidad para asegurar esto es asignarlo como owner ejecutando:

\begin{verbatim}
root:# chown -R fenton /usr/local/maui
\end{verbatim}

Luego de esto se da por finalizada la instalaci\'{o}n. A continuaci\'{o}n es necesario configurar el scheduler editando \mbox{\texttt{/usr/local/maui/maui.cfg}}:

\begin{verbatim}
RMCFG[SERVIDOR.FING.EDU.UY] TYPE=PBS
\end{verbatim}

Para clusters peque\~{n}os quiz\'{a}s sea recomendable disminuir el tiempo de polling para tener una respuesta m\'{a}s r\'{a}pida por parte del scheduler modificando el par\'{a}metro \texttt{RMPOLLINTERVAL}, e.g.:

\begin{verbatim}
RMPOLLINTERVAL 00:00:05
\end{verbatim}

\paragraph{Ejecuci\'{o}n del demonio.}

Al igual que TORQUE, tambi\'{e}n es necesario que Maui se encuentre en ejecuci\'{o}n constante en el cluster. Y de la misma forma que antes, tambi\'{e}n existe un script que puede tomarse de ejemplo para esto en \mbox{\texttt{scripts/maui.example.sh}}.

\subsection{MPI \small{[opcional]}}

Para ejecutar programas paralelos utilizando MPI es necesario contar con una implementaci\'{o}n del est\'{a}ndar MPI. A continuaci\'{o}n veremos las implementaciones que fueron probadas en el sistema.

Cualquiera sea la implementaci\'{o}n elegida, es necesario que sea instalada (o se encuentra accesible) en todos los nodos del cluster.

\subsubsection{MPICH 1.2.7p1 o superior}

MPICH1\cite{mpich1} es una implementaci\'{o}n del est\'{a}ndar MPI versi\'{o}n 1. Para compilar e instalar MPICH1 en \mbox{\texttt{/usr/local/mpich-1.2.7p1}} se debe ejecutar:

\begin{verbatim}
$ ./configure --with-device=ch_p4 \
> --prefix=/usr/local/mpich-1.2.7p1 \
> --with-common-prefix=/usr/local/mpich-1.2.7p1 
$ make 
root:# make install
\end{verbatim}

\subsubsection{Mpiexec 0.83 o superior}

Mpiexec\cite{mpiexec} es un programa que reemplaza al script mpirun del paquete MPICH y es utilizado para inicializar un trabajo paralelo desde TORQUE. Si bien no es obligatorio utilizar Mpiexec, este facilita la integraci\'{o}n de MPICH con TORQUE y es muy recomendable utilizarlo.

\begin{verbatim}
$ ./configure --prefix=/usr/local/mpiexec-0.83 \
> --with-default-comm=mpich-p4 \
> --with-pbs=/usr/local/torque-2.3.0 \
> --with-mpicc=/usr/local/mpich-1.2.7p1/bin/mpicc 
$ make 
root:# make install
\end{verbatim}

\begin{quote}
NOTA: Mpiexec no es necesario si se utiliza OpenMPI.
\end{quote}

\subsubsection{OpenMPI 1.2.6 o superior \small{[recomendado]}}

OpenMPI\cite{openmpi} es una implementaci\'{o}n del est\'{a}ndar MPI versi\'{o}n 2. Para compilar e instalar OpenMPI se debe ejecutar:

\begin{verbatim}
$ ./configure --prefix=/usr/local/openmpi-1.2.6 \
> --with-tm=/usr/local/torque-2.3.0
> --disable-shared --enable-static
$ make
root:# make install
\end{verbatim}

Compilar OpenMPI de forma est\'{a}tica evita problemas de bibliotecas en los diferentes nodos. Aunque tiene dos desventajas: los ejecutables tienen un tama\~{n}o mayor y si las bibliotecas de OpenMPI son actualizadas es necesario re-compilar los proyectos y generar nuevos ejecutables para utilizar la nueva versi\'{o}n.

\section{Instalaci\'{o}n de Fenton}

\subsection{Repositorio del sistema}

Es necesario crear el repositorio ra\'{i}z donde el sistema almacenar\'{a} la estructura de clientes y trabajos para que los usuarios realicen ejecuciones y almacenen sus resultados. Por ejemplo, se crear\'{a} el repositorio en \mbox{\texttt{/home/fenton/repositorio}} y luego le asignaran los permisos adecuados.

\begin{verbatim}
fenton:$ mkdir -p /home/fenton/repositorio
fenton:$ chmod g+w+r /home/fenton/repositorio
\end{verbatim}

El grupo del directorio debe ser \mbox{\texttt{fenton}}. En caso de que sea otro es posible cambiarlo con el comando:

\begin{verbatim}
root:# chgrp fenton /home/fenton/repositorio
\end{verbatim}

Finalmente se debe crear un espacio para el repositorio interno del sistema. Aqu\'{i} el sistema almacenar\'{a} logs de ejecuciones y otros datos que no se encuentran directamente disponibles a los usuarios. La ubicaci\'{o}n recomendada para este directorio es \mbox{\texttt{/home/fenton/repositorio/sistema}}. Para crearlo:

\begin{verbatim}
fenton:$ mkdir -p /home/fenton/repositorio/sistema
fenton:$ chmod g+w+r /home/fenton/repositorio/sistema
\end{verbatim}

Al igual que en el directorio anterior, se debe asegurar que el grupo del directorio sea \mbox{\texttt{fenton}}.

El usuario y el grupo \mbox{\texttt{fenton}} deben existir en todos los nodos del cluster al igual que ambos directorios de repositorios deben ser accesibles desde cualquier nodo del cluster.

Debido a que el repositorio del sistema y la web (como se ver\'{a} m\'{a}s adelante) se encuentran en el directorio home del usuario \texttt{fenton}, se debe verificar que cualquier usuario tenga acceso de lectura al directorio \mbox{\texttt{/home/fenton}}. 

\begin{verbatim}
fenton:$ chmod a+r+x fenton
\end{verbatim}

\subsection{Acceso SSH con autenticaci\'{o}n RSA}

En el nodo de ejecuci\'{o}n de la aplicaci\'{o}n web debe ser posible realizar un acceso SSH sin password con el usuario que ejecuta la aplicaci\'{o}n web (el usuario configurado en el servidor Apache) al usuario del sistema (por defecto \mbox{\texttt{fenton}}) en el nodo maestro. 

El primer paso para configurar este acceso es averiguar el usuario de ejecuci\'{o}n de los scripts en Apache. Este usuario var\'{i}a de distribuci\'{o}n en distribuci\'{o}n. En la instalaci\'{o}n actual este usuario es \mbox{\texttt{www-data}}.

Luego se asignara a este usuario un directorio home. Para esto es necesario editar \mbox{\texttt{/etc/passwd}}:

\begin{verbatim}
www-data:x:33:33:www-data:/home/www-data:/bin/sh
\end{verbatim}

De esta manera se asigna el directorio \mbox{\texttt{/home/www-data}} como home del usuario. Seguramente este directorio no exista, por lo que se debe crear y asignarle \mbox{\texttt{www-data}} como owner.

A continuaci\'{o}n se crea la clave p\'{u}blica y la privada necesarias para la autenticaci\'{o}n:

\begin{verbatim}
root:# su - www-data
www-data:$ ssh-keygen -t dsa
\end{verbatim}

Finalmente se agregar\'{a} a las claves publicas del usuario \mbox{\texttt{fenton}} la reci\'{e}n creada clave de \mbox{\texttt{www-data}}.

\begin{verbatim}
fenton:$ mkdir -p /home/fenton/.ssh
root:# cd /home/fenton/.ssh
root:# cat /home/www-data/.ssh/id_dsa.pub > authorized_keys2
root:# chown fenton.fenton /home/fenton/.ssh/authorized_keys2
\end{verbatim}

Es necesario probar al menos una vez el acceso SSH al nodo maestro para agregar el nodo maestro a los hosts conocidos de SSH. Suponiendo que el servidor apache se encuentra en el nodo maestro se debe ejecutar:

\begin{verbatim}
www-data:$ ssh fenton@localhost
\end{verbatim}

\subsection{Base de datos}

Se debe crear un usuario para el sistema en la base de datos, asignarle una contrase\~{n}a y crear las tablas de datos. A continuaci\'{o}n se detallar\'{a} como realizar esto desde la l\'{i}nea de comando. Como primer paso se crear\'{a} el usuario de la base de datos:

\begin{verbatim}
root:# su - postgres 
postgres:$ createuser fentondb 
\end{verbatim}

Luego se deben crear el esquema de datos y las tablas:

\begin{verbatim}
$ createdb fentondb 
$ psql fentondb
psql=# \i postgres/database.sql
\end{verbatim}

Finalmente se asigna una contrase\~{n}a al nuevo usuario:

\begin{verbatim}
psql=# alter user fentondb password 'fentondb';
\end{verbatim}

\subsection{Aplicaci\'{o}n web}

Para instalar la aplicaci\'{o}n web se debe copiar el directorio \mbox{\texttt{web/}} a la ubicaci\'{o}n deseada. Por ejemplo \mbox{\texttt{/home/fenton/web}}, y luego crear un alias para la aplicaci\'{o}n. En la mayor\'{i}a de los casos basta con copiar el archivo \mbox{\texttt{apache/fenton.conf}} a \mbox{\texttt{/etc/apache2/conf.d}}.

La aplicaci\'{o}n web debe ser accesible desde cualquier nodo del cluster. Esto es necesario debido a que los scripts de prologo y epilogo (ver siguiente paso) requieren ejecutar scripts PHP de la aplicaci\'{o}n.

El sistema debe contar con un interprete de l\'{i}nea de comando de PHP y este se debe encontrar en el directorio \mbox{\texttt{/usr/bin/php}}. Existen varios scripts PHP que se encuentran en \mbox{\texttt{/home/fenton/web/bin}} y que esperan encontrar el interprete en la ubicaci\'{o}n mencionada. En caso de que el interprete de PHP no se encuentre en esa ubicaci\'{o}n o tenga otro nombre (e.g., \mbox{\texttt{/usr/bin/php5}}) puede solucionarse f\'{a}cilmente creando un link al nombre deseado de la siguiente forma:

\begin{verbatim}
ln -s /usr/bin/php5 /usr/bin/php
\end{verbatim}

Por \'{u}ltimo se configurara el script de control de cuota para que ejecute peri\'{o}dicamente en el nodo maestro. Para esto se puede utilizar el comando cron de Linux. 

El script de control se encuentra en \mbox{\texttt{/home/fenton/web/lib/Vigilante.php}} y para ejecutarlo peri\'{o}dicamente es necesario editar la configuraci\'{o}n de la aplicaci\'{o}n cron ejecutando:

\begin{verbatim}
root:# crontab -e
\end{verbatim}

Este comando abrir\'{a} para edici\'{o}n el archivo de configuraci\'{o}n del cron del usuario. Supongase que se quiere ejecutar el script de control una vez cada hora, para esto se edita la configuraci\'{o}n agregando la siguiente l\'{i}nea:

\begin{verbatim}
01  * * * * php /home/fenton/web/lib/Vigilante.php
\end{verbatim}

\subsection{Configuraci\'{o}n de TORQUE}

Es necesario que TORQUE notifique al sistema cuando un trabajo es quitado de la cola de espera para iniciar su ejecuci\'{o}n, y cuando un trabajo termina de ejecutar (ya sea de forma exitosa o no). Para esto se debe editar y copiar el script de pre-ejecuci\'{o}n (prologo) y el script de post-ejecuci\'{o}n (epilogo). Estos scripts deben estar disponibles en todos los nodos del cluster (tanto el maestro como los nodos de c\'{o}mputo). 

Los scripts se encuentran en \mbox{\texttt{torque/prologue}} y \mbox{\texttt{torque/epilogue}}, y se deben copiar a \mbox{\texttt{/var/spool/torque/mom\_priv}}. Luego de copiados es necesario editar estos scripts configurando la variable \mbox{\texttt{PATH\_PHP}} seg\'{u}n la instalaci\'{o}n, e.g.: \mbox{\texttt{/home/fenton/web/bin}}.

\subsection{Configuraci\'{o}n final del sistema}

Como \'{u}ltimo paso, se realizaran los ajustes finales y configurar\'{a} la aplicaci\'{o}n web con la ubicaci\'{o}n de los diferentes componentes del sistema. Para esto se editar\'{a} el archivo de configuraci\'{o}n \mbox{\texttt{/home/fenton/web/lib/Constantes.php}}.

Si la instalaci\'{o}n en su totalidad fue realizada utilizando las ubicaciones sugeridas las modificaciones al archivo de configuraci\'{o}n deber\'{i}an ser m\'{i}nimas. A continuaci\'{o}n se ver\'{a}n algunos de los par\'{a}metros de configuraci\'{o}n:

\begin{itemize}
\item Ubicaci\'{o}n del repositorio y de los archivos temporales.
\begin{verbatim}
define("RAIZ","/home/fenton/repositorio\char");
define("RAIZ_SISTEMA","/home/fenton/repositorio/sistema");
define("TMP", "/tmp");
\end{verbatim}

\item Conexi\'{o}n a la base de datos.
\begin{verbatim}
define("CONEXION_HOST","localhost");
define("CONEXION_PORT","5432");
define("CONEXION_USUARIO","fentondb");
define("CONEXION_PASSWORD","fentondb");
define("CONEXION_BASE","fentondb");
\end{verbatim}

\item Configuraci\'{o}n del launcher MPI.
\begin{verbatim}
define("MPIEXEC","/usr/local/openmpi-1.2.6/bin/mpiexec");
\end{verbatim}

\item URL de la instalaci\'{o}n de Ganglia.
\begin{verbatim}
define("GANGLIA_URL","http://localhost/ganglia");
\end{verbatim}

\item Configuraci\'{o}n del usuario y el host utilizado por el sistema para ejecutar tareas administrativas.
\begin{verbatim}
define("SSH","/usr/bin/ssh"); 
define("USERNAME","fenton"); 
define("HOST","localhost");
\end{verbatim}

\item Opciones generales del sistema.
\begin{verbatim}
define("GRUPOFENTON","fenton");
\end{verbatim} 
\begin{quote}
Grupo al que pertenecen todos los usuarios del sistema.
\end{quote}
\begin{verbatim}
define("REDIRECCION_SALIDA",
 "/home/fenton/web/bin/redireccion_salida.php");
define("OUTPUT","salida\_extra");
define("EJECUTABLE","plantillas/archivos/qsub.script");
define("LOG_EJECUCIONES","../log/ejecuciones.log");
define("TIEMPO_REFRESH_RESULTADOS","5");

define("COMANDOS_EJECUCION",
 "make=make&mpicc=/usr/local/openmpi-1.2.6/bin/mpicc");
\end{verbatim}
\begin{quote}
Lista de comandos disponibles para la ejecuci\'{o}n por parte del usuario del sistema. Deben estar separados entre ellos por un '\&', y cada uno est\'{a} compuesto de dos partes separadas por un '=': la primer parte es la descripci\'{o}n y la segunda parte es ubicaci\'{o}n del comando.
\end{quote}

\item Configuraci\'{o}n de TORQUE y Maui.
\begin{verbatim}
define("PATH_TORQUE","/usr/local/torque-2.3.0");
define("PATH_MAUI","/usr/local/maui");
define("QSUB",PATH_TORQUE."/bin/qsub");
etc...
\end{verbatim}

\end{itemize}

\paragraph{Algunas de las opciones que seguramente siempre se deben configurar son:}
\begin{verbatim}
define("CONEXION_HOST","servidor.fing.edu.uy");
\end{verbatim}
\begin{quote}
Ubicaci\'{o}n del host ejecutando la base de datos.
\end{quote}
\begin{verbatim}
define("GANGLIA_URL","http://servidor.fing.edu.uy/ganglia");
\end{verbatim}
\begin{quote}
URL base de la instalaci\'{o}n de Ganglia.
\end{quote}

\begin{thebibliography}{99}
\bibitem{postgres}
PostgreSQL\\\url{http://www.postgresql.org/}

\bibitem{apache}
Apache\\\url{http://httpd.apache.org/}

\bibitem{php}
PHP\\\url{http://www.php.net/}

\bibitem{ganglia}
Ganglia\\\url{http://ganglia.info/}

\bibitem{torque}
TORQUE\\\url{http://www.clusterresources.com/pages/products/torque-resource-manager.php}

\bibitem{maui}
Maui\\\url{http://www.clusterresources.com/pages/products/maui-cluster-scheduler.php}

\bibitem{mpich1}
MPICH1\\\url{http://www-unix.mcs.anl.gov/mpi/mpich1/}

\bibitem{mpiexec}
Mpiexec\\\url{http://www.osc.edu/~pw/mpiexec/index.php}

\bibitem{openmpi}
OpenMPI\\\url{http://www.open-mpi.org/}

\end{thebibliography}
\end{document}
